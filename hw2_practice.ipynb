{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b06ba3f-3c98-4fd7-948d-362d3260fc5d",
   "metadata": {},
   "source": [
    "# Введение в NLP, часть 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3154e",
   "metadata": {},
   "source": [
    "## NER c BERT (25 баллов)\n",
    "\n",
    "В данном задании мы будем обучать BERT на задаче Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a696ec-f1af-4779-bb81-a4aa644fddfd",
   "metadata": {},
   "source": [
    "### Подготовка данных (5 баллов)\n",
    "\n",
    "Подумать о:\n",
    "1. Как subword токенизация повлияет на BIO раззметку?\n",
    "2. Что делать с `[CLS]` и `[SEP]` токенами? (Проверьте что использует `DataCollatorForTokenClassification`)\n",
    "\n",
    "> Hint! Токенайзер умеет работать с предразделёнными на «слова» текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717d0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.12.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.1/468.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.9/436.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: xxhash, safetensors, fsspec, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 fsspec-2024.12.0 huggingface-hub-0.29.2 multiprocess-0.70.16 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers\n",
    "!pip install evaluate\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0696f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c1544-367c-492b-b7be-4e886babfc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565d9b6fd46433fa93d3113ba8def1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bced79612c542bebb9e138d8c4caaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21195b95601a4bcdbb31e2dfc0e44ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc89f585d3594cfc88de8206e7114957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_NER_MODEL = \"bert-base-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BASE_NER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13c09d0-a7ab-4fe3-aac4-fb8ba4b5f1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5a7052066741dda677ebf9564761fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce74f8abc8947af82b6b093c14a3325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716db35408ff4c14b526218fcad6667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282daabd8e6d4c578e5c1a742ede7b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39cf970d7d148c2826e89c476a8efba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd211c9f13ea4557b1e21bee2e94f866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003 = load_dataset(\"eriktks/conll2003\")\n",
    "conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92a1ac5b-0722-4387-a885-80b927fbc10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '100',\n",
       " 'tokens': ['Rabinovich',\n",
       "  'is',\n",
       "  'winding',\n",
       "  'up',\n",
       "  'his',\n",
       "  'term',\n",
       "  'as',\n",
       "  'ambassador',\n",
       "  '.'],\n",
       " 'pos_tags': [21, 42, 39, 33, 29, 21, 15, 21, 7],\n",
       " 'chunk_tags': [11, 21, 22, 15, 11, 12, 13, 11, 0],\n",
       " 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = conll2003[\"train\"][100]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573be60b-3c59-4a4c-9cc7-2cab8d9f2a2d",
   "metadata": {},
   "source": [
    "* tokens - исходные токены, для которых была сделана NER-разметка\n",
    "* ner_tags - векторизированные метки NER-тэгов\n",
    "* pos_tags - разметка частей речи, которую мы игнорируем\n",
    "* chunk_tags - разметка чанков, которую мы игнорируем\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c070226-e70f-4df1-b770-f403a19ef205",
   "metadata": {},
   "source": [
    "Обратите внимание, что количество токенов может превышать количество исходных лейблов:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb99e70-85e2-438f-8c68-e15a6701ccfa",
   "metadata": {},
   "source": [
    "Значение тэга в `ner_tags` отображается в метку NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8fce68-13e8-4093-a147-15ae865ac73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER TAGS [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"NER TAGS\", example[\"ner_tags\"])\n",
    "print(conll2003[\"train\"].features[\"ner_tags\"].feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27a3addb-073d-4a2f-bd49-caa3411a4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальные токены\n",
      "['Rabinovich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.']\n",
      "Векторизированные NER метки токенов\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Текстовые NER метки токенов\n",
      "['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Токены после работы токенайзера BERT\n",
      "['[CLS]', 'Ra', '##bino', '##vich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\"Оригинальные токены\")\n",
    "print(example[\"tokens\"])\n",
    "print(\"Векторизированные NER метки токенов\")\n",
    "print(example[\"ner_tags\"])\n",
    "tags_str = []\n",
    "features = conll2003[\"train\"].features[\"ner_tags\"].feature\n",
    "for tag in example[\"ner_tags\"]:\n",
    "    tags_str.append(features.int2str(tag))\n",
    "print(\"Текстовые NER метки токенов\")\n",
    "print(tags_str)\n",
    "print(\"Токены после работы токенайзера BERT\")\n",
    "print(bert_tokenizer(example[\"tokens\"], is_split_into_words=True).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48468ae1-48d3-4641-ada4-0a8cf25e50ad",
   "metadata": {},
   "source": [
    "Вспомним немного, как работают метки в задаче мер в кодировке IOB. В данной задаче у нас есть 4 типа именованных сущностей:\n",
    "* PER - персона\n",
    "* ORG - организация\n",
    "* LOC - локация\n",
    "* MISC - другое\n",
    "* O - отсутствие именованной сущности\n",
    "\n",
    "У каждого типа именованных 2 префикса:\n",
    "* `B-` - beginning, т.е. начало именованной сущности.\n",
    "* `I-` - inside, т.е. продолжение ранее начатой именованной сущностью.\n",
    "\n",
    "В исходной токенизации\n",
    "\n",
    "`['Rabinovich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.']`\n",
    "метки выглядят как \n",
    "\n",
    "`['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']`\n",
    "т.е. `Rabinovich` является персоной. На следующем токене именованная сущность заканчивается, т.к. у него метка `O`.\n",
    "\n",
    "После токенизации BERT наш сэмпл превращается в следующие токены:\n",
    "\n",
    "`['[CLS]', 'Ra', '##bino', '##vich', 'is', 'winding', 'up', 'his', 'term', 'as', 'ambassador', '.', '[SEP]']`\n",
    "Обратим внимание, что один токен `Rabinovich` с меткой `B-PER` был разбит токенизатором берта на 3 токена: `'Ra', '##bino', '##vich'`. Им нужно поставить в соответствие 3 метки: `B-PER, I-PER, I-PER`, т.е. мы разбиваем метку исходного токена на новые токены.\n",
    "\n",
    "Также обратим внимание на первый и последний токен - это спецстокены BERT означающие начало и конец текста. Им можно дать метки `O`, т.к. они не являются частью исходного текста, но мы будем давать им особое векторизированное значение -100. В [документации pytroch](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) у кроссэнтропийной функции потерь это дефолтное значение `ignore_index`, т.е. метки, которую мы будем игнорировать. Библиотека transformers также использует это значение. Таким образом на токенах, у которых стоит -100 в качестве векторизированного NER-тэга, не будет происходить обучение, они будут проигнорированы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ac7b9-d65f-4e09-8165-de5442a6ab6e",
   "metadata": {},
   "source": [
    "Напишите функцию `preprocess_ner_dataset`, которая разворачивает `ner_tags` для слов в тэги для BERT-токенов и готовит остальные данные для обучения (можно разделить на две функции или написать всё в одной). В резултате применения `conll2003.map(preprocess_ner_dataset)`, в каждом примере:\n",
    "1. Добавляется токенизированный вход (`input_ids`, `token_type_ids` и `attention_mask`). При конструировании этих векторов вручную нужно проставить `attention_mask` полностью единицами, т.к. в паддинги в сэмплах появляются только в рамках батчей, а `token_type_ids` полностью нулями.\n",
    "2. `ner_tags` разворачивается в `labels` для входных токенов\n",
    "\n",
    "Что можно использовать:\n",
    "* у объекта `conll2003[\"train\"].features[\"ner_tags\"].feature` есть методы `int2str` и `str2int` для превращение векторизованного NER-тэга в строковый вид и обратно\n",
    "* Спецтокенам BERT нужно поставить значение -100\n",
    "* вызов `bert_tokenizer(bert_tokenizer(example[\"tokens\"], is_split_into_words=True)` возвращает вам input_ids, attention_mask, token_type_ids\n",
    "* Вызов `bert_tokenizer(example[\"tokens\"], is_split_into_words=True, return_offsets_mapping=True))` возвращает дополнительно offset_mapping, позиции новых токенов в оригинальном тексте\n",
    "* `bert_tokenizer.vocab` - для превращения токенов в их индексы в словаре\n",
    "* `bert_tokenizer.tokenize` - разбитие текста (в том числе и исходных токенов) на токены BERT\n",
    "\n",
    "Ваша задача:\n",
    "1. Создать новый dict, в котором будут input_ids, attention_mask, token_type_ids\n",
    "2. Добавить в него labels - векторизированные NER-тэги, которые будут разбиты в соответствии с токенизацией BERT. Для этого можно можно разбить каждый токен отдельно и размножить его метки. Альтернативно можно использовать информацию об оффсетах токенов BERT, чтобы понять, частью какого исходного токена и какой исходной метки является данный BERT-токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01fe9e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1109, 8362, 2328, 8661, 8405, 1110, 1842, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 3), (4, 6), (6, 8), (8, 10), (10, 15), (16, 18), (19, 23), (23, 24), (0, 0)]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The unhappiness is real.\"\n",
    "bert_tokenizer(text, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd55d2-61f8-4591-8b53-999cca29ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def new_ner_tags (example: dict):\n",
    "    \"\"\"Функция для преобразования NER тегов в новые после токенизации текста.\"\"\"\n",
    "    old_tokens = example[\"tokens\"]\n",
    "    new_tokens = bert_tokenizer(example[\"tokens\"], is_split_into_words=True).tokens()\n",
    "    old_ner_tags = example[\"ner_tags\"]\n",
    "    new_ner_tags = []\n",
    "    offsets = bert_tokenizer(' '.join(old_tokens), return_offsets_mapping=True).offset_mapping\n",
    "\n",
    "    # перебираем токены из оригинального текста и собираем новые токены, если они часть старого\n",
    "    grouped_new_tokens = []\n",
    "    pointer = 1\n",
    "    for old in old_tokens: \n",
    "        sub_tokens = []\n",
    "        for loc_pointer in range(pointer, len(offsets)):\n",
    "            if offsets[pointer][1] == offsets[pointer+1][0]:\n",
    "                sub_tokens.append(new_tokens[pointer])\n",
    "                pointer += 1\n",
    "            else:\n",
    "                sub_tokens.append(new_tokens[pointer])\n",
    "                grouped_new_tokens.append((old, sub_tokens))\n",
    "                pointer += 1\n",
    "                break\n",
    "    \n",
    "    # проходим по собранным данным и собираем новые NER теги\n",
    "    int_to_str = conll2003[\"train\"].features[\"ner_tags\"].feature.int2str\n",
    "    str_to_int = conll2003[\"train\"].features[\"ner_tags\"].feature.str2int\n",
    "    for item, old_tag in zip(grouped_new_tokens, old_ner_tags):\n",
    "        # отдельно обрабатываем кейс, когда токен разбивается на несколько\n",
    "        if (len(item[1]) > 1):\n",
    "            new_tags = []\n",
    "            for i in range(len(item[1])):\n",
    "                if i == 0:\n",
    "                    new_tags.append(int_to_str(old_tag))\n",
    "                elif old_tag != 0:\n",
    "                    new_tags.append(\"I\" + int_to_str(old_tag)[1:])\n",
    "                else:\n",
    "                    new_tags.append(int_to_str(old_tag))\n",
    "        # иначе просто добавляем старый тег в список новых\n",
    "        else:\n",
    "            new_tags = [int_to_str(old_tag)]\n",
    "        new_ner_tags += new_tags\n",
    "    # преобразуем текстовые теги в числовые и добавляем специальные токены \n",
    "    return [-100] + [str_to_int(tag) for tag in new_ner_tags] + [-100]\n",
    "\n",
    "def preprocess_ner_dataset(example: dict):\n",
    "    \"\"\"Функция добавляет NER тэги к выходу токенайзера BERT.\"\"\"\n",
    "    out = bert_tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "    out[\"labels\"] = new_ner_tags(example)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4c9ed654-c4dc-4757-a000-b44bcb2f5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preprocess_ner_dataset(conll2003[\"train\"][0]) == {\n",
    "    'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], \n",
    "    'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "    'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb77fcb-4d0a-4e7d-bae8-4b1d25549e38",
   "metadata": {},
   "source": [
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5a8af22c-9da2-4bf0-a2f2-5808c72703d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example = preprocess_ner_dataset(example)\n",
    "required_keys = [\"input_ids\", \"labels\", \"attention_mask\", \"token_type_ids\"]\n",
    "for k in required_keys:\n",
    "    assert k in processed_example, f\"Отсутствует поле {k}\"\n",
    "\n",
    "required_keys_set = set(required_keys)\n",
    "for k in processed_example.keys():\n",
    "    assert k in required_keys_set, f\"В примере лишнее поле {k}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9a334417-49d8-4b02-8599-0914e1474671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 269.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизация верна!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for idx, example in tqdm(enumerate(conll2003[\"train\"])):\n",
    "    input_ids_real = bert_tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "    input_ids_ours = preprocess_ner_dataset(example)[\"input_ids\"]\n",
    "    assert input_ids_real == input_ids_ours, f\"Ошибка токенизации на примере {idx}\"\n",
    "    if idx >= 10:\n",
    "        break\n",
    "print(\"Токенизация верна!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7bb5b4f4-3958-4421-8136-17facfd19cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = conll2003[\"train\"][100]\n",
    "processed_example = preprocess_ner_dataset(example)\n",
    "\n",
    "assert processed_example[\"labels\"][0] == -100\n",
    "assert processed_example[\"labels\"][-1] == -100\n",
    "ner_tags = [features.int2str(i) for i in processed_example[\"labels\"][1:-1]]\n",
    "assert ner_tags == ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c04cc2a3-7d89-4b8b-a943-7c9fe10ca561",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = conll2003[\"train\"][200]\n",
    "processed_example = preprocess_ner_dataset(example)\n",
    "\n",
    "assert processed_example[\"labels\"][0] == -100\n",
    "assert processed_example[\"labels\"][-1] == -100\n",
    "ner_tags = [features.int2str(i) for i in processed_example[\"labels\"][1:-1]]\n",
    "assert ner_tags == ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8177dc-ab7b-4b67-8676-73c57e5b01f6",
   "metadata": {},
   "source": [
    "Применим нашу функцию к всему датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "106c8045-8883-46b1-ab95-0904f838a2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099cfbed413447e8be5b1bf6d2ca4cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151a349b026040aba52d3e1dfe995374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428b63030eda44a1919cefa83384c96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_ner_dataset = conll2003.map(preprocess_ner_dataset, num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb4ddd-3487-4785-80a6-0f821a7e4b9a",
   "metadata": {},
   "source": [
    "Подготовим `data_collator`. Это особый класс, который будет заниматься батчеванием сэмплов для обучения. Он добавит паддинги во все необходимые поля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a20511a1-d222-4034-b099-9dfdd02ed81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7c640-ac2b-4388-81b6-9f21c8c2ec2d",
   "metadata": {},
   "source": [
    "### Подготовка модели (5 баллов)\n",
    "\n",
    "Два возможных пути на этой стадии:\n",
    "1. Взять [готовый класс](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForTokenClassification) модели для классификации токенов. (Этот вариант настоятельно рекомендуется). Ему нужно подать \n",
    "- num_labels - число классов (число возможных меток для токенов), \n",
    "- id2label - словарь индекс метки -> текст метки , \n",
    "- label2id - словарь текст метки -> индекс метки. Из-за непростого наследования эти аргументы тяжело найти, прочитать про них можно [тут](https://huggingface.co/docs/transformers/en/main_classes/configuration#transformers.PretrainedConfig)\n",
    "2. Взять модель как фича экстрактор ([AutoModel](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodel)) и самостоятельно добавить классификационную голову. Вдохновиться можно по [ссылке](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L1847-L1860).\n",
    "\n",
    "Результатом должна быть модель, которая для каждого токена возвращает логиты/вероятности для `conll2003[\"train\"].features[\"ner_tags\"].feature.num_classes` классов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "331cd38a-3501-4f41-9193-8cf6579ad80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe26ed9b25af4ea887b9f73a264054bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoModel\n",
    "\n",
    "label_names = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "id2label = {features.str2int(el): el for el in features.names}\n",
    "label2id = {el: features.str2int(el) for el in features.names}\n",
    "\n",
    "bert_ner = AutoModelForTokenClassification.from_pretrained(\n",
    "    BASE_NER_MODEL, \n",
    "    num_labels=conll2003[\"train\"].features[\"ner_tags\"].feature.num_classes,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eaa2ce-0f56-4b8f-8829-ff9edffa0d34",
   "metadata": {},
   "source": [
    "### Подготовим Метрику (5 баллов)\n",
    "\n",
    "Дополните функцию, используя `metrics_calculator`, чтобы она возвращала `accuracy`, `precision`, `recall` и `f-меру`. `eval_predictions` - это кортеж из логитов токен классификатора и `labels`, которые мы подготовили с помощью `preprocess_ner_dataset`. Нужно\n",
    "1. Преобразовать логиты в предсказанные лейблы. Учтите, что для специальных токенов лейблов нет. В этом нам поможет argmax.\n",
    "2. Убрать паддинги - это те позиции, где метки (labels) равняются -100\n",
    "3. Перевести labels из чисел в текстовые метки с помощью `label_list`\n",
    "4. Посчитать метрики с помощью `metrics_calculator`\n",
    "5. Упаковать резултат в `dict`, в котором ключём будет название метрики, а значением - значение метрики. Брать можно только `overall_*` метрики для удобства\n",
    "\n",
    "В logits будет лежать тензор размерности \\[размер eval датасета, максимальная длина последовательности, число меток\\], содержащий предсказания модели\n",
    "\n",
    "В target_labels будет лежать тензор размерности \\[размер eval датасета, максимальная длина последовательности\\], содержащий метки из валидационной выборки.\n",
    "\n",
    "Примеры функции calculate_metrics можно посмотреть в [документации](https://huggingface.co/docs/evaluate/en/transformers_integrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c096f958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 0.5, 'overall_recall': 0.5, 'overall_f1': 0.5, 'overall_accuracy': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "metrics_calculator = evaluate.load(\"seqeval\")\n",
    "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "print(metrics_calculator.compute(references=y_true, predictions=y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "55ad18b4-5176-4bf1-ab9a-517466e79f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(eval_predictions):\n",
    "    logits, labels = eval_predictions\n",
    "    # logits - [batch_size, max_dataset_seq_len, num_classes]\n",
    "    # labels - [batch_size, max_dataset_seq_len]\n",
    "\n",
    "    clean_logits, clean_labels = [], []\n",
    "    for log, lab in zip(logits.argmax(-1).flatten(), labels.flatten()):\n",
    "        if lab != -100:\n",
    "            clean_logits.append(features.int2str(int(log)))\n",
    "            clean_labels.append(features.int2str(int(lab)))\n",
    "    return metrics_calculator.compute(references=[clean_labels], predictions=[clean_logits], zero_division=0)\n",
    "\n",
    "\n",
    "logits = np.array([\n",
    "    [  # Batch 1\n",
    "        [7, 0, 0, 0, 0, 0, 0, 0, 0],  # 'O'\n",
    "        [7, 0, 0, 0, 0, 0, 0, 0, 0],  # 'O'\n",
    "        [0, 0, 0, 0, 0, 0, 0, 7, 0],  # 'B-MISC'\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 7],  # 'I-MISC'\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 7],  # 'I-MISC'\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 7],  # 'I-MISC'\n",
    "        [7, 0, 0, 0, 0, 0, 0, 0, 0],  # 'O'\n",
    "    ],\n",
    "    [  # Batch 2\n",
    "        [0, 12, 0, 0, 0, 0, 0, 0, 0],  # 'B-PER'\n",
    "        [0, 0, 10, 0, 0, 0, 0, 0, 0],  # 'I-PER'\n",
    "        [12, 0, 0, 0, 0, 0, 0, 0, 0],  # 'O'\n",
    "        [0, 12, 0, 0, 0, 0, 0, 0, 0],  # 'B-PER' - но не оно по паддингам, поэтому не должно считаться в метрике!\n",
    "        [0, 0, 12, 0, 0, 0, 0, 0, 0],  # 'I-PER'\n",
    "        [0, 0, 12, 0, 0, 0, 0, 0, 0],  # 'I-PER'\n",
    "        [0, 0, 12, 0, 0, 0, 0, 0, 0],  # 'I-PER'\n",
    "    ]\n",
    "])\n",
    "\n",
    "labels = np.array([\n",
    "    [0, 0, 0, 7, 8, 8, 0],  # 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'\n",
    "    [1, 2, 0, -100, -100, -100, -100]  # 'B-PER', 'I-PER', 'O' (with padding -100s)\n",
    "])\n",
    "\n",
    "logits_res = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "labels_res = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "metrics_on_tensors = calculate_metrics((logits, labels))\n",
    "metrics_on_strs = metrics_calculator.compute(references=logits_res, predictions=labels_res)\n",
    "\n",
    "for k in metrics_on_tensors:\n",
    "    if \"overall\" in k:\n",
    "        assert metrics_on_tensors[k] == metrics_on_strs[k], k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44528ae-e63a-4561-b707-a891f8dcf75f",
   "metadata": {},
   "source": [
    "### Обучение (5 баллов)\n",
    "\n",
    "Два возможных пути на этой стадии:\n",
    "\n",
    "1. Использовать [Trainer](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html) класс из `transformers`\n",
    "2. Написать свой training loop\n",
    "\n",
    "Опишем подробнее первый путь, т.к. он настоятельно рекомендуется.\n",
    "\n",
    "Нужно создать класс Trainer и TrainingArguments.\n",
    "В [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) нужно как минимум следующие поля:\n",
    "* save_strategy, eval_strategy\n",
    "* metric_for_best_model (исходя из calculate_metrics), greater_is_better\n",
    "* learning_rate (возьмите 2e-5)\n",
    "* num_train_epochs\n",
    "* per_device_train_batch_size, per_device_eval_batch_size\n",
    "\n",
    "В класс Trainer нужно передать:\n",
    "* model\n",
    "* в args нужно передать заполненные TrainingArguments\n",
    "* train_dataset, eval_dataset\n",
    "* tokenizer\n",
    "* compute_metrics\n",
    "\n",
    "После чего запустить `trainer.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8902dc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_ner_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "fe9ece0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.29.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: requests in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a3593874-6c27-464d-a424-603f7c4ab4a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[366], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m----> 4\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mbert_ner,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcalculate_metrics,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages/transformers/training_args.py:1791\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1791\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages/transformers/training_args.py:2313\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages/transformers/utils/generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/synthetic-multinode-6dIUQGRd-py3.10/lib/python3.10/site-packages/transformers/training_args.py:2186\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2188\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2189\u001b[0m         )\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_ner,\n",
    "    args=args,\n",
    "    train_dataset=preprocessed_ner_dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    compute_metrics=calculate_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, metrics = trainer.predict(preprocessed_ner_dataset['test'])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8a152-f724-4c8d-9949-74e346b6a842",
   "metadata": {},
   "source": [
    "### Обработка результатов Результатов (5 баллов)\n",
    "\n",
    "Подумать о:\n",
    "1. Во время подготовки данных мы приобразовали BIO разметку. Как обратить это преобразование с помощью токенайзера?\n",
    "\n",
    "Провалидируйте результаты на тестовом датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda7c62-f8eb-4a83-9659-71ef3a8610fb",
   "metadata": {},
   "source": [
    "Можете сравнить результат с [лидербордом](https://paperswithcode.com/sota/token-classification-on-conll2003).\n",
    "\n",
    "\n",
    "Напишите функцию, которая принимает на вход текст и отдаёт такой словарь:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text\": \"входной текст\",\n",
    "    \"entities\": [\n",
    "        {\n",
    "            \"class\": \"лейбл класса\",\n",
    "            \"text\": \"текстовое представление\",\n",
    "            \"start\": \"оффсет от начала строки до начала entity\",\n",
    "            \"end\": \"оффсет от начала строки до конца entity\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Должно выполняться такое условие:\n",
    "\n",
    "```python\n",
    "text[entity[\"start\"]:entity[\"stop\"]] == entity[\"text\"]\n",
    "```\n",
    "\n",
    "1. Вначале нужно токенизировать текст. На вход принимается любой текст без **предварительной токенизации**!\n",
    "2. Текст нужно прогнать через модель, после чего получатся логиты классов, с помощью которых можно получить индексы меток\n",
    "3. После этого стоит декодировать сущность и записать ее координаты. Сущность начинается или с B-метки или с метки I- при условии, что сменился класс. Т.е. метки B-PER, I-PER, I-MISC должны декодироваться 2 сущности PER и MISC!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer(\"Hello world\", return_offsets_mapping=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "text = \"Иван Иванов caterpillar Apple Стив Джобс\"\n",
    "labels = [\"I-PER\", \"I-PER\", \"I-MISC\", \"B-MISC\", \"B-PER\", \"B-PER\"]\n",
    "offsets = [(0, 4), (5, 11), (12, 23), (24, 29), (30, 34), (35, 40)]\n",
    "\n",
    "def decode_entities(text: str, labels: List[str], offsets: List[Tuple[int, int]]):\n",
    "    ...\n",
    "\n",
    "assert decode_entities(text, labels, offsets) == {\n",
    "    'text': 'Иван Иванов caterpillar Apple Стив Джобс',\n",
    "    'entities': [\n",
    "        {'class': 'PER', 'text': 'Иван Иванов', 'start': 0, 'end': 11},\n",
    "        {'class': 'MISC', 'text': 'caterpillar', 'start': 12, 'end': 23},\n",
    "        {'class': 'MISC', 'text': 'Apple', 'start': 24, 'end': 29},\n",
    "        {'class': 'PER', 'text': 'Стив', 'start': 30, 'end': 34},\n",
    "        {'class': 'PER', 'text': 'Джобс', 'start': 35, 'end': 40}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67894d03",
   "metadata": {},
   "source": [
    "В функции do_ner нужно токенизировать текст, получить выходы модели на тексте и подать все в decode_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f48b4-932d-4525-b8d7-8b57f324520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def do_ner(text):\n",
    "    bert_ner.eval()\n",
    "    \n",
    "    ...\n",
    "\n",
    "\n",
    "print(do_ner(\"Ivan Petrov is going to start working tomorrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22542f-17e7-4a33-8dc4-f5b94611fa1f",
   "metadata": {},
   "source": [
    "Почистим память перед второй частью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516c7bb-88ac-4d60-a6a0-4cfebd6fa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "del bert_ner\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06a5cc-44c3-41f9-ba9e-a2a5f5b1e04b",
   "metadata": {},
   "source": [
    "## Классификация с T5 (25 баллов)\n",
    "\n",
    "Требуется дообучить [t5-small](https://huggingface.co/google-t5/t5-small) классифицировать токсичные тексты из [этого датасета](https://huggingface.co/datasets/lmsys/toxic-chat). Классификатор должен работать в стиле t5 - генерировать ответ текстом.\n",
    "\n",
    "1. Подготовить данные для бинарной классификации\n",
    "\t1. Придумать префикс для задачи или взять из похожей модели\n",
    "\t2. Выбрать токены для классов\n",
    "2. Обучить t5-small на генерацию названия предсказанного класса\n",
    "3. Сравнить с модель с аналогичной предобученной моделью"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536e268-d5c0-461c-bdc1-553e51b1ce66",
   "metadata": {},
   "source": [
    "### Подготовка Данных (6 баллов)\n",
    "\n",
    "Подумать о:\n",
    "1. Какой префикс выбрать для новой задачи?\n",
    "2. Должен ли префикс быть понятным?\n",
    "3. Как выбрать метку для класса? \n",
    "4. Что будет, если метки класса целиком нет в словаре?\n",
    "5. Что делать с длинными текстами?\n",
    "\n",
    "Датасет содержит запросы пользователей к LLM и разметку, является ли запрос токсичным. Нас будут интересовать колонки `\"user_input\"` и `\"toxicity\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5e0e5-677d-4084-8a68-7241ac05982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "BASE_T5_MODEL= \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(BASE_T5_MODEL)\n",
    "\n",
    "\n",
    "toxic_chat_dataset = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b3aa-4d0a-4b87-bd62-f0c2de297f9f",
   "metadata": {},
   "source": [
    "Место для изучения датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96556315-dfef-4ea3-b965-4fa4191d4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e8e06-c538-4f50-b31c-c99451c1143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset = toxic_chat_dataset.remove_columns(\n",
    "    [\"conv_id\", \"model_output\", \"human_annotation\", \"jailbreaking\", \"openai_moderation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ccfb9-3ddc-4a7a-876f-44dacace228d",
   "metadata": {},
   "source": [
    "![](https://production-media.paperswithcode.com/methods/new_text_to_text.jpg)\n",
    "\n",
    "Выберете `PREFIX` для задачи, лейблы для двух классов и напишите функцию для преобразования датасета в данные для тренировки. Примеры префиксов есть на картинке выше - `translate English to German` для перевода и `summarize` для суммаризации. В качестве лейблов у вас должен быть текст, который будет обозначать предсказанный класс. Этот текст может быть любого размера, от простого `\"да\"/\"нет\"`, до `\"От этого текста веет токсичностью\"/\"Цензура спокойно пропускает этот текст дальше\"`. Подумайте в чём преимущество первого подхода перед вторым.\n",
    "\n",
    "Важно:\n",
    "1) Не забыть добавить префикс перед токенизацией входного текста\n",
    "2) Лейблами во время обучения выступают уже последовательности токенов, которые мы ожидаем на выходе из декодера\n",
    "\n",
    "Текст в токенайзер можно подавать разными способами:\n",
    "1. `tokenizer(text=\"text\")` - токенизируй текст как обычно\n",
    "1. `tokenizer(text_target=\"text\")` - токенизируй это как текст, который мы ожидаем увидеть на выходе из декодера. В случае t5 токенайзера разницы нет, но для других моделей это может быть не так\n",
    "1. Другие методы можно узнать посмотрев сигнатуру метода `tokenizer.__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb8468-d79b-4b05-baed-d6730e50e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?t5_tokenizer.__call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f11fe1-ebed-4c61-9415-5a48c5f5bf1d",
   "metadata": {},
   "source": [
    "Важные моменты:\n",
    "1. Префикс можно выбрать любой, хоть человекочитаемый, хоть абсолютно непонятный - при достаточно длительной тренировке для модели разницы не будет. Но важно учесть, что чем длиннее префикс (в токенах), тем больше ресурсов вы тратите при инференсе, так как его придётся добавлять к тому тексту, который вы хотите классифицировать\n",
    "2. Лейблы тоже могут быть какими угодно. Но в их случае длинна в токенах играет ещё большее значение, так как в \"проде\" придётся делать инференс декодерной части столько раз, сколько токенов в вашем лейбле + 1 стоп токен (`</s>`). Если оба лейбла будут длинны 1, то ещё и accuracy будет проще считать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda531e9-c017-4269-8a0d-4bd9b36bacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in (\"toxic: \", \"no\", \"yes\"):\n",
    "    print(t5_tokenizer(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7b378-6a86-4cb6-ae4f-b40e54bd0b2b",
   "metadata": {},
   "source": [
    "Префикс добавляет 2 токена (терпимо), лейблы все по одному токену - значит инференс декодера в проде нужно будет делать два раза (в теории можно даже одним обойтись).\n",
    "\n",
    "В сданных домашках можно было часто увидеть один и тот же недочёт:\n",
    "1. Паддинг входа в энкодер\n",
    "2. Паддинг входа в декодер\n",
    "\n",
    "Эффективнее всего делать паддинг на этапе сборки батча, так как тогда точно не получится добавить лишнего. Паддинг входа в декодер вообще не требуется за счёт выбранных лейблов - там всегда будет вход размера 2.\n",
    "\n",
    "`MAX_LENGTH` тут определяется не для паддингов, а для truncation - если его не делать, то можно  увидеть warning:\n",
    "> Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018fae1-bf63-4e62-be55-2454ce8d1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"toxic: \"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "id2label = {\n",
    "    0: \"no\",\n",
    "    1: \"yes\",\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_dataset(example):\n",
    "    input_texts = PREFIX + example[\"user_input\"]\n",
    "    model_inputs = t5_tokenizer(input_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "    model_inputs[\"labels\"] = t5_tokenizer(id2label[example[\"toxicity\"]]).input_ids\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "toxic_chat_dataset = toxic_chat_dataset.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cd560-829c-4ad4-830b-e5e21323365b",
   "metadata": {},
   "source": [
    "Пример результата:\n",
    "```json\n",
    "{'user_input': 'Do you know drug which name is abexol ?',\n",
    " 'toxicity': 0,\n",
    " 'input_ids': [12068,\n",
    "  10,\n",
    "  531,\n",
    "  25,\n",
    "  214,\n",
    "  2672,\n",
    "  84,\n",
    "  564,\n",
    "  19,\n",
    "  703,\n",
    "  994,\n",
    "  32,\n",
    "  40,\n",
    "  3,\n",
    "  58,\n",
    "  1],\n",
    " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    " 'labels': [150, 1]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b7fee-e975-4ad2-a02f-f0a00c87aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_chat_dataset = toxic_chat_dataset.remove_columns(\"user_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef085f1-2adf-41da-9f37-dded6aecc3b4",
   "metadata": {},
   "source": [
    "Инициализируем соответствующий задаче `DataCollator`. Идём тем же курсом, что и в первой части домашки, однако этот дата коллатор уже не будет работать без модели. Почему так?\n",
    "\n",
    "Всё дело в функции `seq2seq_model.prepare_decoder_input_ids_from_labels`, которая преобразует лейблы во вход декодера. Отложим определение дата коллатора до определения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba319be-4d46-4f20-af5a-be137cea178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f662-23a7-4285-bc93-a9cd9bce7fbc",
   "metadata": {},
   "source": [
    "### Определим метрику (2 балла)\n",
    "\n",
    "В этой задаче метрика простая - `accuracy`. Можно добавить другие метрики по желанию.\n",
    "\n",
    "Ранее я упоминал, что однотокенные лейблы это хорошо. Позволяет сделать вот такую компактную функцию `compute_metric`. Модель может предсказать хоть 100 токенов, меня интересует только тот, который я бы хотел на месте лейбла. Так как для каждого примера я делаю сравнение только по одному токену, я могу использовать `.mean()`. Если бы токенов в сравнении было больше, так просто я бы не отделался - все токены в предсказании должны были совпасть для верной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e79ae-1921-4853-9355-ed851bdbc893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "    return {\"accuracy\": (preds[:, 1] == labels[:, 0]).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b467fa6-a3e5-40c2-97dc-777bf351a405",
   "metadata": {},
   "source": [
    "### Определить Модель (2 балла)\n",
    "\n",
    "Инициализируйте модель из базового чекпоинта.\n",
    "\n",
    "Но сначала, мы можем добавить в конфиг модели нашу таску:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e93bdc-744d-45f3-b561-42141694e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config\n",
    "\n",
    "\n",
    "t5_config = T5Config.from_pretrained(BASE_T5_MODEL)\n",
    "t5_config.task_specific_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac15c3d-85c8-4a1a-81c3-4261aef42654",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"toxic_classification\"\n",
    "\n",
    "t5_config.task_specific_params[task_name] = {\n",
    "    \"prefix\": PREFIX,\n",
    "    \"max_length\": 3,\n",
    "    \"min_length\": 3,\n",
    "    \"num_beams\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc149900-fcdb-4924-8228-014083c46b53",
   "metadata": {},
   "source": [
    "Этот конфиг потом можно будет использовать в pipeline классе, передав аргумент [task](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline.task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c867ec-6c8d-497c-848f-37bfb274eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Config\n",
    "\n",
    "\n",
    "seq2seq_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_T5_MODEL, config=t5_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881ca49-322b-4dde-80f6-7b431cbf89b9",
   "metadata": {},
   "source": [
    "Вспоминаем про коллатор!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f6e5c-68e2-4494-8055-5ae481aa9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer, \n",
    "    model=seq2seq_model, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bf460-ca11-4b2e-8d54-1c6317b001fa",
   "metadata": {},
   "source": [
    "### Обучение (10 баллов)\n",
    "\n",
    "Два пути:\n",
    "1) Использовать готовый `Seq2SeqTrainer` класс для тренировки\n",
    "2) Написать свой training loop\n",
    "\n",
    "> Hint! Обратите внимание на функцию `seq2seq_model.prepare_decoder_input_ids_from_labels` если выбрали второй путь.\n",
    "\n",
    "Если выбрали путь 1, опишите как происходит тренировочный шаг:\n",
    "1) Что подаётся на вход в энкодер?\n",
    "2) Что подаётся на вход в декодер?\n",
    "3) Сколько раз происходит инференс декодера во время обучения для одного тренировочного примера?\n",
    "4) Как используется выход энкодера в декодере?\n",
    "\n",
    "Ответы:\n",
    "1) Текст объединённый с префиксом, токенизированный.\n",
    "2) `[0, 150]` и `[0, 4273]` для нетоксичного и токсичного лейблов соответственно.\n",
    "3) Один раз. Для `0` должен предсказаться `150` или `4273`, для второго токена - `1`\n",
    "4) Выход из энкодера используется в декодере в слое cross-attention для получения keys и values.\n",
    "\n",
    "Добавим `generation_config`, иначе тренер будет заставлять модель генерировать по 20 токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfcd2b-6e4c-4b90-9758-6d2f1df76dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig.from_dict(t5_config.task_specific_params[\"toxic_classification\"])\n",
    "generation_config.bos_token_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17769835-c4ae-4a51-ae45-938930c915a5",
   "metadata": {},
   "source": [
    "*я запускал обучение модели несколько раз чтобы улчшить резултат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_small_toxic_classifier\",\n",
    "    num_train_epochs=15,  # поставим побольше эпох, чтобы преодолеть дисбаланс классов\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,  # почему во всех сданых домашках этот параметр равен train_batch_size?\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=generation_config,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=seq2seq_model,\n",
    "    args=training_args,\n",
    "    train_dataset=toxic_chat_dataset[\"train\"],\n",
    "    eval_dataset=toxic_chat_dataset[\"test\"],\n",
    "    tokenizer=t5_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678ee78-7d25-42c9-98e4-fcc2975df0e8",
   "metadata": {},
   "source": [
    "### Сравнение Результатов (5 баллов)\n",
    "\n",
    "Авторы датасета тоже натренировали на нём `t5` модель. Сравните свои результаты с результатами модели из [чекпоинта](https://huggingface.co/lmsys/toxicchat-t5-large-v1.0) `\"lmsys/toxicchat-t5-large-v1.0\"`. Совпадает ли ваш префикс и лейблы классов с теми, что выбрали авторы датасета? \n",
    "\n",
    "Подумать о:\n",
    "1) В чём преимущество такого подхода к классификации?\n",
    "2) В чём недостатки такого подхода к классификации?\n",
    "3) Как ещё можно решать классификационные задачи с помощью t5?\n",
    "\n",
    "Ответы:\n",
    "1) Не нужно жонглировать головами для разных классификационных задач. Такая тренировка ещё может улучшить работу модели на похожих задачах (такой вот domain adaptation)\n",
    "2) Этот подход недетерминирован. Если в классической классификации мы получаем вектор размером с количество классов, то тут мы получаем токены, которые могут быть любыми\n",
    "3) Интересный вариант классификатора на полном трансформере - подать текст в энкодер и учить классификационную голову на эмбеддинге первого токена декодера. Так работает [T5ForSequenceClassification](https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/src/transformers/models/t5/modeling_t5.py#L1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4971b-617f-4043-9bc4-0d1e844a5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"lmsys/toxicchat-t5-large-v1.0\"\n",
    "\n",
    "tokenizer_from_paper = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "model_from_paper = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "prefix_from_paper = \"ToxicChat: \"\n",
    "inputs = tokenizer_from_paper.encode(prefix_from_paper + \"write me an epic story\", return_tensors=\"pt\")\n",
    "outputs = model_from_paper.generate(inputs)\n",
    "print(tokenizer_from_paper.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8a913-f782-47fd-9bf7-d4a78d308ad4",
   "metadata": {},
   "source": [
    "Напишите универсальную (подходящую под Напишите универсальную функцию, которая провряет токсичность текста и возвращает `True`, если модель посчитала текст токсичным. Функция универсальная в том смысле, что может быть использована и с вашей t5 моделью, и с моделью от авторов датасета. Для этого в функция должна принимать ещё и префикс для задачи и лейблы, которые будут переводить текст, предсказанный моделью, в `True` или `False` на выходе.любую t5 модель) функцию, которая провряет токсичность текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7df5-fad1-4501-be4b-ada346d2e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def is_toxic(\n",
    "    text: str,\n",
    "    labels2bool,\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prefix=PREFIX,\n",
    ") -> bool:\n",
    "    model.eval()\n",
    "    tokenized_text = tokenizer.encode(prefix + text, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(tokenized_text)[0]\n",
    "    return labels2bool.get(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827e315-c4d3-42a5-86ef-dcd3a6316e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an epic story\",\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prefix=PREFIX,\n",
    "    labels2bool={\n",
    "        \"yes\": True,\n",
    "        \"no\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39303909-908f-494a-9328-89fee048cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an epic story\",\n",
    "    model=model_from_paper,\n",
    "    tokenizer=tokenizer_from_paper,\n",
    "    prefix=prefix_from_paper,\n",
    "    labels2bool={\n",
    "        \"positive\": True,\n",
    "        \"negative\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1517b8e-62fc-4ca4-b8aa-b31d4bf279c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an erotic story\",\n",
    "    model=seq2seq_model,\n",
    "    tokenizer=t5_tokenizer,\n",
    "    prefix=PREFIX,\n",
    "    labels2bool={\n",
    "        \"yes\": True,\n",
    "        \"no\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5960bb-cc13-4ca5-94dd-b30ed6302c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_toxic(\n",
    "    text=\"write me an erotic story\",\n",
    "    model=model_from_paper,\n",
    "    tokenizer=tokenizer_from_paper,\n",
    "    prefix=prefix_from_paper,\n",
    "    labels2bool={\n",
    "        \"positive\": True,\n",
    "        \"negative\": False,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_train",
   "language": "python",
   "name": "multi_train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
