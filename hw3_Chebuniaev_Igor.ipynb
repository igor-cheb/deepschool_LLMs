{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnqvoOGqZ2oX"
   },
   "source": [
    "# Устанавливаем зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "DcuFItWyPbey",
    "outputId": "84740114-b3d2-4735-cb27-0eb055b12a15",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformer_lens in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (1.4.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (3.3.2)\n",
      "Requirement already satisfied: einops>=0.6.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.8.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.2.38)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (2.2.3)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (13.9.4)\n",
      "Requirement already satisfied: sentencepiece in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch>=2.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (2.6.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.43 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (4.49.0)\n",
      "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.0.5)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (4.4.2)\n",
      "Requirement already satisfied: typing-extensions in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformer_lens) (0.19.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (24.2)\n",
      "Requirement already satisfied: psutil in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.29.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.13)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
      "Requirement already satisfied: networkx in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.2->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers>=4.43->transformer_lens) (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.22.0)\n",
      "Requirement already satisfied: setproctitle in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jaxtyping in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (0.2.38)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jaxtyping) (0.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-22gr9clq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-22gr9clq\n",
      "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 1e6129d08cae7af9242d9ab5d3ed322dd44b4dd3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from circuitsvis==0.0.0) (5.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.23 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from circuitsvis==0.0.0) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from circuitsvis==0.0.0) (2.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.21.0)\n",
      "Requirement already satisfied: filelock in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from sympy==1.13.1->torch<3.0,>=2.0->circuitsvis==0.0.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformer_lens\n",
    "%pip install einops\n",
    "%pip install jaxtyping\n",
    "%pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAVtXXZiPJlr",
    "outputId": "68197107-bbe2-4a8c-ba7f-2584b8b81c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
    "import sys\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "import torch as t\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict, Callable\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import webbrowser\n",
    "\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "reference_gpt2 = reference_gpt2.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW8URqc6taDT"
   },
   "source": [
    "Конфиг, который хранит в себе всю информацию о размерностях модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjdS6H9pO9E0",
    "outputId": "4189dff2-96a5-41d3-d32b-9a2851395184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768 # он же hidden_dim - внутрення размерность модели\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257 # он же vocab_size, размер словаря модели\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024 # число позиционных эмбеддингов\n",
    "    d_head: int = 64 # размерность головы аттеншена\n",
    "    d_mlp: int = 3072 # внутренняя размерность FFN-слоя\n",
    "    n_heads: int = 12 # число голов аттеншена\n",
    "    n_layers: int = 12 # число слоев трансформера\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "ctCDi2ccPx-H"
   },
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    output = layer(input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    try: reference_output = gpt2_layer(input)\n",
    "    except: reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")\n",
    "\n",
    "def load_gpt2_test_attention(cls, gpt2_layer, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "\n",
    "    Q, K, V = layer._get_qkv(input)\n",
    "    assert (Q.shape == torch.Size([1, 35, 12, 64]) and\n",
    "        K.shape == torch.Size([1, 35, 12, 64]) and\n",
    "        V.shape == torch.Size([1, 35, 12, 64])), \"Wrong Q, K, V shapes\"\n",
    "\n",
    "    attention_scores = layer._get_attention_dotprod(Q, K)\n",
    "    assert attention_scores.shape == torch.Size([1, 12, 35, 35]), \"Wrong Q * K.T shape\"\n",
    "\n",
    "    attn_probs = layer._get_attention_scores(attention_scores)\n",
    "    assert attn_probs.shape == torch.Size([1, 12, 35, 35]), \"Wrong attn_probs shapes\"\n",
    "    assert torch.isclose(torch.ones(torch.Size([1, 12, 35])).to(device), attn_probs.sum(dim=-1), atol=1e-4, rtol=1e-3).all(), \"attn_probs must sum up to ones along proba dimensions\"\n",
    "\n",
    "    res = layer._get_final_projection(V, attn_probs)\n",
    "    assert res.shape == torch.Size([1, 35, 768]), \"Wrong attention result shape\"\n",
    "\n",
    "    print(\"All attention substeps are correct\\n\")\n",
    "\n",
    "    load_gpt2_test(cls, gpt2_layer, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSNvnHYaQrqZ",
    "outputId": "f589db12-b4db-4780-f95c-700a173ff089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]], device='cuda:0')\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0L4t5KL8XY5",
    "outputId": "6a3bf701-e785-4ded-b8d6-af587afacb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "Все работает, мы готовы к выполнению задания!\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(logits.shape)\n",
    "\n",
    "print(\"Все работает, мы готовы к выполнению задания!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kIRmiHZ87sw"
   },
   "source": [
    "# Embeddings - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ogi6zcV9taDV"
   },
   "source": [
    "Здесь нам даются токены размерности `[batch_size, seq_len]` - индексы слов в словаре. Нужно описать слой Embed, который будет отображать каждый токен в соответствующий вектор из матрицы эмбеддингов. Таким образом каждому токену предоставляется вектор, который будет иметь размерности `[batch_size, seq_len, d_model]`\n",
    "\n",
    "Внимание - здесь не нужно исользовать цикл for и проходиться по матрице. Все стандартные операции доступны в [документации](https://pytorch.org/docs/stable/nn.functional.html), в частности тут нам понадобится одна из операций в секции [sparse functions](https://pytorch.org/docs/stable/nn.functional.html#sparse-functions).\n",
    "\n",
    "Важное замечание - на самом деле этот слой уже есть [готовый в pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), но мы в учебных целях переписываем его сами.\n",
    "\n",
    "\n",
    "Также можно решить этот пример через индексацию или через einops.\n",
    "\n",
    "**Вообще почти во всех примерах есть несколько возможных стилей описания операций над тензорами - через torch.nn.functional, через различные индексации и трюки pytorch, через einops - можно делать любым удобным способом!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srX7Xj8FaMHX",
    "outputId": "6d41a4e3-0145-4614-b3ab-415da9138f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        return torch.nn.functional.embedding(input=input_ids, weight=self.W_E)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "rand_int_test(Embed, [batch_size, seq_len])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmitTuesEM3_"
   },
   "source": [
    "# Positional Embeddings - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwvFNnzAtaDV"
   },
   "source": [
    "В трансформерах есть не только обычные эмбеддинги, которые отвечают за \"смысл\" токенов, но и позиционные эмбеддинги! Вход у них такой же, как и у обычных эмбеддингов, только они должны эмбеддить позиции токенов, а не сами токены. Т.е. в матрице W_pos хранятся не эмбеддинги токенов, а эмбеддинги позиций.\n",
    "\n",
    "Поэтому в этом слое нужно:\n",
    "1. По tokens получить тензор positions размера `[batch_size, seq_len]`\n",
    "2. Заэмбеддить тензор positions, как в предыдущем слое.\n",
    "\n",
    "Важно - как и в предыдущем случае, для этот слой обычно используется через [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trB2m2P8Rgrk",
    "outputId": "2d85d0a5-c5df-48b6-a0ff-c5041af2240b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        position_ixs = torch.arange(start=0, end=input_ids.shape[1], device=device).repeat(input_ids.shape[0], 1).long()\n",
    "        out = torch.nn.functional.embedding(input=position_ixs, weight=self.W_pos)\n",
    "        return out \n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "rand_int_test(PosEmbed, [batch_size, seq_len])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G50AieI-taDW"
   },
   "source": [
    "# LM head - 5 баллов\n",
    "\n",
    "Финальный слой. У нас есть выходы из трансформера размерности `[batch_size, seq_len, d_model]`. Это контекстуализированные представления каждого токена. По ним мы предсказываем следующий токен, т.е. применяем линейный слой - умножаем на матрицу `[d_model, vocab_size]`.\n",
    "\n",
    "В этом нам поможет секция [linear functions](https://pytorch.org/docs/stable/nn.functional.html#linear-functions). Не забудьте про bias!\n",
    "\n",
    "В pytorch этот слой тоже есть - [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLXcAsSMU58C",
    "outputId": "55ac160a-4913-47a0-f75f-d2d864a14e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LM_head, но для совместимости с библиотекой для проверки пришлось назвать его Unembed\n",
    "# по аналогии с тем, что мы из индексов в словаре получаем эмбеддинги, а тут из эмбеддингов обратно\n",
    "# распределение по словарю\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # Ваш код здесь!\n",
    "        return  torch.nn.functional.linear(input=x, weight=self.W_U.T, bias=self.b_U)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 768\n",
    "rand_float_test(Unembed, [batch_size, seq_len, d_model])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfxZG2l7A-wQ"
   },
   "source": [
    "# Attention Формулы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30Q7cmNtFudx"
   },
   "source": [
    "1. **Входные эмбеддинги**:\n",
    "   $$X \\in \\mathbb{R}^{seq \\times d} $$\n",
    "2. **Маскированный мультихед-аттеншен (Masked Multi-Head Attention)**:\n",
    "$$M = \\begin{cases}\n",
    " &  m_{ij} = -\\infty, \\quad i < j \\\\\n",
    " &  m_{ij} = 0\n",
    "\\end{cases} $$\n",
    "\n",
    "$$\n",
    "M = \\begin{pmatrix}\n",
    "0 & -\\infty & -\\infty & \\ldots & -\\infty \\\\\n",
    "0 & 0 & -\\infty & \\ldots & -\\infty \\\\\n",
    "0 & 0 & 0 & \\ldots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FwfAHyzQ1Bs"
   },
   "source": [
    "3. Для каждой головы $ h_i $:\n",
    "\n",
    "    3.1 **Матрицы весов для запросов, ключей и значений**:\n",
    "     - $ W_Q \\in \\mathbb{R}^{d \\times d_h} $\n",
    "     - $ W_K \\in \\mathbb{R}^{d \\times d_h} $\n",
    "     - $ W_V \\in \\mathbb{R}^{d \\times d_h} $\n",
    "     \n",
    "    3.2. **Запросы, ключи и значения**:\n",
    "     - $ Q = X W_Q \\in \\mathbb{R}^{seq \\times d_h} $\n",
    "     - $ K = X W_K \\in \\mathbb{R}^{seq \\times d_h} $\n",
    "     - $ V = X W_V \\in \\mathbb{R}^{seq \\times d_h} $\n",
    "\n",
    "    3.3. **Скалярные произведения запросов и ключей**:\n",
    "     - $ \\frac{Q K^T}{\\sqrt{d_h}} + M \\in \\mathbb{R}^{seq \\times seq} $\n",
    "\n",
    "    3.4. **Веса внимания**:\n",
    "     - $ \\alpha = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_h}} + M\\right) \\in \\mathbb{R}^{seq \\times seq} $\n",
    "\n",
    "    3.5. **Агрегация значений**:\n",
    "     - $ z = \\alpha V \\in \\mathbb{R}^{seq \\times d_h} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEuVgMeMR6ce"
   },
   "source": [
    "4. **Конкатенация выходов всех голов**:\n",
    "   - $ Z = \\text{Concat}(z_1, z_2, \\ldots, z_h) \\in \\mathbb{R}^{seq \\times d} $\n",
    "\n",
    "5. **Выходной линейный слой**:\n",
    "   - Матрица весов: $ W^O \\in \\mathbb{R}^{d \\times d} $\n",
    "   - Итоговый выход: $ O = Z W^O \\in \\mathbb{R}^{seq \\times d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAXH6U-ktaDX"
   },
   "source": [
    "# Attention Алгоритм\n",
    "Самое сложное в этом домашнем задании - подсчет механизма внимания. Как и в предыдущих вариантах, считать можно через torch или с помощью einops и любыми другими удобными способами.\n",
    "\n",
    "\n",
    "В данном задании нужно реализовать multihead attention с маскированием. Давайте разбираться по шагам, что нам нужно сделать.\n",
    "\n",
    "Далее будет описан один из возможных алогритмов написания аттеншена, но повторимся - писать можно любым удобным способом (голый torch или einops).\n",
    "\n",
    "1. Нам попадает на вход вектор x `[batch, seq_len, d_model]`. Нужно превратить его в матрицы проекций i-й головы аттеншена: Q_i, K_i, V_i. Для этого у нас есть матрицы W_Q, W_K, W_V (и их bias!). Это набор n_heads матриц размеров `[d_model, d_head]`. Зачастую число голов n_head и d_head подобраны так, что d_model == n_head * d_head, наш случай не исключение. Предлагается перевести (этот шаг сделан) матрицу `[num_heads, d_model, d_head]` в матрицу `[d_model, num_heads * d_head]` = `[d_model, d_model]`, после чего получить через матричное умножение на X размерности `[batch_size, seq_len, d_model]` получить матрицы Q, K, V размерностей `[batch_size, seq_len, d_model] = [batch_size, seq_len, num_heads * d_head]` и преобразовать их к виду `[batch_size, seq_len, num_heads, d_head]`. **Не забудьте при матричном умножении транспонировать матрицы W_Q, W_K, W_V, если пойдете этим путем!** В качестве шпаргалки посмотрите, как происходило умножение в lm_head!\n",
    "\n",
    "2. После этого можно сделать первый шаг и посчитать attention_scores, т.е. домножить $Q \\times K^T$. Тут нам поможет .transpose или .permute вместе с torch.matmul. Нужно переставить размерности матриц таким образом, чтобы финальное матричное умножение происходило по двум последним размерностям `[seq_len, d_head]` на `[d_head, seq_len]`, а все предыдущие размерности `[batch_size, num_heads]` совпадали\n",
    "\n",
    "\n",
    "3. Не забудем нормализацию, т.е. делим attention_scores на sqrt(d_head)\n",
    "4. Теперь нужно использовать маскирование! В данном задании предполагается, что у нас нет паддингов, поэтому нам нужно наложить маску с одним простым условием: i-й элемент не может смотреть на j-й элемент, если j > i. Это треугольная маска, с ней нам поможет приведение треугольной форме, которое вам предлагается найти в pytorch! Замаскированные значения нужно заполнить каким-нибудь большим по модулю отрицательным числом В классе уже опредеелно значение IGNORE, можно использовать его. Для этого реализуйте и используйте функцию `apply_causal_mask`. Заполнять значениями можно через индексацию, например через `torch.masked_fill`.\n",
    "\n",
    "5. Теперь к замаскированным attention_scores `[batch_size, num_heads, seq_len, seq_len]` нужно применить softmax. Подумайте, по какой размерности его применять и на что это повлияет.\n",
    "\n",
    "6. После этого остается последнее матричное умножение softmax(attention_scores) на V, к которому тоже придется применить .view, .permute и torch.matmul\n",
    "\n",
    "7. Теперь, если вы следовали этому плану у вас остается матрица `ouput` размерностей `[batch_size, num_heads, seq_len, d_head]`. С помощью permute и view собираем (конкатенируем) ее обратно в матрицу `[batch_size, seq_len, num_heads * d_head] = [batch_size, seq_len, d_model]` и применяем к ней выходной линейный слой W_O. Всё, аттеншен готов!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc5WmLVCESEt"
   },
   "source": [
    "# Attention - 5 баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TH0LmW3CtaDY",
    "outputId": "233ee87c-8cca-430a-c4f8-e2f2fb8f1868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
    "\n",
    "    def _get_qkv(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Tuple[Float[Tensor, \"batch seq_len num_heads d_head\"]]:\n",
    "        \"\"\"1. Трансформируем матрицы проекций в формат [d_model, d_model] и получаем проекции  Q, K, V\"\"\"\n",
    "        # Берем размерности\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_heads = self.cfg.n_heads\n",
    "        d_head = self.cfg.d_head\n",
    "\n",
    "        W_Q = self.W_Q.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_K = self.W_K.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_V = self.W_V.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "\n",
    "        b_Q = self.b_Q.view(-1)\n",
    "        b_K = self.b_K.view(-1)\n",
    "        b_V = self.b_V.view(-1)\n",
    "\n",
    "\n",
    "        Q = torch.nn.functional.linear(input=x, weight=W_Q.T, bias=b_Q).view(batch_size, seq_len, num_heads, d_head)\n",
    "        K = torch.nn.functional.linear(input=x, weight=W_K.T, bias=b_K).view(batch_size, seq_len, num_heads, d_head)\n",
    "        V = torch.nn.functional.linear(input=x, weight=W_V.T, bias=b_V).view(batch_size, seq_len, num_heads, d_head)\n",
    "\n",
    "        return Q, K, V\n",
    "\n",
    "    def _get_attention_dotprod(\n",
    "        self,\n",
    "        Q: Float[Tensor, \"batch seq_len num_heads d_head\"],\n",
    "        K: Float[Tensor, \"batch seq_len num_heads d_head\"]\n",
    "    ) -> Float[Tensor, \"batch num_heads seq_len seq_len\"]:\n",
    "        \"\"\"Q x K^T\"\"\"\n",
    "        # Ваш код здесь\n",
    "        return torch.matmul(input=Q.permute(0, 2, 1, 3), other=K.permute(0, 2, 3, 1))\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads seq_len seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads seq_len seq_len\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        Используем треугольную маску, чтобы не смотреть в будущее!\n",
    "        В качестве масикировочного значения перед софтмаксом можно использовать self.IGNORE (-inf)\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        fill_ixs = torch.triu_indices(attn_scores.shape[-2], attn_scores.shape[-1], 1)\n",
    "        attn_scores[:, :, fill_ixs[0], fill_ixs[1]] = self.IGNORE\n",
    "        return attn_scores\n",
    "        \n",
    "    def _get_attention_scores(\n",
    "        self,\n",
    "        attention_scores: Float[Tensor, \"batch num_heads seq_len seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch num_heads seq_len seq_len\"]:\n",
    "        \"\"\"\n",
    "        Нормализация, маскирование и softmax\n",
    "        Для маскирования используйте и допишите функцию apply_causal_mask\n",
    "        \n",
    "        \"\"\"\n",
    "        #Нормализация\n",
    "        attention_scores = attention_scores/ torch.sqrt(torch.tensor(self.cfg.d_head))\n",
    "        # маскирование\n",
    "        attention_scores = self.apply_causal_mask(attention_scores)\n",
    "        # softmax\n",
    "        attention_scores = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "        return attention_scores\n",
    "\n",
    "    def _get_final_projection(\n",
    "        self,\n",
    "        V: Float[Tensor, \"batch seq_len num_heads d_head\"],\n",
    "        attn_probs: Float[Tensor, \"batch num_heads seq_len seq_len\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        \"\"\"Финальная проекция\n",
    "        permute [ batch, num_heads, seq_len, d_head]\"\"\"\n",
    "        batch_size, seq_len = V.shape[0], V.shape[1]\n",
    "        d_model = self.cfg.d_model\n",
    "        num_heads = self.cfg.n_heads\n",
    "        d_head = self.cfg.d_head\n",
    "        output = torch.matmul(input=attn_probs, other=V.permute(0, 2, 1, 3))\n",
    "        output_inverse_permute = output.permute(0, 2, 1, 3)\n",
    "        output = output_inverse_permute.reshape(batch_size, seq_len, num_heads * d_head)\n",
    "        # return (output @ self.W_O.view(self.cfg.d_model, self.cfg.d_model))\n",
    "        return torch.nn.functional.linear(input=output, weight=self.W_O.view(self.cfg.d_model, self.cfg.d_model).T)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # 1. получаем проекции  Q, K, V\n",
    "        Q, K, V = self._get_qkv(x)\n",
    "        # 2. Q x K^T\n",
    "        attention_scores = self._get_attention_dotprod(Q, K)\n",
    "\n",
    "        # 3. Нормализация, маскирование и softmax\n",
    "        attn_probs = self._get_attention_scores(attention_scores)\n",
    "\n",
    "        # 6. Финальная проекция\n",
    "        # permute [ batch, num_heads, seq_len, d_head]\n",
    "        res = self._get_final_projection(V, attn_probs)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "dump = json.load(open(\"attention_activations.json\"))\n",
    "torch.manual_seed(1)\n",
    "attn = Attention(cfg)\n",
    "x = torch.FloatTensor(dump[\"x\"])\n",
    "Q, K, V = attn._get_qkv(x)\n",
    "attention_scores = attn._get_attention_dotprod(Q, K)\n",
    "attn_probs = attn._get_attention_scores(attention_scores)\n",
    "res = attn._get_final_projection(V, attn_probs)\n",
    "\n",
    "\n",
    "assert torch.isclose(Q, torch.FloatTensor(dump[\"Q\"]), atol=1e-4, rtol=1e-3).all()\n",
    "assert torch.isclose(K, torch.FloatTensor(dump[\"K\"]), atol=1e-4, rtol=1e-3).all()\n",
    "assert torch.isclose(V, torch.FloatTensor(dump[\"V\"]), atol=1e-4, rtol=1e-3).all()\n",
    "\n",
    "assert torch.isclose(attention_scores, torch.FloatTensor(dump[\"attention_scores\"]), atol=1e-4, rtol=1e-3).all()\n",
    "assert torch.isclose(attn_probs, torch.FloatTensor(dump[\"attn_probs\"]), atol=1e-4, rtol=1e-3).all()\n",
    "assert torch.isclose(res, torch.FloatTensor(dump[\"res\"]), atol=1e-4, rtol=1e-3).all()\n",
    "\n",
    "print(\"All good\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3385, 0.3332, 0.3283, 0.0000],\n",
       "        [0.2558, 0.2567, 0.2608, 0.2267]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5QCIXQ1_3mr",
    "outputId": "57cf5b14-db40-4f64-9c92-ed22a12dfdb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!! torch.Size([1, 35, 768])\n",
      "!!!! torch.Size([768])\n",
      "All attention substeps are correct\n",
      "\n",
      "!!!! torch.Size([1, 35, 768])\n",
      "!!!! torch.Size([768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "0.17% of the values are correct\n",
      "\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "!!!! torch.Size([2, 4, 768])\n",
      "!!!! torch.Size([768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 768\n",
    "\n",
    "load_gpt2_test_attention(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])\n",
    "rand_float_test(Attention, [batch_size, seq_len, d_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZYloeArtaDY"
   },
   "source": [
    "Если вы справились с этим, то поздравляю - ничего сложнее мы сегодня уже не будем делать)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anGg23a4_aTy"
   },
   "source": [
    "# MLP - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRLuXGJb6NT2"
   },
   "source": [
    "Реализуем MLP слой - это 2 матричных умножения с нелинейностью GELU.\n",
    "\n",
    "- $$ \\text{MLP}(X) = (\\text{GeLU}(X W_1 + b_1)) W_2 + b_2 \\in \\mathbb{R}^{\\text{seq} \\times d}$$\n",
    "-    $$W_1 \\in \\mathbb{R}^{d \\times d_{mlp}}, \\quad b_1 \\in \\mathbb{R}^{d_{mlp}} \\\\\n",
    "W_2 \\in \\mathbb{R}^{d_{mlp} \\times d}, \\quad b_2 \\in \\mathbb{R}^{d} \\\\ $$\n",
    "\n",
    "\n",
    "$$GELU(X) = 0.5 * x * (1 + tanh(\\sqrt {\\frac {2} {\\pi}} * (x + 0.44715 * x^3)))$$\n",
    "\n",
    "если будете использовать gelu из pytorch, то **обязательно** проставьте approximate=\"tanh\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hQCIDevT0h3",
    "outputId": "f8b4636f-cbbd-485f-95e4-376c69205e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "rand_float_test(MLP, [batch_size, seq_len, d_model])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVAp7w1gR8tU"
   },
   "source": [
    "# Normalization - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aM9JUorIwwj"
   },
   "source": [
    "**Layer Normalization**:\n",
    "   - $ \\text{LayerNorm}(X) = \\frac{X - \\mu}{\\sigma} \\cdot \\gamma + \\beta $\n",
    "   - $\\mu = \\text{mean}(X, \\text{dim}=-1) \\in \\mathbb{R}^{d}$\n",
    "   - $\\sigma = \\sqrt{\\text{var}(X, \\text{dim}=-1) + \\epsilon} \\in \\mathbb{R}^{d}$\n",
    "   - $\\gamma \\in \\mathbb{R}^{d}$\n",
    "   - $\\beta \\in \\mathbb{R}^{d}$\n",
    "   \n",
    "   \n",
    "1. Супер важно! Не забудьте про эпсилон, который хранится в cfg!\n",
    "2. В [подсчете дисперсии](https://pytorch.org/docs/stable/generated/torch.var.html) не используете коррекцию Бесселя! Для этого в зависимости от версии pytorch поставьте `unbiased=False` или `correction=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnNVykkAP49l",
    "outputId": "f7150a76-8740-43bb-d636-1f75e7c07e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model)) # gamma\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model)) # beta\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9RPPHidUzRB"
   },
   "source": [
    "# Transformer Block - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3JLTYzhtaDa"
   },
   "source": [
    "Это блок трансформера, который получает на вход тензор x `[batch_size, seq_len, d_model]` и выдает тензор таких же размерностей. Блок GPT2 немного отличается от классического трансформера, который мы обучали.\n",
    "\n",
    "\n",
    "![image.png](https://camo.githubusercontent.com/ebd052b635f156d5d24224f25fa078d804156be51125cd6626b92d9f8b406bbb/68747470733a2f2f6c6f6e6570617469656e742d313235373934353937382e636f732e61702d6368656e6764752e6d7971636c6f75642e636f6d2f53656c656374696f6e5f3030312e706e67)\n",
    "\n",
    "\n",
    "GPT2 следует схеме PreLN, а \"классический\" трансформер схеме PostLN. **Реализовать нужно PreLN схему!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7fEX6-8VRjV",
    "outputId": "e8b64397-04b3-432b-faea-df35a77b17ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6PzPgRREjCQ"
   },
   "source": [
    "# Transformer - 5 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXqEaNh0taDa"
   },
   "source": [
    "Собираем все в один большой трансформер.\n",
    "1. Применяем эмбеддинги и позиционные эмбеддинги, складываем результаты\n",
    "2. Прогоняем в цикле через все блоки трансформера\n",
    "3. Применяем финальную нормализацию и lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdbjvO9sVLrk",
    "outputId": "22747aa5-359f-4407-ed18-ede33de53cea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RcNrSDzgVjQg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "demo_logits = demo_gpt2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyiHpWcvV2mn",
    "outputId": "805c6829-677e-4cf4-9b9c-62e71489066b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 4.5647\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.087911\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"],\n",
    "    tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "cb86c5cf03004ed3a93c8c2816cd2af5",
      "b1ac2759e9bc43968b03e6a9ea9079d3",
      "01d155e1affd4e728651929723f6f546",
      "6a3038523275473c917c07d76c709634",
      "659674bf3ce8449783d8847f20007ae4",
      "43bcabf0e8524ddabaaff7c5051ade34",
      "6bebfa6823cd4b44a49da06fee878f24",
      "7832c0bf11054a97b7e48222d1ced4a7",
      "a0defebd249e48f6a2e95391d0e496f4",
      "2c9a9babf88b4156b783b1a8fdb6bad8",
      "0b656f02e2bb4c968afd6f0b27927acc"
     ]
    },
    "id": "jFqu7OlPV685",
    "outputId": "2100b25a-2bd4-4459-c9ed-f9f16cf9ebf0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb86c5cf03004ed3a93c8c2816cd2af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "test_string = '''The Total Perspective Vortex derives its picture of the whole Universe on the principle of'''\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG0zGo4iJFMa"
   },
   "source": [
    "Если вы обнаружили, что генерация зацикливается - так и должно быть! Эту проблему мы будем решать с помощью сэмплирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HcRpbA1Epq2"
   },
   "source": [
    "# Сэмплирование - 10 баллов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK5fbWLLWwSS"
   },
   "source": [
    "\n",
    "1. **Temperature Sampling**:\n",
    "   - Применяется первым, поскольку изменение температуры изменяет масштабы логитов перед дальнейшими операциями.\n",
    "\n",
    "2. **Frequency Penalty**:\n",
    "   - Применяется следующим, чтобы учесть частоты токенов до того, как логиты будут обрезаны методами top-k или top-p.\n",
    "\n",
    "3. **Top-k Sampling**:\n",
    "   - Применяется после temperature sampling и frequency penalty, так как он отбирает фиксированное количество наиболее вероятных токенов.\n",
    "\n",
    "4. **Top-p (Nucleus Sampling)**:\n",
    "   - Применяется после top-k sampling, чтобы отфильтровать токены на основе совокупной вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgdKJfPfbR-J"
   },
   "source": [
    "Обозначим размер словаря для удобства $\\Sigma = vocab\\_size$\n",
    "\n",
    "Пусть $ \\text{logits} \\in \\mathbb{R}^{\\text{seq} \\times \\Sigma} $:\n",
    "\n",
    "1. **Temperature Sampling**:\n",
    "   $$\n",
    "   \\text{logits}'_{i,j} = \\frac{\\text{logits}_{i,j}}{T} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, |vocab_size|]\n",
    "   $$\n",
    "\n",
    "2. **Frequency Penalty**:\n",
    "   $$\n",
    "   \\text{penalty}(t_j) = \\alpha \\cdot f(t_j)\n",
    "   $$\n",
    "   $$\n",
    "   \\text{logits}''_{i,j} = \\text{logits}'_{i,j} - \\text{penalty}(t_j) \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "   $$\n",
    "\n",
    "4. **Top-k Sampling**:\n",
    "   $$\n",
    "   top\\_k\\_indices_i = \\text{argtop-k}(\\text{logits}''_i, k) \\quad \\forall \\ i \\in [1, \\text{seq}]\n",
    "   $$\n",
    "   $$\n",
    "   \\text{mask}_{i,j} =\n",
    "   \\begin{cases}\n",
    "   1 & \\text{если} \\ j \\in top\\_k\\_indices_i \\\\\n",
    "   0 & \\text{иначе}\n",
    "   \\end{cases} \\\\\n",
    "   \\text{logits}'''_{i,j} = \\text{logits}''_{i,j} \\cdot \\text{mask}_{i,j} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "   $$\n",
    "\n",
    "6. **Top-p (Nucleus Sampling)**:\n",
    "   $$\n",
    "   sorted\\_logits_i, sorted\\_indices_i = \\text{sort}(\\text{logits}'''_i, \\text{descending=True}) \\quad ∀ \\ i \\in [1, \\text{seq}]\n",
    "   $$\n",
    "   $$\n",
    "   probs_i = softmax(sorted\\_logits_i) \\quad\n",
    "   $$\n",
    "   $$\n",
    "    cumulative\\_probs_{i,j} = \\sum_{k=1}^{j} \\text{probs}_{i,k} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma\n",
    "    \\quad \\forall \\ i \\in [1, \\text{seq}]\n",
    "   $$\n",
    "   $$\n",
    "   top\\_p\\_mask_{i,j} =\n",
    "   \\begin{cases}\n",
    "   1, & cumulative\\_probs_{i,j} \\leq p \\\\\n",
    "   0 &\n",
    "   \\end{cases} \\\\\n",
    "   \\text{logits}^{\\text{final}}_{i,j} = sorted\\_logits_{i,j} \\cdot top\\_p\\_mask_{i,j} \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, \\Sigma]\n",
    "   $$\n",
    "\n",
    "8. **Softmax**:\n",
    "   $$\n",
    "   \\mathbf{probs}_{i,j} = \\text{softmax}(\\text{logits}^{\\text{final}}_{i,j}) \\quad \\forall \\ i \\in [1, \\text{seq}], \\ j \\in [1, |\\Sigma_t|]\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{probs}_{i,j} = \\frac{e^{\\text{logits}^{\\text{final}}_{i,j}}}{\\sum_{k=1}^{|\\Sigma_t|} e^{\\text{logits}^{\\text{final}}_{i,k}}}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KhLvuGw5UpOh"
   },
   "outputs": [],
   "source": [
    "model_cfg = Config()\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "tokenizer = reference_gpt2.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "opp-1FmGWgGn"
   },
   "outputs": [],
   "source": [
    "class TransformerSampler:\n",
    "\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        '''\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "        new tokens are chosen.\n",
    "        '''\n",
    "        if prompt == '':\n",
    "            prompt = '<|endoftext|>'\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").flatten().to(device)\n",
    "        input_ids = input_ids.view(1, -1)\n",
    "\n",
    "        for _ in range(max_tokens_generated):\n",
    "            logits = self.model(input_ids)\n",
    "            logits = logits[:, -1, :]\n",
    "            next_id = self.sample_next_token(input_ids[0], logits, **kwargs)\n",
    "            if next_id > self.cfg.d_vocab:\n",
    "                break\n",
    "            next_id = t.tensor([next_id]).to(device).unsqueeze(0)\n",
    "            input_ids = t.cat((input_ids, next_id), 1)\n",
    "\n",
    "        out = ''\n",
    "        for idx in input_ids:\n",
    "            out += tokenizer.decode(idx.cpu().squeeze())\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "        seed=None\n",
    "    ):\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            t.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return TransformerSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = TransformerSampler.apply_frequency_penalty(input_ids, logits, frequency_penalty)\n",
    "        if top_k > 0:\n",
    "            return TransformerSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return TransformerSampler.sample_top_p(logits, top_p)\n",
    "        return TransformerSampler.sample_basic(logits)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        '''\n",
    "        Returns the most likely token (as an int).\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(logits: Float[Tensor, \"d_vocab\"], temperature: float) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Applies temperature scaling to the logits.\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(input_ids: Int[Tensor, \"seq_len\"], logits: Float[Tensor, \"d_vocab\"], freq_penalty: float) -> Float[Tensor, \"d_vocab\"]:\n",
    "        '''\n",
    "        Applies a frequency penalty to the logits.\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        '''\n",
    "        Samples from the distribution defined by the logits.\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "        '''\n",
    "        Samples from the top k most likely tokens.\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        ...\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
    "        '''\n",
    "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "        '''\n",
    "        # Ваш код здесь\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tear0Nn0WjZY",
    "outputId": "0c02d4f8-4d23-4b5f-b9ee-f28179f3b13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
      "\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.'\n",
      "\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output!r}\\n\")\n",
    "\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3VYdChTdNLc",
    "outputId": "161167a5-a44d-4aa5-a1e8-7cbb797a06ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
      "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "logits = t.tensor([1, 2]).log()\n",
    "\n",
    "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "\n",
    "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0no5kj-dTos",
    "outputId": "a45ed3f2-bd97-445d-b997-7c128378bc62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = TransformerSampler.apply_frequency_penalty(input_ids.squeeze(), logits, 2.0)\n",
    "\n",
    "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
    "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "e6aeba87a9cd42da8b7c14cfd506d335",
      "897a1aaceb5f42b5beb0a1c2f59bd908",
      "da792b96c1b5459a88c47ea612646944",
      "79d4743c09a34eb6b27732f4b4410590",
      "599302975b7742d18b03f4e7034f530f",
      "ac6d043aa0a44c0fa0bce6602ff6c619",
      "f57326b1e46444f2b18731a45d9fb1ff",
      "8b9a2725c1bb4cefa23a6db1e4b3168e",
      "e4b4ffa6149b4dccacf651a5ebc33fa9",
      "ce00838211304457b185e2376a2fb063",
      "814e876d1ca04bf2a64d86c973c24864"
     ]
    },
    "id": "AjQi5zDMWtTJ",
    "outputId": "ad85ecf8-3c4f-4bee-b7c9-9b246da28a0c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6aeba87a9cd42da8b7c14cfd506d335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.6385, observed freq 0.6410\n",
      "OK\n",
      "Word: ' house' . Expected freq 0.3615, observed freq 0.3590\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Top-P test\n",
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_10pct = {\n",
    "    \" church\": 0.6384,\n",
    "    \" house\": 0.3615, # These are the two most likely tokens, and add up to >10%\n",
    "}\n",
    "top_10pct_sum = sum(expected_top_10pct.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 10000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.11)\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_10pct:\n",
    "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\")\n",
    "    print( \"OK\" if abs(observed_freq - expected_freq) < 0.01 else \"Try increasing N if this fails by a small amount.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "1bcbca5aa5fd4c8cb258bb80ae772cc6",
      "2c180133585748ff8b72c2a747262f91",
      "1a46345e26d5442d8904a7e7b0173179",
      "8efb32e29c8f462e804c870b4e1e091a",
      "10840eef43f142f6a6ffd0a98352ad69",
      "076b0176ad534435ae33a61439bc189f",
      "f71bf0c0777b46c7999f178659d49820",
      "79880fa5c18b49adbba8b7c213ad692f",
      "cf20c853f5b64040a4c68ae1d76f756d",
      "8fae9e3bb55b4572b69a78ff2e510f1b",
      "c2a1d2155ab4423c9ff662a182d3bd5d"
     ]
    },
    "id": "QqA2OquDFf69",
    "outputId": "363a125f-ce1c-41a6-898d-2e89fab1115e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcbca5aa5fd4c8cb258bb80ae772cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.6385, observed freq 0.6325\n",
      "OK\n",
      "Word: ' house' . Expected freq 0.3615, observed freq 0.3675\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Top-K test\n",
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_10pct = {\n",
    "    \" church\": 0.6384,\n",
    "    \" house\": 0.3615, # These are the two most likely tokens, and add up to >10%\n",
    "}\n",
    "top_10pct_sum = sum(expected_top_10pct.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 10000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_k=2)\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_10pct:\n",
    "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\")\n",
    "    print( \"OK\" if abs(observed_freq - expected_freq) < 0.01 else \"Try increasing N if this fails by a small amount.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2oYGd-wCz-O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rami_hw",
   "language": "python",
   "name": "rami_hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01d155e1affd4e728651929723f6f546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7832c0bf11054a97b7e48222d1ced4a7",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0defebd249e48f6a2e95391d0e496f4",
      "value": 100
     }
    },
    "076b0176ad534435ae33a61439bc189f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b656f02e2bb4c968afd6f0b27927acc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10840eef43f142f6a6ffd0a98352ad69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a46345e26d5442d8904a7e7b0173179": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79880fa5c18b49adbba8b7c213ad692f",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf20c853f5b64040a4c68ae1d76f756d",
      "value": 10000
     }
    },
    "1bcbca5aa5fd4c8cb258bb80ae772cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c180133585748ff8b72c2a747262f91",
       "IPY_MODEL_1a46345e26d5442d8904a7e7b0173179",
       "IPY_MODEL_8efb32e29c8f462e804c870b4e1e091a"
      ],
      "layout": "IPY_MODEL_10840eef43f142f6a6ffd0a98352ad69"
     }
    },
    "2c180133585748ff8b72c2a747262f91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_076b0176ad534435ae33a61439bc189f",
      "placeholder": "​",
      "style": "IPY_MODEL_f71bf0c0777b46c7999f178659d49820",
      "value": "100%"
     }
    },
    "2c9a9babf88b4156b783b1a8fdb6bad8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43bcabf0e8524ddabaaff7c5051ade34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "599302975b7742d18b03f4e7034f530f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "659674bf3ce8449783d8847f20007ae4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a3038523275473c917c07d76c709634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c9a9babf88b4156b783b1a8fdb6bad8",
      "placeholder": "​",
      "style": "IPY_MODEL_0b656f02e2bb4c968afd6f0b27927acc",
      "value": " 100/100 [00:43&lt;00:00,  1.66it/s]"
     }
    },
    "6bebfa6823cd4b44a49da06fee878f24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7832c0bf11054a97b7e48222d1ced4a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79880fa5c18b49adbba8b7c213ad692f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79d4743c09a34eb6b27732f4b4410590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce00838211304457b185e2376a2fb063",
      "placeholder": "​",
      "style": "IPY_MODEL_814e876d1ca04bf2a64d86c973c24864",
      "value": " 10000/10000 [01:04&lt;00:00, 144.45it/s]"
     }
    },
    "814e876d1ca04bf2a64d86c973c24864": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "897a1aaceb5f42b5beb0a1c2f59bd908": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac6d043aa0a44c0fa0bce6602ff6c619",
      "placeholder": "​",
      "style": "IPY_MODEL_f57326b1e46444f2b18731a45d9fb1ff",
      "value": "100%"
     }
    },
    "8b9a2725c1bb4cefa23a6db1e4b3168e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8efb32e29c8f462e804c870b4e1e091a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fae9e3bb55b4572b69a78ff2e510f1b",
      "placeholder": "​",
      "style": "IPY_MODEL_c2a1d2155ab4423c9ff662a182d3bd5d",
      "value": " 10000/10000 [00:05&lt;00:00, 1289.76it/s]"
     }
    },
    "8fae9e3bb55b4572b69a78ff2e510f1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0defebd249e48f6a2e95391d0e496f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ac6d043aa0a44c0fa0bce6602ff6c619": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1ac2759e9bc43968b03e6a9ea9079d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43bcabf0e8524ddabaaff7c5051ade34",
      "placeholder": "​",
      "style": "IPY_MODEL_6bebfa6823cd4b44a49da06fee878f24",
      "value": "100%"
     }
    },
    "c2a1d2155ab4423c9ff662a182d3bd5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb86c5cf03004ed3a93c8c2816cd2af5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1ac2759e9bc43968b03e6a9ea9079d3",
       "IPY_MODEL_01d155e1affd4e728651929723f6f546",
       "IPY_MODEL_6a3038523275473c917c07d76c709634"
      ],
      "layout": "IPY_MODEL_659674bf3ce8449783d8847f20007ae4"
     }
    },
    "ce00838211304457b185e2376a2fb063": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf20c853f5b64040a4c68ae1d76f756d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da792b96c1b5459a88c47ea612646944": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b9a2725c1bb4cefa23a6db1e4b3168e",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4b4ffa6149b4dccacf651a5ebc33fa9",
      "value": 10000
     }
    },
    "e4b4ffa6149b4dccacf651a5ebc33fa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6aeba87a9cd42da8b7c14cfd506d335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_897a1aaceb5f42b5beb0a1c2f59bd908",
       "IPY_MODEL_da792b96c1b5459a88c47ea612646944",
       "IPY_MODEL_79d4743c09a34eb6b27732f4b4410590"
      ],
      "layout": "IPY_MODEL_599302975b7742d18b03f4e7034f530f"
     }
    },
    "f57326b1e46444f2b18731a45d9fb1ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f71bf0c0777b46c7999f178659d49820": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
