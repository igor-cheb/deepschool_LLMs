{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "TDKBmIM2Alp9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDKBmIM2Alp9",
        "outputId": "5caf5b24-fd75-47b5-cc5b-4d225259ed21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (4.48.2)\n",
            "Requirement already satisfied: datasets in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wTU-fwZygvUr",
      "metadata": {
        "id": "wTU-fwZygvUr"
      },
      "source": [
        "# Домашнее задание: Доступные LLM\n",
        "\n",
        "В этом домашнем задании мы познакомимся с библиотекой transformers и разберемся, как можно open source пользоваться моделями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "11861b12",
      "metadata": {
        "id": "11861b12"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# можете сменить на mps на макбуке, но лично у меня он криво работает\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3",
      "metadata": {
        "id": "ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"
      },
      "source": [
        "# Знакомство с Transformers - 35 баллов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3df5693",
      "metadata": {
        "id": "a3df5693"
      },
      "source": [
        "## Создание модели и предсказание следующего токена - 5 баллов\n",
        "Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и олучить следующий токен через жадную генерацию!\n",
        "\n",
        "1. Для создания модели используйте метод `from_pretrained` у `AutoModelForCausalLM` и `AutoTokenizer`;\n",
        "2. Чтобы токенизировать текст вызовите `tokenizer(text, return_tensors=\"pt\")`, тогда вы получите словарь тензоров\n",
        "3. Передайте его ключи и значения в качестве аргументов в `__call__` (forward) метод модели и получите logits размерности \\[batch_size, seq_len, vocab_size\\]\n",
        "4. По logits предскажите следующий токен и детокенизируйте его с помощью `tokenizer.decode`\n",
        "\n",
        "**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a6ab0d8-60a0-4def-a45c-b1becf4930e1",
      "metadata": {
        "id": "5a6ab0d8-60a0-4def-a45c-b1becf4930e1"
      },
      "outputs": [],
      "source": [
        "def move_to_device(inputs, device):\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = v.to(device)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "YMp8daVEhYKE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YMp8daVEhYKE",
        "outputId": "b18f3f84-e561-420b-caeb-449fba3eb557"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "model_name = \"openai-community/gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device) # Ваш код здесь\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name) # ваш код здесь\n",
        "\n",
        "\n",
        "text = \"This is a sample text\"\n",
        "\n",
        "    # Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n",
        "    # после этого мы получим logits [batch_size = 1, seq_len, d_model]\n",
        "    # По этому тензору нужно предсказать следующее слово!\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs=inputs, device=device)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs[\"logits\"]\n",
        "    next_token_idx: int = logits.argmax(dim=-1)[0, -1].item()\n",
        "\n",
        "next_token = tokenizer.decode([next_token_idx])\n",
        "\n",
        "assert next_token.strip() == \"file\"\n",
        "\n",
        "\n",
        "# ---- Конец кода ----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6809813",
      "metadata": {
        "id": "e6809813"
      },
      "source": [
        "## Используем Generate - 5 баллов\n",
        "\n",
        "Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n",
        "Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n",
        "\n",
        "Для генерации нескольких токенов сразу есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n",
        "\n",
        "Ваша задача написать для модели выше генерацию по тексту с:\n",
        "* Температурой - 0.9\n",
        "* Top-K - 20\n",
        "* Repetition Penalty (Frequency Penalty) - 1.2\n",
        "* максимальное число новых токенов - 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "a6b62dbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a6b62dbf",
        "outputId": "4d14e70d-83b3-4dca-9361-959e9d19c572"
      },
      "outputs": [],
      "source": [
        "text = \"This is still a sample text, but\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    results = []\n",
        "    for i in range(10):\n",
        "        # ---- Ваш код здесь ----\n",
        "        gens = model.generate(\n",
        "            **inputs,\n",
        "            temperature=.9,\n",
        "            top_k=20,\n",
        "            repetition_penalty=1.2,\n",
        "            max_new_tokens=10,\n",
        "\n",
        "            # добавим, чтобы убрать варнинги\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        generation: str = tokenizer.decode(gens.flatten().cpu().numpy()) # сгенерированный текст\n",
        "        results.append(generation)\n",
        "#     # ---- Конец кода ----\n",
        "assert len(set(results)) > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "53f89282",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"This is still a sample text, but it's not very accurate:\\nIn the following\",\n",
              " \"This is still a sample text, but hopefully it's readable.\\nAs I said at\",\n",
              " 'This is still a sample text, but what if you had it in your code instead?',\n",
              " \"This is still a sample text, but I can't figure it out.\\n\\n(\",\n",
              " \"This is still a sample text, but it's got some good formatting (the font looks\",\n",
              " \"This is still a sample text, but I'll try to give an example of what the\",\n",
              " 'This is still a sample text, but I wanted to see how far the story progressed from',\n",
              " \"This is still a sample text, but I'm getting some errors.\\nAnd there you\",\n",
              " 'This is still a sample text, but it may be more useful than the previous ones.',\n",
              " \"This is still a sample text, but it's much nicer if you don't mind adding\"]"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7",
      "metadata": {
        "id": "8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"
      },
      "source": [
        "## Generate Batched - 5\n",
        "Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n",
        "\n",
        "Когда появляется несколько текстов разной длины, то появляются и паддинги.\n",
        "\n",
        "Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n",
        "\n",
        "Тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1]\n",
        "    [5, 6,  7,  1,  2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Представим, что мы сгенерировали еще один токен, тогда\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [3, 2, -1, -1, -1, 7]\n",
        "    [5, 6,  7,  1,  2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [1, 1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n",
        "Тогда исходная последовательность будет:\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2]\n",
        "    [ 5,  6,  7, 1, 2]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1],\n",
        "    [1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "и после генерации следующего токена\n",
        "\n",
        "```python\n",
        "input_ids = [\n",
        "    [-1, -1, -1, 3, 2, 7]\n",
        "    [ 5,  6,  7, 1, 2, 8]\n",
        "]\n",
        "attention_mask = [\n",
        "    [0, 0, 0, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1]\n",
        "]\n",
        "```\n",
        "\n",
        "В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n",
        "\n",
        "Для этого нам придется использовать параметр padding_side=\"left\" в конструкторе токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1",
      "metadata": {
        "id": "5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1"
      },
      "outputs": [],
      "source": [
        "# ---- Ваш код здесь ----\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, padding_side=\"left\") # ваш код здесь\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
      "metadata": {
        "id": "1bd38bdc-3e5e-400d-8815-e9c08a757c03",
        "outputId": "6120007d-3cfc-40cf-ec91-9a5dfe3f65ee",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n",
        "\n",
        "# ---- Ваш код здесь ----\n",
        "with torch.no_grad():\n",
        "# Внимание! В данном задании нужна жадная генерация!\n",
        "\n",
        "# Соберите оба текста в один батч и положите результаты генерации в\n",
        "# batched_generations\n",
        "    inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "    inputs = move_to_device(inputs, device)\n",
        "    batched_generations: List[str] = tokenizer.batch_decode(model.generate(**inputs, pad_token_id=tokenizer.eos_token_id))\n",
        "\n",
        "# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n",
        "# в single_generations\n",
        "    single_generations: List[str] = []\n",
        "    for i, m in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"]):\n",
        "        out = model.generate(input_ids=i.view(1,-1), attention_mask=m.view(1,-1), pad_token_id=tokenizer.eos_token_id)\n",
        "        single_generations.append(tokenizer.decode(out.flatten()))\n",
        "# ---- Конец кода ----\n",
        "\n",
        "assert len(batched_generations) == 2 and len(single_generations) == 2\n",
        "for s, b in zip(batched_generations, single_generations):\n",
        "    assert s == b\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da008c-3653-40d5-89ba-cd831352fd3d",
      "metadata": {
        "id": "f5da008c-3653-40d5-89ba-cd831352fd3d"
      },
      "source": [
        "## Скоринг, Perplixity - 10 баллов\n",
        "\n",
        "Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n",
        "\n",
        "Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n",
        "\n",
        "Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n",
        "\n",
        "```\n",
        "P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n",
        "```\n",
        "\n",
        "Т.е. это вероятность слова при условии его левого контекста.\n",
        "Зачастую ее обозначают как $\\P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n",
        "Эти вероятности можно взять из выходного вектора!\n",
        "\n",
        "Давайте попробуем подсчитать вероятность и perplexity текстов!\n",
        "perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n",
        "\n",
        "$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n",
        "\n",
        "В этом задании нужно:\n",
        "1. Посчитать вероятность **text**\n",
        "2. Посчитать перплексию **text**\n",
        "\n",
        "Еще одна важная деталь:\n",
        "работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n",
        "Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n",
        "\n",
        "$$ p = p_1 * p_2 * p_3 $$\n",
        "$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n",
        "$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n",
        "\n",
        "В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
      "metadata": {
        "id": "e1c7ba39-a451-43a2-ac55-629c99259abe",
        "outputId": "540686ba-c8c6-4c50-eefd-3e1fc9b4ad3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning of sentence (BOS) token = `<|endoftext|>`\n",
            "End of sentence (EOS) token  = `<|endoftext|>`\n",
            "tensor(2.1782e-14, device='cuda:0')\n",
            "tensor(51.0196, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n",
        "print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n",
        "text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# ---- Ваш код здесь ----\n",
        "inputs = move_to_device(inputs, device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # ваш код здесь!\n",
        "    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n",
        "    logits = model(**inputs).logits[:, :-1, :]\n",
        "    labels = inputs.input_ids[:, 1:].unsqueeze(-1)\n",
        "\n",
        "    # 2. Превращаем logits в log_probs\n",
        "    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n",
        "    # для этого нам поможет torch.gather\n",
        "    selected_log_probs = logits.log_softmax(dim=-1).gather(dim=-1, index=labels)\n",
        "\n",
        "    # 4. Считаем вероятности и perplexity!\n",
        "    ppl = (selected_log_probs.flatten().sum()/ (-selected_log_probs.shape[1])).exp()\n",
        "    text_P = selected_log_probs.flatten().sum().exp()\n",
        "# ---- Конец кода ----\n",
        "\n",
        "\n",
        "print(text_P)\n",
        "print(ppl)\n",
        "\n",
        "# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f244eac-7cb1-4689-8adc-46662891e657",
      "metadata": {
        "id": "4f244eac-7cb1-4689-8adc-46662891e657"
      },
      "source": [
        "## Вопросы - 5 баллов\n",
        "\n",
        "**Ответьте на вопросы текстом прямо здесь!**\n",
        "\n",
        "\n",
        "1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n",
        "2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CwqRLxa2mPku",
      "metadata": {
        "id": "CwqRLxa2mPku"
      },
      "outputs": [],
      "source": [
        "# ваш ответ тут\n",
        "# ---- Ваш код здесь ----\n",
        "# Лучшее P(X) стремится к 1 снизу, а лучшее значение перплексии -- тоже к 1, но сверху. \n",
        "\n",
        "# Если модель уверена в верных токенах, то она будет показывать их вероятности близкими к 1, перемножая такие числа получаем чуть меньше 1 или 1. \n",
        "# Перплексия -- экспонента от суммы логарифмов почти единиц, то есть экспонента от почти нуля, то есть чуть больше 1 или 1.\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd5038-620b-48bb-bbc1-db3729141d78",
      "metadata": {
        "id": "5ddd5038-620b-48bb-bbc1-db3729141d78"
      },
      "source": [
        "# Chat-Models - 20 баллов\n",
        "\n",
        "Теперь мы познакомимся с chat-моделями, т.е. с моделями, которые предоставляют возможность общаться с ними как с ассистентом. Эти модели не просто продолджают текст слева-направо, а дают ответ на заданный вопрос.\n",
        "\n",
        "## Формат - 5 баллов\n",
        "\n",
        "Все chat-модели принимают ответ в особом формате, который позволяет им различать, кому принадлежит фраза - пользователю (user) или модели (assistant).\n",
        "Давайте попробуем подать модели вопрос без какого-либо форматирования."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "id": "7f5fe593-63a8-406d-9678-6d805c180670",
      "metadata": {
        "id": "7f5fe593-63a8-406d-9678-6d805c180670"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ccea37c6df14f5cb144736cce2dc4e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64b7ced9bdef429ab8aa2f53aa60e5b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cffe4809544462e95da7c0fd54a275f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81565ff1e2f244f3bbc90f5e2d2ab504",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6c59f82322e45d59010f6aec7f7a15f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c1cd5709d4b4a75bd761c448147c8c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e00825b3f9d548179359082ad6820a2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c991f201d06b4908b970dc942d39e094",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", torch_dtype=torch.half).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7134f0bb-1ee4-4508-a26d-5326ea96562b",
        "outputId": "60f4aa05-0abb-4c90-b9a9-b62cb4c306c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello how are you?\" or just wait for you to approach you. You can also ask the waiter if you can\n",
            "============\n",
            "hello how are you today.' This is an effective call-to-action because a response is almost guaranteed. If you\n",
            "============\n",
            "hello how are you?\n",
            "Bob looks at you with a confused expression.  \"Oh, nothing, but I'm\n",
            "============\n",
            "hello how are you\". The user's attempt to respond with \"I am excellent\" triggers the correct response because the user\n",
            "============\n",
            "hello how are you'?<|im_end|>\n",
            "============\n"
          ]
        }
      ],
      "source": [
        "text = \"hello how are you\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "for i in range(5):\n",
        "    print(tokenizer.decode(model.generate(**move_to_device(inputs, device), max_new_tokens=20, use_cache=True, do_sample=True, pad_token_id=tokenizer.eos_token_id)[0]))\n",
        "    print(\"====\" * 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc",
      "metadata": {
        "id": "3fd50470-64b9-4a21-8748-0e9c5ea439fc"
      },
      "source": [
        "Видим, что текст зачастую разламывается:\n",
        "1. Иногда модель продолжает текст как базовая LLM\n",
        "2. Иногда пытается придумать роли спикерам и трасформироваться в формат диалога\n",
        "3. Иногда просто выдает бессвязный текст.\n",
        "\n",
        "Это происходит потому, что формат входных данных сильно отличается от того, что модель видела на обучении.\n",
        "Как мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "id": "fec7ca96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fec7ca96",
        "outputId": "88c57c71-aeb8-48a6-adfd-73f433c7fc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful assistant, who always helps user<|im_end|>\n",
            "<|im_start|>user\n",
            "How to learn about LLMs?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "You can always attend deepschool!<|im_end|>\n",
            "<|im_start|>user\n",
            "Thank you!<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prefix = tokenizer.apply_chat_template(\n",
        "    conversation=\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, who always helps user\"},\n",
        "        {\"role\": \"user\", \"content\": \"How to learn about LLMs?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"You can always attend deepschool!\"},\n",
        "        {\"role\": \"user\", \"content\": \"Thank you!\"},\n",
        "    ],\n",
        "    tokenize=False)\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ew_7ZmE7-miE",
      "metadata": {
        "id": "ew_7ZmE7-miE"
      },
      "source": [
        "Как мы видим в тексте ходы и роли разделены тэгом `<|im_start|>`. В таком формате модель училась поддерживать диалог. Давайте отформатируем следующий диалог и подадим его в генерацию модели. Подробнее про apply_chat_template можно прочитать в [туториале](https://huggingface.co/docs/transformers/main/en/chat_templating#applychattemplate). Обратите внимание на опцию add_generation_prompt! Эта опция добавляет текст таким образом, чтобы в конце была очередь генерировать текст от лица модели. Давайте попробуем собрать диалог и сгенерировать моделью ответ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "e79a3701-c80f-4b90-90bd-fa010e32ea36",
        "outputId": "f32afb2b-a306-4e08-f576-567c5d2e25bb"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"hello\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n",
        "    {\"role\": \"user\", \"content\": \"I love you\"},\n",
        "]\n",
        "\n",
        "\n",
        "# ---- Ваш код здесь ----\n",
        "prefix = tokenizer.apply_chat_template(conversation=messages, tokenize=False, add_generation_prompt=True)\n",
        "# ---- Конец кода ----\n",
        "reference = \"\"\"<|im_start|>system\n",
        "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
        "<|im_start|>user\n",
        "hello<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I'm good. How can I help you today<|im_end|>\n",
        "<|im_start|>user\n",
        "I love you<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "assert prefix.strip() == reference.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4284b18d-4f9b-4e7d-b3ea-bb365e90093c",
        "outputId": "fcdbfc09-294d-4eb7-d2f1-b3d9bfd9b103"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
            "<|im_start|>user\n",
            "hello<|im_end|>\n",
            "<|im_start|>assistant\n",
            "I'm good. How can I help you today<|im_end|>\n",
            "<|im_start|>user\n",
            "I love you<|im_end|>\n",
            "<|im_start|>assistant\n",
            "That's a great sentiment! What do you think about the weather today?<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "inputs =  move_to_device(inputs, device)\n",
        "out = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(out.flatten()))\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72482f3-c296-46f3-851c-57b4f91a717b",
      "metadata": {
        "id": "a72482f3-c296-46f3-851c-57b4f91a717b"
      },
      "source": [
        "# Benchmark - 15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6",
      "metadata": {
        "id": "52f422a9-c2ee-4c17-8aee-1830f1d143e6"
      },
      "source": [
        "Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n",
        "* question - вопрос\n",
        "* choices - варианты ответа\n",
        "* answer - номер правильного ответа (нумерация с нуля)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "530d1721-6623-4ca6-816c-d4f90203ceb2",
        "outputId": "db0c10eb-7a9e-4db3-e7f8-0c169830f3cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "927cf3c5a88a4364b56ad4400c59529c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e88235af97543c5ac7e7725edb7609a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76433ba44b9548c991d810f79a0d9363",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/11.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df3a7e7aef7b4729a6831bfcc6fc1eae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63d87ca95dc5427e8a9b3ede97e26d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dev-00000-of-00001.parquet:   0%|          | 0.00/3.58k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a340d18d9d4716baf7cdaa67ad83bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b1228ec95d8414faf79895c492338cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea17df5acc7489c9883b788deef9ac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'As of 2016, about what percentage of adults aged 18 years or older were overweight?',\n",
              " 'subject': 'global_facts',\n",
              " 'choices': ['10%', '20%', '40%', '80%'],\n",
              " 'answer': 2}"
            ]
          },
          "execution_count": 273,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n",
        "mmlu[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wafeDm4KB6lI",
      "metadata": {
        "id": "wafeDm4KB6lI"
      },
      "source": [
        "Наша задача здесь - выбрать моделью один из четырех ответов и получить точность больше 0.25.\n",
        "\n",
        "Есть несколько вариантов, как это делать. **Эти варианты отличаются по сложности и являются взаимоисключающими. За подход с генерацией можно получить максимум 5 баллов, за подход со скорингом по 1 сэмплу можно получить только 10 баллов, а за подход со скорингом батчей можно получить все 15 баллов**\n",
        "\n",
        "### Генерация ответа - 5 баллов\n",
        "\n",
        "Можно генерировать ответ напрямую. Для этого нужно:\n",
        "1. Составить историю диалога из qeustion и choices с помощью messages и apply_chat_template\n",
        "1. Сгенерировать ответ\n",
        "1. Соотнести сгенерированный ответ с одним из вариантов ответа\n",
        "\n",
        "У этого подхода есть один важный недостаток - модель могут сгенерировать ответ, не являющийся одним из заданных вариантов ответа. Соотнесение такой генерации с ответом решается эвристиками и скорее всего приведет к множеству ошибок.\n",
        "\n",
        "\n",
        "### Скоринг по сэмплам - 10 баллов\n",
        "\n",
        "У нас есть вопрос и 4 варианта ответа в 4 набора messages и подсчитать вероятность $$P(choices_i | question)$$, то есть условную вероятность каждого ответа при заданном вопросе, т.е. сделать то же самое, что мы делали в задаче про вероятность текста и perplexity.\n",
        "\n",
        "1. Берем текст и вариант ответа, собираем из них промпт c функции `sample_to_texts` (проще будет в этом задании обойтись без apply_chat_template)\n",
        "2. Подаем это в модель через `model(**inputs)`, берем в выходах `logits`. С помощью logits и input_ids считаем вероятность токенов, которые мы подали модели.\n",
        "3. Здесь опционально можно считать как вероятность всего текста, так и только вероятность $$P(choices_i | question)$$ Т.к. для всех 4х вариантов ответа у нас общий префикс, то его вероятность будет общей константной для всех ответов.\n",
        "4. Выбираем ответ, которому была дана наибольшая вероятность нашей моделью.\n",
        "\n",
        "В этом варианте легко получить номер ответа, который модель оценивает выше и не нужно применять эвристики.\n",
        "\n",
        "### Скоринг батчами - 15 баллов\n",
        "\n",
        "Этот вариант отличается от предыдущего только тем, что нужно скорить за раз не один сэмпл и 4 ответа к нему, а несколько сэмплов за раз, т.е. обрабатывать данные батчом.\n",
        "\n",
        "Дополнительная сложность этого варианта заключается в том, что у нас возникают сэмплы различной длины, которые мы добиваем паддингами. **Паддинги не нужно учитывать при подсчете вероятностей, это служебные токены!**\n",
        "\n",
        "Чтобы вероятность паддингов не влияла на итоговую вероятность текста, можно на этапе, где у вас подсчитаны все вероятности токенов (вместе с паддингами) взять `inputs[\"attention_mask\"]` и \"занулить\" по нему вероятности паддингов (если вы считаете log_probs, если вы честно умножаете вероятности, тогда вероятности паддингов нужно поставить равными единице).\n",
        "\n",
        "В качестве проверки точности можете проверить, что качество с батчом размера 1 не сильно отличается от батча размера 3 (не более, чем на 0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc14ce5-267d-40da-b35a-193c60cc68ca",
      "metadata": {
        "id": "efc14ce5-267d-40da-b35a-193c60cc68ca"
      },
      "outputs": [],
      "source": [
        "def sample_to_texts(sample):\n",
        "    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n",
        "\n",
        "def calc_acc(p, y):\n",
        "    assert len(p) == len(y)\n",
        "    return sum(pi == yi for pi, yi in zip(p, y)) / len(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "230aed9b-32af-4a9d-8615-c2a2ddb864b2",
      "metadata": {
        "id": "230aed9b-32af-4a9d-8615-c2a2ddb864b2"
      },
      "outputs": [],
      "source": [
        "y_true = [sample[\"answer\"] for sample in mmlu]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccb1953-cf28-4629-9786-4fb71c178ac8",
      "metadata": {
        "id": "dccb1953-cf28-4629-9786-4fb71c178ac8"
      },
      "source": [
        "Считаем вероятности по одному question и choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hlw878TTFowB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlw878TTFowB",
        "outputId": "c4bc0807-8128-4a8c-a82c-e89ddb0411c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "all_prompts = sum([sample_to_texts(mmlu[i]) for i in range(len(mmlu))], [])\n",
        "assert len(all_prompts) == 400\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "...\n",
        "# ---- Конец кода ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66308e6",
      "metadata": {},
      "source": [
        "## Ответьте на следующие вопросы (5 баллов в сумме):\n",
        "1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n",
        "2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\n",
        "Стоит ли по-вашему добавлять эти метки?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f811187-7b10-4382-a6f5-ebec7afa8125",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f811187-7b10-4382-a6f5-ebec7afa8125",
        "outputId": "8af38af8-3c82-4e60-826b-46de57cbd9a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Ваш код здесь ----\n",
        "...\n",
        "# ---- Конец кода ----"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rami_hw",
      "language": "python",
      "name": "rami_hw"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
