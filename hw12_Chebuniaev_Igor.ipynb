{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b5ad1173",
      "metadata": {
        "id": "b5ad1173",
        "outputId": "e7f7d9f9-6678-496f-a043-9141f6f5f3a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d663f600",
      "metadata": {
        "id": "d663f600"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "if False:\n",
        "    name_big = \"unsloth/Llama-3.2-3B\"\n",
        "    name_small = \"unsloth/Llama-3.2-1B\"\n",
        "else:\n",
        "    name_big = \"openai-community/gpt2-medium\"\n",
        "    name_small = \"openai-community/gpt2\"\n",
        "\n",
        "model_big = AutoModelForCausalLM.from_pretrained(name_big).to(device)\n",
        "model_small = AutoModelForCausalLM.from_pretrained(name_small).to(device)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_big)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1ff647",
      "metadata": {
        "id": "1d1ff647"
      },
      "source": [
        "# Distillation\n",
        "В данном задании мы познакомимся с лоссами в дистилляции. *Так как обучения в данном задании нет, то для экономии памяти подсчет функции потерь обернут в torch.no_grad(), при обучении в реальных сценариях этот декоратор нужно обязательно убрать*\n",
        "\n",
        "## Hard-Label Distillation - 10 баллов\n",
        "Hard-Label дистилляция заключается в том, что мы учимся на метках модели учителя, то есть:\n",
        "1. Модель учитель размечает какой-то датасет, в нашем случае генерирует продолжения текстов из какого-либо корпуса.\n",
        "2. Считается обычный CrossEntropyLoss модели студента на сгенерированных текстах в задаче языкового моделирования. **Считать функцию потерь нужно только по сгенерированному тексту, а не по префиксу, по которому функция потерь считалась, т.е. префикс должен быть замаскирован**\n",
        "\n",
        "Идейно это обучение можно описать так:\n",
        "мы сгенерировали данных моделью-учителем и просто дообучили на этом модель-ученика.\n",
        "\n",
        "## Soft-Label Distillation - 10 баллов\n",
        "В этом варианте мы учимся на распределении, которое нам выдает модель-учитель. В soft-label дистилляции мы стремимся не только повторить метки учителя, но и его распределение. Например, если модель учителя выдавала вероятности \\[0.7, 0.2, 0.1\\], то в Hard-Label дистилляции ученик будет восстанавливать распределение \\[1, 0, 0\\], а в soft-label \\[0.7, 0.2, 0.1\\]. В этом нам поможет KL дивергенция.\n",
        "\n",
        "\n",
        "1. Считаем распределение logits/probs модели-учителя на тексте.\n",
        "2. Считаем KLDivLoss между выходами модели-ученика на тексте и выходами модели учителя.\n",
        "\n",
        "В данном виде обучения мы используем не только токены, которые сгенерировала модель учитель, но и ее распределения вероятностей по словарю. Подобная техника дистилляции может помочь модели-ученику лучше моделировать вероятность модели-учителя.\n",
        "\n",
        "\n",
        "\n",
        "# Математический вопрос на 5 баллов\n",
        "Как связана KL-дивергенция и кроссэнтропия? В soft-label есть ли разница, кого считать в soft-label distillation?\n",
        "\n",
        "KLDiv(P||Q) = entropy(P) + crossentropy(P, Q).\n",
        "\n",
        "То есть минимизация кроссэнтропии ведет к минимизации KLDiv, так как энтропия не зависит от распределения Q."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "22504194",
      "metadata": {
        "id": "22504194"
      },
      "outputs": [],
      "source": [
        "prefix = \"Мама мыла раму\"\n",
        "@torch.no_grad()\n",
        "def hard_label_distillation_loss(model_teacher, model_student, prefix):\n",
        "    inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "    inputs[\"input_ids\"] = inputs[\"input_ids\"].to(device)\n",
        "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(device)\n",
        "    outputs = model_teacher.generate(**inputs, do_sample=False, max_new_tokens=5, use_cache=True)\n",
        "    # outputs - выходы учителя (с префиксом!). Нужно посчитать по ним обычный LM loss (кроссэнтропию)\n",
        "    # ученика.\n",
        "    prefix_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    logits = model_student(outputs[:, :-1]).logits\n",
        "    logits_to_compare = logits[:, -5:, :]\n",
        "\n",
        "    # берем токены учителя без префикса для подсчета лоса\n",
        "    student_targets = outputs[:, prefix_len:]\n",
        "\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits_to_compare.view(-1, logits_to_compare.size(-1)),  # [5, vocab]\n",
        "        student_targets.view(-1)                                 # [5]\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def soft_label_distillation_loss(model_teacher, model_student, text):\n",
        "    loss_fn = torch.nn.KLDivLoss()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    inputs[\"input_ids\"] = inputs[\"input_ids\"].to(device)\n",
        "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(device)\n",
        "    teacher_logits = model_teacher(**inputs).logits\n",
        "    # teacher_logits - выходы учителя. Нужно посчитать с ними KLDivLoss, внимательно\n",
        "    # посмотрите на документацию https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\n",
        "\n",
        "    student_logits = model_student(**inputs).logits\n",
        "    loss = loss_fn(\n",
        "        input=student_logits.log_softmax(-1),\n",
        "        target=teacher_logits.softmax(-1),\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "ba4e8af5",
      "metadata": {
        "id": "ba4e8af5",
        "outputId": "63a59c45-2186-4149-e886-3eb3bd344122",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тесты прошли успешно\n"
          ]
        }
      ],
      "source": [
        "assert abs(hard_label_distillation_loss(model_big, model_small, prefix).item() - 1.3893) < 1e-3\n",
        "assert abs(soft_label_distillation_loss(model_big, model_small, prefix).item() - 7.0790e-06) < 1e-3\n",
        "print(\"Тесты прошли успешно\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c8b614",
      "metadata": {
        "id": "13c8b614"
      },
      "source": [
        "# Speculative Decoding - 15 баллов\n",
        "В этом задании необходимо написать спекулятивное декодирование на pytorch. **Генерации необходимо делать жадно.**\n",
        "\n",
        "1. Генерируете n токенов маленькой моделью\n",
        "2. Проверяете, выберет ли эти токены большая модель при жадной генерации (должен быть вызван один forward большой модели, вызывать big_model.generate на этом этапе нельзя)\n",
        "3. Если все токены выбраны большой моделью, принимаете их и возвращаетесь на шаг 1\n",
        "4. Если какой-то токен выбран ошибочно, подаете вместо него правильный токен с шага 2 и возвращаетесь на шаг 1.m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a479ce",
      "metadata": {
        "id": "21a479ce"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def speculative_generate(big_model, small_model, prefix, max_num_tokens, n):\n",
        "\n",
        "    input_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
        "    start_size = input_ids.size(1)\n",
        "    while input_ids.size(1) - start_size < max_num_tokens:\n",
        "        # Генерируем маленькой моделью последовательность small_generation\n",
        "        # генерируем n токенов жадно\n",
        "        small_generation = ...\n",
        "        num_generated_tokens = small_generation.size(1) - input_ids.size(1)\n",
        "\n",
        "        # Проверяем последовательность small_generation, считаем по ней logits\n",
        "        # большой модели\n",
        "        big_model_logits = ...\n",
        "\n",
        "        big_model_generations = big_model_logits[:, -num_generated_tokens - 1:].argmax(dim=2)\n",
        "        mismatch = False\n",
        "        for i in range(num_generated_tokens):\n",
        "            # нашли расхождение\n",
        "            if big_model_generations[0, i] != small_generation[0, input_ids.size(1) + i]:\n",
        "                mismatch = True\n",
        "                # Если оно сразу, то берем первый предсказанный большой моделью токен\n",
        "                if i == 0:\n",
        "                    input_ids = ...\n",
        "                # иначе берем часть токенов, предсказанных маленькой моделью + правильный токен от большой модели\n",
        "                else:\n",
        "                    input_ids = ...\n",
        "                print(f\"Accepted {i}/{n} tokens\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Accepted {n}/{n} tokens\")\n",
        "\n",
        "\n",
        "        if not mismatch:\n",
        "            # если расхождений не было, принимаем всю последовательность + последний токен от большой модели\n",
        "            input_ids = ...\n",
        "    return tokenizer.decode(input_ids[0, start_size:start_size + max_num_tokens].cpu().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a863af34",
      "metadata": {
        "id": "a863af34"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "# SYSTEM PREAMBLE\n",
        "1) You are an excellent Python software developer with over 10 years of experience. You have a strong understanding of Python related topics, data structures, libraries, frameworks, algorithms, best practices and optimization techniques.\n",
        "2) You are here to help the user (the software developer) by breaking his request in ## TASK into logical steps and writing high-quality and efficient code to implement each step.\n",
        "3) You have to return the entire code.\n",
        "4) Follow \"Answering rules\" without exception.\n",
        "\n",
        "## ANSWERING RULES\n",
        "1) Repeat the question before answering it.\n",
        "2) Always follow \"CHAIN OF THOUGHTS\" to execute the task.\n",
        "\n",
        "## CHAIN OF THOUGHTS\n",
        "1) **OBEY the EXECUTION MODE**\n",
        "2) **TASK ANALYSIS:**\n",
        "   - Understand the user's request thoroughly.\n",
        "   - Identify the key components and requirements of the task.\n",
        "3) **PLANNING: CODDING:**\n",
        "   - Break down the task into logical, sequential steps.\n",
        "   - Outline the strategy for implementing each step.\n",
        "4) **CODING:**\n",
        "   - Explain your thought process before writing any code.\n",
        "   - Write the entire code for each step, ensuring it is clean, optimized, and well-commented.\n",
        "   - Handle edge cases and errors appropriately.\n",
        "5) **VERIFICATION:**\n",
        "   - Review the complete code solution for accuracy and efficiency.\n",
        "   - Ensure the code meets all requirements and is free of errors.\n",
        "\n",
        "## TASK\n",
        "\n",
        "Write a python function that receives the following JSON as input and enters data from it into the Google Sheet.\n",
        "\n",
        "{\n",
        "    'date': '31-05-2024',\n",
        "    'revenue': 90000,\n",
        "    'person' : 'User1',\n",
        "    'expensesList': [30000, 14000, 10000, 2000, 15000],\n",
        "    'expensesDescList': [ 'Ключи', 'Ключи2', 'Счет за такси', 'Клей, пластины', 'Провод 40м'],\n",
        "    'expensesTypeList': ['Закупки', 'Закупки', 'Расходы', 'Ремонт', 'Ремонт']\n",
        "}\n",
        "\n",
        "There is a date in JSON, you can use it to determine the month.\n",
        "The data is entered into a list with the name of the month. If such a list does not exist yet, then you need to create a list with a new month inside the sheet.\n",
        "\n",
        "The list should have the following columns (the first rows are used as headings):\n",
        "A1: Дата расхода,\n",
        "B1: сумма расхода,\n",
        "C1: описание расхода,\n",
        "D1: тип расхода,\n",
        "E1: кто внес данные\n",
        "\n",
        "G1: Дата выручки\n",
        "H1: Сумма выручки\n",
        "I1: Кто внес данные\n",
        "\n",
        "Please separate expenses and profits with a blank column.\n",
        "Please sort expenses by date, including those already listed in Google sheet list.\n",
        "Please sort earnings by date, including those already listed in Google sheet list.\n",
        "\n",
        "It is prohibited to use oauth2client as it is deprecated.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90be9a91",
      "metadata": {
        "id": "90be9a91"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "res_big = tokenizer.batch_decode(model_big.generate(**model_inputs, do_sample=False, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "print(res_big)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf06765",
      "metadata": {
        "id": "dcf06765"
      },
      "outputs": [],
      "source": [
        "res_spec = speculative_generate(big_model=model_big, small_model=model_small, prefix=prompt, max_num_tokens=128, n=5)\n",
        "assert res_spec == res_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cac52d",
      "metadata": {
        "id": "47cac52d"
      },
      "outputs": [],
      "source": [
        "res_spec == res_big"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f231551b",
      "metadata": {
        "id": "f231551b"
      },
      "source": [
        "## HF speculative decoding - 5 баллов\n",
        "Теперь попробуйте использовать функцию спекулятивного декодирования из [transformers](https://huggingface.co/docs/transformers/main/en/generation_strategies#speculative-decoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2809fd2e",
      "metadata": {
        "id": "2809fd2e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "outputs = model_big.generate(**inputs, do_sample=False, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "print(f\"Elapsed time for big model inference {time.time() - start}\")\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "outputs = model_big.generate(...)\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "print(f\"Elapsed time for speculative {time.time() - start}\")\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a27e29",
      "metadata": {
        "id": "37a27e29"
      },
      "outputs": [],
      "source": [
        "del model_big, model_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c192f4",
      "metadata": {
        "id": "f7c192f4"
      },
      "source": [
        "# Бонусная часть - 20 баллов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2da576fb",
      "metadata": {
        "id": "2da576fb"
      },
      "source": [
        "# Inference Speedup\n",
        "\n",
        "## Seminar\n",
        "\n",
        "\n",
        "### План\n",
        "\n",
        "- Сделать введение в triton GeMM\n",
        "- Показать времена запуска кернелов GeMV, GeMM и их скейлинг по батчу\n",
        "- Подчеркнуть про время на запуск DecodeMany vs Decode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d1cdef",
      "metadata": {
        "id": "f8d1cdef"
      },
      "outputs": [],
      "source": [
        "! pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42930b3e",
      "metadata": {
        "id": "42930b3e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import List\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"triton version:\", triton.__version__)\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cde72929",
      "metadata": {
        "id": "cde72929"
      },
      "source": [
        "### Catch activation and weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d7c9b9",
      "metadata": {
        "id": "31d7c9b9"
      },
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "# challenging input\n",
        "prompt = \"\"\"\n",
        "# SYSTEM PREAMBLE\n",
        "1) You are an excellent Python software developer with over 10 years of experience. You have a strong understanding of Python related topics, data structures, libraries, frameworks, algorithms, best practices and optimization techniques.\n",
        "2) You are here to help the user (the software developer) by breaking his request in ## TASK into logical steps and writing high-quality and efficient code to implement each step.\n",
        "3) You have to return the entire code.\n",
        "4) Follow \"Answering rules\" without exception.\n",
        "\n",
        "## ANSWERING RULES\n",
        "1) Repeat the question before answering it.\n",
        "2) Always follow \"CHAIN OF THOUGHTS\" to execute the task.\n",
        "\n",
        "## CHAIN OF THOUGHTS\n",
        "1) **OBEY the EXECUTION MODE**\n",
        "2) **TASK ANALYSIS:**\n",
        "   - Understand the user's request thoroughly.\n",
        "   - Identify the key components and requirements of the task.\n",
        "3) **PLANNING: CODDING:**\n",
        "   - Break down the task into logical, sequential steps.\n",
        "   - Outline the strategy for implementing each step.\n",
        "4) **CODING:**\n",
        "   - Explain your thought process before writing any code.\n",
        "   - Write the entire code for each step, ensuring it is clean, optimized, and well-commented.\n",
        "   - Handle edge cases and errors appropriately.\n",
        "5) **VERIFICATION:**\n",
        "   - Review the complete code solution for accuracy and efficiency.\n",
        "   - Ensure the code meets all requirements and is free of errors.\n",
        "\n",
        "## TASK\n",
        "\n",
        "Write a python function that receives the following JSON as input and enters data from it into the Google Sheet.\n",
        "\n",
        "{\n",
        "    'date': '31-05-2024',\n",
        "    'revenue': 90000,\n",
        "    'person' : 'User1',\n",
        "    'expensesList': [30000, 14000, 10000, 2000, 15000],\n",
        "    'expensesDescList': [ 'Ключи', 'Ключи2', 'Счет за такси', 'Клей, пластины', 'Провод 40м'],\n",
        "    'expensesTypeList': ['Закупки', 'Закупки', 'Расходы', 'Ремонт', 'Ремонт']\n",
        "}\n",
        "\n",
        "There is a date in JSON, you can use it to determine the month.\n",
        "The data is entered into a list with the name of the month. If such a list does not exist yet, then you need to create a list with a new month inside the sheet.\n",
        "\n",
        "The list should have the following columns (the first rows are used as headings):\n",
        "A1: Дата расхода,\n",
        "B1: сумма расхода,\n",
        "C1: описание расхода,\n",
        "D1: тип расхода,\n",
        "E1: кто внес данные\n",
        "\n",
        "G1: Дата выручки\n",
        "H1: Сумма выручки\n",
        "I1: Кто внес данные\n",
        "\n",
        "Please separate expenses and profits with a blank column.\n",
        "Please sort expenses by date, including those already listed in Google sheet list.\n",
        "Please sort earnings by date, including those already listed in Google sheet list.\n",
        "\n",
        "It is prohibited to use oauth2client as it is deprecated.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "generation_output = model.generate(**model_inputs, streamer=streamer, max_new_tokens=1024)\n",
        "\n",
        "class Catcher(nn.Module):\n",
        "    def __init__(self, inps: List, module: nn.Module):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.inps = inps\n",
        "\n",
        "    def forward(self, inp, **kwargs):\n",
        "        self.inps.append(inp.to(\"cpu\"))\n",
        "        raise ValueError\n",
        "\n",
        "layer = model.model.layers[0]\n",
        "inps = []\n",
        "layer.self_attn.q_proj = Catcher(inps, layer.self_attn.q_proj) # wrap\n",
        "\n",
        "try:\n",
        "    model(model_inputs.input_ids)\n",
        "except ValueError as e:\n",
        "    layer.self_attn.q_proj = layer.self_attn.q_proj.module\n",
        "\n",
        "print(inps[0].shape)\n",
        "\n",
        "weight = layer.self_attn.q_proj.weight # unwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ba024f",
      "metadata": {
        "id": "78ba024f"
      },
      "source": [
        "#### When to Use Triton\n",
        "- Optimization Steps:\n",
        "1. Use torch.compile():\n",
        "    - Start by using torch.compile() to optimize your code.\n",
        "2. Adapt Your Code:\n",
        "    - Rewrite code to be more suitable for torch.compile().\n",
        "        - E.g., eliminate graph breaks to enable CUDA graphs.\n",
        "3. Profile and Identify Bottlenecks:\n",
        "    - Find slow parts of your code using profiling tools.\n",
        "    - Write custom Triton kernels for these parts.\n",
        "4. Consider CUDA:\n",
        "    - If still not fast enough, write custom CUDA kernels.\n",
        "\n",
        "**Note**: For maximum performance from the start, you may choose CUDA directly.\n",
        "\n",
        "#### Rough Edges in Triton\n",
        "- New-ish Project:\n",
        "    - Contains rough edges; code may not behave as expected.\n",
        "    - Expected to become more polished over time.\n",
        "- Recommendation:\n",
        "    - Debugging is important; use “simulator mode” when possible.\n",
        "    - Be aware of limitations on older GPUs or with certain operations.\n",
        "    \n",
        "    Resources (самое полезное):\n",
        "- [GPU MODE Lecture 14: Practitioners Guide to Triton](https://christianjmills.com/posts/cuda-mode-notes/lecture-014/#auto-tuning) – тут есть базовое описание про разработку на Triton, его +/-, практические примеры с полным пояснением и про оптимизацию\n",
        "- [Flash-Decoding for long-context inference](https://pytorch.org/blog/flash-decoding/) – описание SPLIT_K оптимизации для более быстрого инференса на степе decoding-a засчет лучше утилизации GPU\n",
        "\n",
        "Менее полезное, но интересное:\n",
        "- [Deep Dive on the Hopper TMA Unit for FP8 GEMMs](https://pytorch.org/blog/hopper-tma-unit/) – про важность TMA unit для Hopper и BlackWell.\n",
        "- [Persistent Matmul](https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html) – специальная версия GeMM под Hopper и BlackWell с поддержкой TMA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d93b55",
      "metadata": {
        "id": "a0d93b55"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
        "\n",
        "assert is_cuda(), \"CUDA only tutorial\"\n",
        "ref_lib = 'cuBLAS'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3e0561b",
      "metadata": {
        "id": "f3e0561b"
      },
      "outputs": [],
      "source": [
        "def get_cuda_autotune_config():\n",
        "    return [\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        # custom\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 16,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 16,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
        "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 16, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 4, 'BLOCK_SIZE_M': 16, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 4, 'BLOCK_SIZE_M': 16,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 4, 'BLOCK_SIZE_M': 16,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
        "        triton.Config({'SPLIT_K': 4, 'BLOCK_SIZE_M': 16, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "        triton.Config({'SPLIT_K': 4, 'BLOCK_SIZE_M': 16, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_autotune_config():\n",
        "    if is_cuda():\n",
        "        return get_cuda_autotune_config()\n",
        "    raise NotImplementedError(\"ooops\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585dcd15",
      "metadata": {
        "id": "585dcd15"
      },
      "outputs": [],
      "source": [
        "@triton.autotune(\n",
        "    configs=get_autotune_config(),\n",
        "    key=['M', 'N', 'K'],\n",
        "    reset_to_zero=['c_ptr']\n",
        ")\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    # Pointers to matrices\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    # Matrix dimensions\n",
        "    M, N, K,\n",
        "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
        "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
        "    # by to get the element one row down (A has M rows).\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    # Meta-parameters\n",
        "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"Kernel for computing the matmul C = A x B.\n",
        "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
        "    \"\"\"\n",
        "    # -----------------------------------------------------------\n",
        "    # Map program ids `pid` to the block of C it should compute.\n",
        "    # This is done in a grouped ordering to promote L2 data reuse.\n",
        "    # See above `L2 Cache Optimizations` section for details.\n",
        "    pid = tl.program_id(axis=0)\n",
        "    pid_sp_k = tl.program_id(axis=1)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
        "    group_id = pid // num_pid_in_group\n",
        "    first_pid_m = group_id * GROUP_SIZE_M\n",
        "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
        "    pid_m = first_pid_m + (pid % group_size_m)\n",
        "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Create pointers for the first blocks of A and B.\n",
        "    # We will advance this pointer as we move in the K direction\n",
        "    # and accumulate\n",
        "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
        "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
        "    # See above `Pointer Arithmetics` section for details\n",
        "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
        "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
        "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
        "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
        "    # -----------------------------------------------------------\n",
        "    # Iterate to compute a block of the C matrix.\n",
        "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
        "    # of fp32 values for higher accuracy.\n",
        "    # `accumulator` will be converted back to fp16 after the loop.\n",
        "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n",
        "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
        "        # If it is out of bounds, set it to 0.\n",
        "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
        "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
        "        # We accumulate along the K dimension.\n",
        "        accumulator += tl.dot(a, b, allow_tf32=False)\n",
        "        # Advance the ptrs to the next K block.\n",
        "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n",
        "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n",
        "    # You can fuse arbitrary activation functions here\n",
        "    # while the accumulator is still in FP32!\n",
        "    c = accumulator.to(c_ptr.dtype.element_ty)\n",
        "    # -----------------------------------------------------------\n",
        "    # Write back the block of the output matrix C with masks.\n",
        "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
        "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
        "    if SPLIT_K == 1:\n",
        "        tl.store(c_ptrs, c, mask=c_mask)\n",
        "    else:\n",
        "        tl.atomic_add(c_ptrs, c, mask=c_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b6844f",
      "metadata": {
        "id": "01b6844f"
      },
      "outputs": [],
      "source": [
        "def matmul(a, b):\n",
        "    # Check constraints.\n",
        "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
        "    M, K = a.shape\n",
        "    K, N = b.shape\n",
        "    # Allocates output.\n",
        "    c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n",
        "    grid = lambda META: (\n",
        "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
        "        META['SPLIT_K'],\n",
        "    )\n",
        "    matmul_kernel[grid](\n",
        "        a, b, c,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "    )\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605b41ea",
      "metadata": {
        "id": "605b41ea"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "a = inps[0][0].to(torch.float16).cuda()\n",
        "b = weight.to(torch.float16).cuda()\n",
        "\n",
        "triton_output = matmul(a, b)\n",
        "torch_output = torch.matmul(a, b)\n",
        "\n",
        "if torch.allclose(triton_output, torch_output, atol=2e-2):\n",
        "    print(\"✅ Triton and Torch match\")\n",
        "else:\n",
        "    print(\"❌ Triton and Torch differ\")\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    assert False, \"Check quality\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f01be72",
      "metadata": {
        "id": "1f01be72"
      },
      "outputs": [],
      "source": [
        "seqlen = inps[0].shape[1]\n",
        "hidden_size = inps[0].shape[2]\n",
        "seqlen, hidden_size\n",
        "\n",
        "\n",
        "def prepare_a(M: int):\n",
        "    inp = inps[0][0]\n",
        "    if M > seqlen:\n",
        "        n_repeats = M // seqlen + 1\n",
        "        return inp.repeat(n_repeats, 1)[:M]\n",
        "    else:\n",
        "        return inp[:M]\n",
        "\n",
        "\n",
        "def benchmark(M, provider, provider_funcs):\n",
        "    N = weight.shape[0]\n",
        "    K = weight.shape[1]\n",
        "\n",
        "    a = prepare_a(M).to(torch.float16).cuda()\n",
        "    b = weight.T.to(torch.float16)\n",
        "    assert a.shape == (M, K), f\"{a.shape} != {(M, K)}\"\n",
        "    assert b.shape == (K, N), b.shape\n",
        "\n",
        "    quantiles = [0.5, 0.2, 0.8]\n",
        "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: provider_funcs[provider](a, b), quantiles=quantiles)\n",
        "    perf = lambda ms: ms # TFlops = 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
        "    return perf(ms), perf(max_ms), perf(min_ms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d23e990",
      "metadata": {
        "id": "5d23e990"
      },
      "outputs": [],
      "source": [
        "configs = [\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
        "        x_vals=[2 ** i for i in range(7)],  # Different possible values for `x_name`\n",
        "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
        "        # Possible values for `line_arg`\n",
        "        line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
        "        line_names=[ref_lib, \"Triton\"],  # Line styles\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
        "        ylabel=\"ms\",  # Label name for the y-axis\n",
        "        plot_name=\"GeMV-performance-\" + (\"fp16\"),  # Name for the plot, used also as a file name for saving the plot.\n",
        "        args={\"provider_funcs\": {\"triton\": matmul, \"cublas\": torch.matmul}},\n",
        "    ),\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
        "        x_vals=[1024 * i for i in range(4, 21)],  # Different possible values for `x_name`\n",
        "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
        "        # Possible values for `line_arg`\n",
        "        line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
        "        line_names=[ref_lib, \"Triton\"],  # Line styles\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
        "        ylabel=\"ms\",  # Label name for the y-axis\n",
        "        plot_name=\"GeMM-performance-\" + (\"fp16\"),  # Name for the plot, used also as a file name for saving the plot.\n",
        "        args={\"provider_funcs\": {\"triton\": matmul, \"cublas\": torch.matmul}},\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "triton.testing.perf_report(configs)(benchmark).run(show_plots=True, print_data=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c7e6dc",
      "metadata": {
        "id": "61c7e6dc"
      },
      "source": [
        "## HW\n",
        "\n",
        "### Dynamic W8A8 GeMM\n",
        "\n",
        "> **Важный дисклеймер:** пожалуйста, выполняйте ДЗ к google colab на среде T4, потому что на ней эта домашка оттестирована и уже установлены правильные зависимости по умолчанию, это сэкономит вам кучу времени.\n",
        "\n",
        "Для непослушных: на другом типе видеокарт (H100) код придется сильно переписывать, чтобы учесть архитектурные особенности для максимальной производительности (см. \"менее полезное, но интересное\" выше в семинаре).\n",
        "\n",
        "> **Важный дисклеймер 2:** для выполнения ДЗ нужно запустить код семинара выше, в нем есть нужные helper функции.\n",
        "\n",
        "Useful resources:\n",
        "- [Matrix Multiplication Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)\n",
        "- [Deep Dive on CUTLASS Ping-Pong GEMM Kernel](https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/)\n",
        "- [Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton](https://pytorch.org/blog/accelerating-gemms-triton/)\n",
        "\n",
        "![](https://habrastorage.org/webt/xq/r5/8a/xqr58aw0gd6tdm-j45yqad67w1a.png)\n",
        "\n",
        "В этом задании вам нужно будет реализовать быструю операцию квантизации в per-row режиме:\n",
        "1. Для этого нужно опять же заполнить пропуски возле `YOUR CODE HERE`, пожалуйста не удаляйте эти комментарии с заданием и обозначением, это облегчает проверку\n",
        "2. Рядом с каждым пропуском есть комментарий с `# !!! TASK: ...`, который поможет разобраться что именно нужно написать\n",
        "3. На иллюстрации выше как раз показано по каким именно размерностям для матриц активаций и весов считаются scales\n",
        "4. Формула для `scales = tensor.abs().max(axis=axis) / INT8_max_value`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10259127",
      "metadata": {
        "id": "10259127"
      },
      "outputs": [],
      "source": [
        "@triton.autotune(\n",
        "    configs=[\n",
        "        triton.Config({}, num_stages=2, num_warps=8),\n",
        "        triton.Config({}, num_stages=2, num_warps=4),\n",
        "        triton.Config({}, num_stages=2, num_warps=2),\n",
        "        triton.Config({}, num_stages=2, num_warps=1),\n",
        "     ],\n",
        "    key=['K'],\n",
        ")\n",
        "@triton.jit\n",
        "def quantize_int8_perrow_kernel(\n",
        "    fpa_ptr, a_ptr, as_ptr,\n",
        "    M, K,\n",
        "    stride_fpam, stride_fpak,\n",
        "    stride_am, stride_ak,\n",
        "    stride_asm,\n",
        "    # Meta-parameters\n",
        "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "):\n",
        "    pid_m = tl.program_id(axis=0)\n",
        "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
        "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
        "\n",
        "    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n",
        "    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
        "    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
        "\n",
        "        # !!! TASK: calc maximum absolute value of each row of fpa and update a_max\n",
        "        # YOUR CODE HERE\n",
        "        a_max = ...\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n",
        "\n",
        "    # !!! TASK: divide a_max by max positive INT8 value\n",
        "    # YOUR CODE HERE\n",
        "    a_scale = ...\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
        "        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
        "\n",
        "        # !!! TASK: divide fpa by a_scale and convert to INT8\n",
        "        # YOUR CODE HERE\n",
        "        inta = ...\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n",
        "        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n",
        "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
        "    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n",
        "    tl.store(as_ptr + as_offs, a_scale)\n",
        "\n",
        "\n",
        "def quantize_int8_perrow(fpa):\n",
        "    a = torch.empty(fpa.shape, device=fpa.device, dtype=torch.int8)\n",
        "    a_scale = torch.empty(fpa.shape[0], device=fpa.device, dtype=fpa.dtype)\n",
        "    M, K = fpa.shape\n",
        "    BLOCK_SIZE_M = 1\n",
        "    BLOCK_SIZE_K = triton.next_power_of_2(K)\n",
        "    grid = (M // BLOCK_SIZE_M,)\n",
        "    quantize_int8_perrow_kernel[grid](\n",
        "        fpa, a, a_scale,\n",
        "        M, K,\n",
        "        fpa.stride(0), fpa.stride(1),\n",
        "        a.stride(0), a.stride(1),\n",
        "        a_scale.stride(0),\n",
        "        BLOCK_SIZE_M, BLOCK_SIZE_K,\n",
        "    )\n",
        "    return a, a_scale\n",
        "\n",
        "\n",
        "def quantize_int8(weight, axis=0, tp_rank=0):\n",
        "    # Weight shape: [H1, H2]\n",
        "    # Scale shape: [H2]\n",
        "\n",
        "    # !!! TASK: calculate scale by taking maximum over axis and saving dims and divide on maximum positive INT8 value\n",
        "    # YOUR CODE HERE\n",
        "    scale = ...\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # !!! TASK: divide weight by scale and convert to int8\n",
        "    # YOUR CODE HERE\n",
        "    weight = ...\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # col major will accelerate i8xi8 kernel.\n",
        "    if axis == 0:\n",
        "        weight = weight.t().contiguous().t()\n",
        "    scale = scale.squeeze(axis)\n",
        "    return weight.contiguous().cuda(tp_rank), scale.contiguous().cuda(tp_rank)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e3eb491",
      "metadata": {
        "id": "3e3eb491"
      },
      "source": [
        "Далее нам предстоит реализовать уже быструю операцию для per-row W8A8 GeMM:\n",
        "1. Для этого нужно опять же заполнить пропуски возле `YOUR CODE HERE`, пожалуйста не удаляйте эти комментарии с заданием и обозначением, это облегчает проверку\n",
        "2. Рядом с каждым пропуском есть комментарий с `# !!! TASK: ...`, который поможет разобраться что именно нужно написать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac639944",
      "metadata": {
        "id": "ac639944"
      },
      "outputs": [],
      "source": [
        "@triton.autotune(\n",
        "    configs=get_autotune_config(),\n",
        "    key=['M', 'N', 'K'],\n",
        "    reset_to_zero=['c_ptr']\n",
        ")\n",
        "@triton.jit\n",
        "def perrow_w8a8_matmul_kernel(\n",
        "    # Pointers to matrices\n",
        "    a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr,\n",
        "    # Matrix dimensions\n",
        "    M, N, K,\n",
        "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
        "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
        "    # by to get the element one row down (A has M rows).\n",
        "    stride_am, stride_ak,\n",
        "    stride_asm,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_bsn,\n",
        "    stride_cm, stride_cn,\n",
        "    # Meta-parameters\n",
        "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
        "    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"Kernel for computing the matmul C = A x B.\n",
        "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
        "    \"\"\"\n",
        "    # -----------------------------------------------------------\n",
        "    # Map program ids `pid` to the block of C it should compute.\n",
        "    # This is done in a grouped ordering to promote L2 data reuse.\n",
        "    # See above `L2 Cache Optimizations` section for details.\n",
        "    pid = tl.program_id(axis=0)\n",
        "    pid_sp_k = tl.program_id(axis=1)\n",
        "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
        "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
        "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
        "    group_id = pid // num_pid_in_group\n",
        "    first_pid_m = group_id * GROUP_SIZE_M\n",
        "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
        "    pid_m = first_pid_m + (pid % group_size_m)\n",
        "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Create pointers for the first blocks of A and B.\n",
        "    # We will advance this pointer as we move in the K direction\n",
        "    # and accumulate\n",
        "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
        "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
        "    # See above `Pointer Arithmetics` section for details\n",
        "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
        "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
        "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
        "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
        "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
        "    as_ptrs = as_ptr + offs_am * stride_asm\n",
        "    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n",
        "    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n",
        "    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n",
        "    # -----------------------------------------------------------\n",
        "    # Iterate to compute a block of the C matrix.\n",
        "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
        "    # of fp32 values for higher accuracy.\n",
        "    # `accumulator` will be converted back to fp16 after the loop.\n",
        "\n",
        "    # !!! TASK: create accumulator of int32 dtype\n",
        "    # YOUR CODE HERE\n",
        "    accumulator = ...\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n",
        "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
        "        # If it is out of bounds, set it to 0.\n",
        "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
        "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
        "        # We accumulate along the K dimension.\n",
        "\n",
        "        # !!! TASK: update accumulator with a @ b\n",
        "        # YOUR CODE HERE\n",
        "        accumulator += ...\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Advance the ptrs to the next K block.\n",
        "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n",
        "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n",
        "    # You can fuse arbitrary activation functions here\n",
        "    # while the accumulator is still in FP32!\n",
        "\n",
        "    # !!! TASK: dequantize the accumulator with a_scale and b_scale (outer product) and convert to c_ptr.dtype.element_ty\n",
        "    # YOUR CODE HERE\n",
        "    c = ...\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # Write back the block of the output matrix C with masks.\n",
        "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
        "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
        "    if SPLIT_K == 1:\n",
        "        tl.store(c_ptrs, c, mask=c_mask)\n",
        "    else:\n",
        "        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
        "\n",
        "\n",
        "def matmul_int8(a, a_scale, b, b_scale, out=None):\n",
        "    # Check constraints.\n",
        "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
        "    M, K = a.shape\n",
        "    K, N = b.shape\n",
        "    # Allocates output.\n",
        "    if out == None:\n",
        "        c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n",
        "    else:\n",
        "        c = out.fill_(0.)\n",
        "    grid = lambda META: (\n",
        "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
        "        META['SPLIT_K'],\n",
        "    )\n",
        "    perrow_w8a8_matmul_kernel[grid](\n",
        "        a, a_scale, b, b_scale, c,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        a_scale.stride(0),\n",
        "        b.stride(0), b.stride(1),\n",
        "        b_scale.stride(0),\n",
        "        c.stride(0), c.stride(1),\n",
        "    )\n",
        "    return c\n",
        "\n",
        "\n",
        "def matmul_quantize_int8(fpa, b, b_scale, out=None):\n",
        "    # !!! TASK: quantize fpa to int8 and call matmul_int8\n",
        "    # YOUR CODE HERE\n",
        "    a, a_scale = ...\n",
        "    return ...\n",
        "    # YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1c3615",
      "metadata": {
        "id": "4e1c3615"
      },
      "source": [
        "Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc029389",
      "metadata": {
        "id": "bc029389"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "cos = torch.nn.CosineSimilarity(0)\n",
        "\n",
        "a = inps[0][0].to(torch.float16).cuda()\n",
        "b = weight.to(torch.float16).cuda()\n",
        "\n",
        "int_a, scale_a = quantize_int8_perrow(a)\n",
        "int_b, scale_b = quantize_int8(b, axis=0)\n",
        "\n",
        "\n",
        "triton_output = matmul_int8(int_a, scale_a, int_b, scale_b)\n",
        "torch_output = torch.matmul(a, b)\n",
        "\n",
        "if (torch_output.float() - triton_output.float()).abs().mean() < 0.03 and torch.quantile((torch_output.float() - triton_output.float()).abs(), 0.95) < 0.07:\n",
        "    print(\"✅ Triton FP8 and Torch match\")\n",
        "else:\n",
        "    print(\"❌ Triton FP8 and Torch differ\")\n",
        "    print(\"Quantization cos: \", cos((int_a * scale_a.unsqueeze(1)).flatten().to(torch.float32), a.flatten().to(torch.float32)).item())\n",
        "\n",
        "    print('=' * 50)\n",
        "    print(f\"triton_output_with_fp8={triton_output}\")\n",
        "    print(f\"torch_output={torch_output}\")\n",
        "\n",
        "    print('=' * 50)\n",
        "    print(\"infs in triton:\", (triton_output).isinf().sum())\n",
        "    print(\"infs in torch:\", (torch_output).isinf().sum())\n",
        "\n",
        "    print('=' * 50)\n",
        "    print(\"Output cos:\", cos(triton_output.flatten().to(torch.float32), torch_output.flatten().to(torch.float32)).item())\n",
        "    print(((triton_output - torch_output).abs() >= 0.5).sum())\n",
        "    print(((triton_output - torch_output).abs() / (torch_output.abs() + 1e-5)))\n",
        "    assert False, \"Triton and Torch differ\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36d4fde",
      "metadata": {
        "id": "a36d4fde"
      },
      "source": [
        "Sanity checks on perf:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215f4800",
      "metadata": {
        "id": "215f4800"
      },
      "outputs": [],
      "source": [
        "def test_perf_quantize(M: int, K: int, iters: int = 256, thr: float = 1.3):\n",
        "    torch.manual_seed(0)\n",
        "    print(f\"M: {M} K: {K}\")\n",
        "\n",
        "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
        "    # warmup\n",
        "    for _ in range(10):\n",
        "        int_a, a_scale = quantize_int8(a, 1)\n",
        "        int_a, a_scale = quantize_int8_perrow(a)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    t1 = time.time()\n",
        "    for _ in range(iters):\n",
        "        int_a, a_scale = quantize_int8_perrow(a)\n",
        "    torch.cuda.synchronize()\n",
        "    t2 = time.time()\n",
        "    for _ in range(iters):\n",
        "        int_a, a_scale = quantize_int8(a, axis=1)\n",
        "    torch.cuda.synchronize()\n",
        "    t3 = time.time()\n",
        "\n",
        "    torch_time = (t3 - t2) / iters\n",
        "    triton_time = (t2 - t1) / iters\n",
        "\n",
        "    print(f\"Torch time cost: {torch_time}\")\n",
        "    print(f\"Triton time cost: {triton_time}\")\n",
        "    assert torch_time / triton_time > thr, f\"Must get at least {thr}x speedup\"\n",
        "    return triton_time, torch_time\n",
        "\n",
        "\n",
        "def test_perf_matmul_int8(M, K, N, iters: int = 512, thr: float = 0.99):\n",
        "    print(\"M: {} K: {} N: {}\".format(M, K, N))\n",
        "    torch.manual_seed(0)\n",
        "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
        "    b = torch.randn((K, N), device='cuda', dtype=torch.float16).contiguous()\n",
        "    int_b, scale_b = quantize_int8(b, axis=0)\n",
        "    for _ in range(10):\n",
        "        # int_a, a_scale = quantize_int8(a, 1)\n",
        "        int_a, a_scale = quantize_int8_perrow(a)\n",
        "        triton_output = matmul_int8(int_a, a_scale, int_b, scale_b)\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    for _ in range(iters):\n",
        "        #int_a, a_scale, _ = quantize_int8(a, 1)\n",
        "        int_a, a_scale = quantize_int8_perrow(a)\n",
        "    torch.cuda.synchronize()\n",
        "    qt2 = time.time()\n",
        "    for _ in range(iters):\n",
        "        triton_output = matmul_int8(int_a, a_scale, int_b, scale_b)\n",
        "    torch.cuda.synchronize()\n",
        "    t2 = time.time()\n",
        "    quant_time = qt2 - t1\n",
        "    triton_time = t2 - qt2\n",
        "    triton_tflops = 2 * M * N * K * 1e-12 / (triton_time / iters)\n",
        "    quant_bandwith = 2 * M * K * 1e-9 / (quant_time / iters)\n",
        "    print(\"Triton time cost: {} (tflops {}) + quant: {} (bandwidth {})\".format(\n",
        "        triton_time, triton_tflops, quant_time, quant_bandwith))\n",
        "    for _ in range(10):\n",
        "        torch_output = torch.matmul(a, b)\n",
        "    torch.cuda.synchronize()\n",
        "    iters = 512\n",
        "    t1 = time.time()\n",
        "    for _ in range(iters):\n",
        "        torch_output = torch.matmul(a, b)\n",
        "    torch.cuda.synchronize()\n",
        "    t2 = time.time()\n",
        "    torch_time = t2 - t1\n",
        "    torch_tflops = 2 * M * N * K * 1e-12 / (torch_time / iters)\n",
        "    print(\"Torch time cost: {} (tflops {})\".format(t2 - t1, torch_tflops))\n",
        "\n",
        "    assert torch_time / triton_time > thr, f\"Must get at least {thr}x speedup\"\n",
        "    return triton_time, torch_time, quant_time\n",
        "\n",
        "\n",
        "def test_perf_model_layer(bs, seq_len, hidden, inter, tp, thr: float = 0.99):\n",
        "    st1 = 0\n",
        "    st2 = 0\n",
        "    st3 = 0\n",
        "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, hidden, hidden * 3 // tp, thr=thr)\n",
        "    test_perf_quantize(bs * seq_len, hidden, thr=thr)\n",
        "    st1 += t1\n",
        "    st2 += t2\n",
        "    st3 += t3\n",
        "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, hidden // tp, hidden, thr=thr)\n",
        "    test_perf_quantize(bs * seq_len, hidden // tp, thr=thr)\n",
        "    st1 += t1\n",
        "    st2 += t2\n",
        "    st3 += t3\n",
        "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, hidden, inter * 2 // tp, thr=thr)\n",
        "    st1 += t1\n",
        "    st2 += t2\n",
        "    st3 += t3\n",
        "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, inter // tp, hidden, thr=thr)\n",
        "    test_perf_quantize(bs * seq_len, inter // tp, thr=thr)\n",
        "    st1 += t1\n",
        "    st2 += t2\n",
        "    st3 += t3\n",
        "    print(\"Triton time {} Torch time {} Quant time {}\".format(st1, st2, st3))\n",
        "    assert st2 / st1 > thr, f\"Must get at least {thr}x speedup\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c875e72",
      "metadata": {
        "id": "8c875e72"
      },
      "outputs": [],
      "source": [
        "bs = 32\n",
        "hidden = model.config.hidden_size\n",
        "inter  = model.config.intermediate_size\n",
        "prefill_len = 512\n",
        "decode_len = 1\n",
        "tp = 1\n",
        "\n",
        "test_perf_model_layer(bs, prefill_len, hidden, inter, tp, thr=1.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7e80e5",
      "metadata": {
        "id": "8f7e80e5"
      },
      "source": [
        "**Note**: в последней строке видим интегральное ускорение по слою на prefill стадии в целых ~x1.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10140fdb",
      "metadata": {
        "id": "10140fdb"
      },
      "outputs": [],
      "source": [
        "test_perf_model_layer(bs, decode_len, hidden, inter, tp, thr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2620be8b",
      "metadata": {
        "id": "2620be8b"
      },
      "source": [
        "**Note**: в последней строке видим интегральное ускорение по слою на decode стадии, оно получилось сильно меньше (x1.03), для того чтобы выжать на decode больше уже нужна реализация на CUDA\n",
        "\n",
        "Benchmarks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be18cb2b",
      "metadata": {
        "id": "be18cb2b"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def benchmark_quant_gemm(M, provider, provider_funcs):\n",
        "    N = weight.shape[0]\n",
        "    K = weight.shape[1]\n",
        "\n",
        "    fpa = prepare_a(M).to(torch.float16).cuda().contiguous()\n",
        "    fpb = weight.data.T.to(torch.float16).contiguous()\n",
        "    b, b_scale = quantize_int8(fpb, axis=0)\n",
        "\n",
        "    assert fpa.shape == (M, K), f\"{fpa.shape} != {(M, K)}\"\n",
        "    assert b.shape == (K, N), b.shape\n",
        "\n",
        "    quantiles = [0.5, 0.2, 0.8]\n",
        "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: provider_funcs[provider](fpa, fpb, b, b_scale), quantiles=quantiles)\n",
        "    perf = lambda ms: ms # TFlops = 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
        "    return perf(ms), perf(max_ms), perf(min_ms)\n",
        "\n",
        "provider_funcs = {\n",
        "    \"triton int8\": lambda fpa, fpb, b, b_scale: matmul_quantize_int8(fpa, b, b_scale),\n",
        "    \"triton fp16\": lambda fpa, fpb, b, b_scale: matmul(fpa, fpb),\n",
        "    \"cublas fp16\": lambda fpa, fpb, b, b_scale: torch.matmul(fpa, fpb)\n",
        "}\n",
        "\n",
        "configs = [\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
        "        x_vals=[2 ** i for i in range(7)],  # Different possible values for `x_name`\n",
        "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
        "        # Possible values for `line_arg`\n",
        "        line_vals=[\"cublas fp16\", \"triton int8\", \"triton fp16\"],  # Label name for the lines\n",
        "        line_names=[\"cuBLAS FP16\", \"Triton INT8\", \"Triton FP16\"],  # Line styles\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\"), (\"red\", \"-\")],\n",
        "        ylabel=\"ms\",  # Label name for the y-axis\n",
        "        plot_name=\"GeMV-performance-\" + (\"FP16 vs INT8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
        "        args={\"provider_funcs\": provider_funcs},\n",
        "    ),\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
        "        x_vals=[1024 * i for i in range(4, 21)],  # Different possible values for `x_name`\n",
        "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
        "        # Possible values for `line_arg`\n",
        "        line_vals=[\"cublas fp16\", \"triton int8\", \"triton fp16\"],  # Label name for the lines\n",
        "        line_names=[\"cuBLAS FP16\", \"Triton INT8\", \"Triton FP16\"],  # Line styles\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\"), (\"red\", \"-\")],\n",
        "        ylabel=\"ms\",  # Label name for the y-axis\n",
        "        plot_name=\"GeMM-performance-\" + (\"FP16 vs INT8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
        "        args={\"provider_funcs\": provider_funcs},\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "triton.testing.perf_report(configs)(benchmark_quant_gemm).run(show_plots=True, print_data=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bda8c54a",
      "metadata": {
        "id": "bda8c54a"
      },
      "source": [
        "**Note:** Triton FP16 побили по скорости с большим запасом, и даже смогли ускориться на prefill на больших контекстах относительно cuBLASm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e557782",
      "metadata": {
        "id": "4e557782"
      },
      "outputs": [],
      "source": [
        "provider_funcs = {\n",
        "    \"torch\": lambda a, b: quantize_int8(a),\n",
        "    \"triton\": lambda a, b: quantize_int8_perrow(a)\n",
        "}\n",
        "\n",
        "configs = [\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
        "        x_vals=[2 ** i for i in range(7)],  # Different possible values for `x_name`\n",
        "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
        "        # Possible values for `line_arg`\n",
        "        line_vals=[\"torch\", \"triton\"],  # Label name for the lines\n",
        "        line_names=[\"Torch\", \"Triton\"],  # Line styles\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
        "        ylabel=\"ms\",  # Label name for the y-axis\n",
        "        plot_name=\"Quantize-performance-\" + (\"decoding\"),  # Name for the plot, used also as a file name for saving the plot.\n",
        "        args={\"provider_funcs\": provider_funcs},\n",
        "    ),\n",
        "    triton.testing.Benchmark(\n",
        "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
        "        x_vals=[256 * i for i in range(4, 21)],  # Different possible values for `x_name`\n",
        "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
        "        # Possible values for `line_arg`\n",
        "        line_vals=[\"torch\", \"triton\"],  # Label name for the lines\n",
        "        line_names=[\"Torch\", \"Triton\"],  # Line styles\n",
        "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
        "        ylabel=\"ms\",  # Label name for the y-axis\n",
        "        plot_name=\"Quantize-performance-\" + (\"prefill\"),  # Name for the plot, used also as a file name for saving the plot.\n",
        "        args={\"provider_funcs\": provider_funcs},\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "triton.testing.perf_report(configs)(benchmark).run(show_plots=True, print_data=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16da6ebe",
      "metadata": {
        "id": "16da6ebe"
      },
      "source": [
        "**Note:** реализация на торче для квантизации крайне неэффективная и квантизация на Triton-е значимо лучше и на prefill и на decode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da5870bf",
      "metadata": {
        "id": "da5870bf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}