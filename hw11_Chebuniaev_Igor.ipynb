{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6831fe55",
   "metadata": {},
   "source": [
    "# KV Cache - 10 баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29b7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045cc82",
   "metadata": {},
   "source": [
    "Представим, что у нас есть очень простая мини-LLM:\n",
    "1. Она эмбеддит токены\n",
    "2. Считает аттеншн (обычный, не multihead) токенов друг с другом с causal mask (не смотрит в будущее!)\n",
    "3. После этого выходы attention подаются в линейный слой для получения распределеняи по словарю\n",
    "\n",
    "На примере такой модели давайте попробуем имплементировать KV-Cache.\n",
    "\n",
    "Ниже за вас написан метод forward - этот метод это обычный forward нейросети, который считает attention всех токенов со всеми токенами.\n",
    "\n",
    "Вам же нужно имплементировать метод forward_kv_cache, который принимает:\n",
    "* x - тензор размерности \\[batch, seq_len = 1\\]\n",
    "* prev_output - выход модели с предыдущего шага типа Output\n",
    "\n",
    "Метод forward_kv_cache должен выполнять следующие действия:\n",
    "1. Эмбеддинг токена x\n",
    "2. Проекция x в QKV\n",
    "3. Расширение k_cache и v_cache из prev_ouptut k/v проекциями x\n",
    "4. Подсчет аттеншена между q_x и k_cache и v_cache\n",
    "5. Конкатенация аттеншена в prev_output.attention_weights\n",
    "6. Возврат logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce42ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before generation tensor([[1, 3, 0]])\n",
      "After generation no cache tensor([[1, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]])\n",
      "After generation kv cache tensor([[1, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Output:\n",
    "    logits: torch.Tensor = None\n",
    "    k_cache: torch.Tensor = None\n",
    "    v_cache: torch.Tensor = None\n",
    "    attn_weights: torch.Tensor = None\n",
    "    \n",
    "\n",
    "\n",
    "class SimpleAttentionLLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.lin = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        q = self.W_Q(x)\n",
    "        k = self.W_K(x)\n",
    "        v = self.W_V(x)\n",
    "        \n",
    "        attn_scores = torch.matmul(q, k.permute(0, 2, 1))\n",
    "        mask = torch.tril(torch.ones_like(attn_scores))\n",
    "        attn_scores = attn_scores.masked_fill(~mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=2)\n",
    "        weights_V = torch.matmul(attn_weights, v)\n",
    "        logits = self.lin(weights_V)\n",
    "        return Output(\n",
    "            logits=logits,\n",
    "            k_cache=k,\n",
    "            v_cache=v,\n",
    "            attn_weights=attn_weights # batch, seq_len, seq_len\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward_kv_cache(self, x, prev_output):\n",
    "        # эмбеддинг токена x\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # 1. Проецируем x в q, k, v\n",
    "        q = self.W_Q(x)\n",
    "        k = self.W_K(x)\n",
    "        v = self.W_V(x)\n",
    "        \n",
    "        # print(f\"{q.shape=}\")\n",
    "        # print(f\"{k.shape=}\")\n",
    "\n",
    "        # берем старый кэш\n",
    "        k_cache = prev_output.k_cache\n",
    "        v_cache = prev_output.v_cache\n",
    "\n",
    "        # расширяем его состояниями k, v последнего токена\n",
    "        # с помощью torch.cat\n",
    "        k_cache_new = torch.cat(tensors=(k_cache, k), dim=1)\n",
    "        v_cache_new = torch.cat(tensors=(v_cache, v), dim=1)\n",
    "        \n",
    "        # считаем attention_score, то есть матричное умножение между q и k_cache_new\n",
    "        \n",
    "        attn_scores = q @ k_cache_new.permute(0, 2, 1)\n",
    "    \n",
    "        \n",
    "        # считаем softmax\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        # домножаем softmax на V\n",
    "        \n",
    "        # print(f\"{attn_scores.shape=}\")\n",
    "        # print(f\"{v_cache_new.shape=}\")\n",
    "        weights_V = attn_weights @ v_cache_new\n",
    "        logits = self.lin(weights_V)\n",
    "        \n",
    "        batch = x.size(0)\n",
    "        seq_len = k_cache.size(1)\n",
    "        \n",
    "        attn_weights_old = prev_output.attn_weights\n",
    "        zeros_right = torch.zeros(batch, seq_len, 1)\n",
    "        # добавляем нули в аттеншене так, чтобы у старых токенов был нулевой аттеншн на новый токен\n",
    "        attn_weights_all = torch.cat((attn_weights_old, zeros_right), dim=2)\n",
    "        # Добавляем аттеншн текущего нового токена по старым\n",
    "        attn_weights_all = torch.cat((attn_weights_all, attn_weights), dim=1)\n",
    "        \n",
    "        return Output(\n",
    "            logits=logits,\n",
    "            k_cache=k_cache_new,\n",
    "            v_cache=v_cache_new,\n",
    "            attn_weights=attn_weights_all\n",
    "        )\n",
    "        \n",
    "torch.manual_seed(1)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "d_model = 128\n",
    "vocab_size = 7\n",
    "\n",
    "layer = SimpleAttentionLLM(d_model, vocab_size)\n",
    "for param in layer.parameters():\n",
    "    nn.init.normal_(param)\n",
    "\n",
    "x = torch.randint(0, 7, (batch_size, seq_len))\n",
    "x_copy = x.clone()\n",
    "\n",
    "print(\"Before generation\", x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        outputs = layer(x)\n",
    "        logits = outputs.logits\n",
    "        # берем последний токен, т.к. по нему предсказываем!\n",
    "        next_token = logits[:, -1].argmax(dim=1, keepdim=True)\n",
    "        x = torch.cat((x, next_token), dim=1) # добавляем новый токен по размерности seq_len\n",
    "\n",
    "no_cache_output = outputs\n",
    "print(\"After generation no cache\", x)\n",
    "\n",
    "\n",
    "x = x_copy.clone()\n",
    "final_tokens = x.clone()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # prefill\n",
    "    outputs = layer(x)\n",
    "    logits = outputs.logits\n",
    "    # берем последний токен, т.к. по нему предсказываем!\n",
    "    next_token = logits[:, -1].argmax(dim=1, keepdim=True)\n",
    "    # обратите внимание, что раньше было 10 шагов!\n",
    "    final_tokens = torch.cat((final_tokens, next_token), dim=1)\n",
    "    for i in range(9):\n",
    "        outputs = layer.forward_kv_cache(next_token, outputs)\n",
    "        next_token = outputs.logits[:, -1].argmax(dim=1, keepdim=True)\n",
    "        final_tokens = torch.cat((final_tokens, next_token), dim=1)\n",
    "    \n",
    "print(\"After generation kv cache\", final_tokens)\n",
    "\n",
    "cache_outputs = outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62b506",
   "metadata": {},
   "source": [
    "Выведем матрицы attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305f232d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attention kv cache')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(no_cache_output.attn_weights[0])\n",
    "axes[0].set_title(\"Attention no cache\")\n",
    "axes[1].imshow(cache_outputs.attn_weights[0])\n",
    "axes[1].set_title(\"Attention kv cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326b101e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Downloading vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: triton in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: regex in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (2024.11.6)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-6.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: psutil in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (7.0.0)\n",
      "Requirement already satisfied: sentencepiece in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: numpy in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (4.67.1)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.51.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (4.53.0.dev0)\n",
      "Collecting huggingface-hub>=0.32.0 (from huggingface-hub[hf_xet]>=0.32.0->vllm)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tokenizers>=0.21.1 (from vllm)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: protobuf in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (5.29.3)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (3.11.13)\n",
      "Collecting openai>=1.52.0 (from vllm)\n",
      "  Downloading openai-1.82.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic>=2.9 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (2.10.6)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (11.1.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Downloading llguidance-0.7.26-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting outlines==0.1.11 (from vllm)\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.19 (from vllm)\n",
      "  Downloading xgrammar-0.1.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (3.17.0)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (26.2.1)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Downloading gguf-0.17.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Downloading mistral_common-1.5.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pyyaml in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: einops in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (0.8.1)\n",
      "Collecting compressed-tensors==0.9.4 (from vllm)\n",
      "  Downloading compressed_tensors-0.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm)\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (3.2.1)\n",
      "Requirement already satisfied: scipy in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from vllm) (1.15.2)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n",
      "  Downloading opentelemetry_exporter_otlp-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n",
      "  Downloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading ray-2.46.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting torch==2.7.0 (from vllm)\n",
      "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchaudio==2.7.0 (from vllm)\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision==0.22.0 (from vllm)\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.30 (from vllm)\n",
      "  Downloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: referencing in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
      "  Downloading airportsdata-20250523-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
      "  Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0->vllm)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch==2.7.0->vllm) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from torch==2.7.0->vllm) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->vllm)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton\n",
      "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from triton) (68.2.2)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm) (24.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm)\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.52.0->vllm)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm)\n",
      "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.26.0->vllm)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.26.0->vllm)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.33.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.33.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk>=1.26.0->vllm)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (2.27.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading cupy_cuda12x-13.4.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2025.1.31)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from aiohttp->vllm) (1.18.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.52.0->vllm) (1.2.2)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api>=1.26.0->vllm)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.14.7-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: httpcore==1.* in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.26.0->vllm) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.23.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.7.0->vllm) (1.3.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Downloading vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl (377.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.2/377.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0mm\n",
      "\u001b[?25hDownloading compressed_tensors-0.9.4-py3-none-any.whl (100 kB)\n",
      "Downloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading gguf-0.17.0-py3-none-any.whl (95 kB)\n",
      "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Downloading llguidance-0.7.26-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "Downloading mistral_common-1.5.6-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.82.1-py3-none-any.whl (720 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.5/720.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp-1.33.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.33.1-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "Downloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl (5.6 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading ray-2.46.0-cp310-cp310-manylinux2014_x86_64.whl (68.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blake3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Downloading cachetools-6.0.0-py3-none-any.whl (10 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading watchfiles-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading airportsdata-20250523-py3-none-any.whl (912 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading cupy_cuda12x-13.4.1-cp310-cp310-manylinux2014_x86_64.whl (104.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading rich_toolkit-0.14.7-py3-none-any.whl (24 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: py-cpuinfo, nvidia-cusparselt-cu12, fastrlock, blake3, wrapt, websockets, uvloop, uvicorn, triton, sympy, shellingham, python-multipart, python-dotenv, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, msgspec, msgpack, llvmlite, llguidance, lark, jiter, interegular, importlib-metadata, httptools, hf-xet, grpcio, googleapis-common-protos, gguf, dnspython, distro, diskcache, cupy-cuda12x, cloudpickle, cachetools, astor, airportsdata, watchfiles, tiktoken, starlette, opentelemetry-exporter-otlp-proto-common, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, huggingface-hub, email-validator, depyf, deprecated, typer, tokenizers, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-api, openai, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, ray, outlines_core, opentelemetry-semantic-conventions, mistral_common, fastapi-cli, xgrammar, xformers, torchvision, torchaudio, outlines, opentelemetry-sdk, compressed-tensors, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, vllm\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 5.2.0\n",
      "    Uninstalling importlib-metadata-5.2.0:\n",
      "      Successfully uninstalled importlib-metadata-5.2.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.31.4\n",
      "    Uninstalling huggingface-hub-0.31.4:\n",
      "      Successfully uninstalled huggingface-hub-0.31.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "circuitsvis 0.0.0 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 8.6.1 which is incompatible.\n",
      "transformer-lens 2.15.0 requires typeguard<5.0,>=4.2, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed airportsdata-20250523 astor-0.8.1 blake3-1.0.5 cachetools-6.0.0 cloudpickle-3.1.1 compressed-tensors-0.9.4 cupy-cuda12x-13.4.1 deprecated-1.2.18 depyf-0.18.0 diskcache-5.6.3 distro-1.9.0 dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.12 fastapi-cli-0.0.7 fastrlock-0.8.3 gguf-0.17.0 googleapis-common-protos-1.70.0 grpcio-1.71.0 hf-xet-1.1.2 httptools-0.6.4 huggingface-hub-0.32.3 importlib-metadata-8.6.1 interegular-0.3.3 jiter-0.10.0 lark-1.2.2 llguidance-0.7.26 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.5.6 msgpack-1.1.0 msgspec-0.19.0 ninja-1.11.1.4 numba-0.61.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 openai-1.82.1 opencv-python-headless-4.11.0.86 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-exporter-otlp-proto-http-1.33.1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-semantic-conventions-ai-0.4.9 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pycountry-24.6.1 python-dotenv-1.1.0 python-multipart-0.0.20 ray-2.46.0 rich-toolkit-0.14.7 shellingham-1.5.4 starlette-0.46.2 sympy-1.14.0 tiktoken-0.9.0 tokenizers-0.21.1 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 typer-0.16.0 uvicorn-0.34.2 uvloop-0.21.0 vllm-0.9.0.1 watchfiles-1.0.5 websockets-15.0.1 wrapt-1.17.2 xformers-0.0.30 xgrammar-0.1.19\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45fc6",
   "metadata": {},
   "source": [
    "По возможности данное задание необходимо выполнять на сервере, чтобы было удобно запускать параллельно фреймворк и нагрузочное тестирование. Для удобства работы в jupyter сделан следующий трюк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-01 10:35:34 [__init__.py:243] Automatically detected platform cuda.\n",
      "INFO 06-01 10:35:37 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-01 10:35:37 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-01 10:35:37 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-01 10:35:38 [api_server.py:1289] vLLM API server version 0.9.0.1\n",
      "INFO 06-01 10:35:39 [cli_args.py:300] non-default args: {'dtype': 'half', 'max_model_len': 64, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 64, 'max_num_seqs': 1}\n",
      "INFO 06-01 10:35:49 [config.py:793] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-01 10:35:49 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=64.\n",
      "INFO 06-01 10:35:55 [__init__.py:243] Automatically detected platform cuda.\n",
      "INFO 06-01 10:35:58 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-01 10:35:58 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-01 10:35:58 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-01 10:35:58 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-01 10:35:58 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='microsoft/phi-1_5', speculative_config=None, tokenizer='microsoft/phi-1_5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=64, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=microsoft/phi-1_5, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 64}\n",
      "WARNING 06-01 10:35:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x720528f44f10>\n",
      "INFO 06-01 10:35:59 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-01 10:35:59 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-01 10:35:59 [gpu_model_runner.py:1531] Starting to load model microsoft/phi-1_5...\n",
      "INFO 06-01 10:35:59 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "ERROR 06-01 10:35:59 [core.py:500] EngineCore failed to start.\n",
      "ERROR 06-01 10:35:59 [core.py:500] Traceback (most recent call last):\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 491, in run_engine_core\n",
      "ERROR 06-01 10:35:59 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self._init_executor()\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.collective_rpc(\"load_model\")\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 06-01 10:35:59 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/utils.py\", line 2605, in run_method\n",
      "ERROR 06-01 10:35:59 [core.py:500]     return func(*args, **kwargs)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 164, in load_model\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.model_runner.load_model()\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1534, in load_model\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 58, in get_model\n",
      "ERROR 06-01 10:35:59 [core.py:500]     return loader.load_model(vllm_config=vllm_config,\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 273, in load_model\n",
      "ERROR 06-01 10:35:59 [core.py:500]     model = initialize_model(vllm_config=vllm_config,\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py\", line 61, in initialize_model\n",
      "ERROR 06-01 10:35:59 [core.py:500]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 317, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.model = PhiModel(vllm_config=vllm_config,\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 213, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 625, in make_layers\n",
      "ERROR 06-01 10:35:59 [core.py:500]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 626, in <listcomp>\n",
      "ERROR 06-01 10:35:59 [core.py:500]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 215, in <lambda>\n",
      "ERROR 06-01 10:35:59 [core.py:500]     lambda prefix: PhiLayer(\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 181, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.mlp = PhiMLP(config, quant_config)\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 153, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.fc2 = RowParallelLinear(\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 1199, in __init__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     self.quant_method.create_weights(\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "ERROR 06-01 10:35:59 [core.py:500]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 06-01 10:35:59 [core.py:500]   File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 06-01 10:35:59 [core.py:500]     return func(*args, **kwargs)\n",
      "ERROR 06-01 10:35:59 [core.py:500] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 8.19 MiB is free. Process 2623 has 81.80 MiB memory in use. Process 92539 has 2.20 GiB memory in use. Including non-PyTorch memory, this process has 1.51 GiB memory in use. Of the allocated memory 1.38 GiB is allocated by PyTorch, and 37.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 504, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 491, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/utils.py\", line 2605, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 164, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1534, in load_model\n",
      "    self.model = get_model(vllm_config=self.vllm_config)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 58, in get_model\n",
      "    return loader.load_model(vllm_config=vllm_config,\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 273, in load_model\n",
      "    model = initialize_model(vllm_config=vllm_config,\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py\", line 61, in initialize_model\n",
      "    return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 317, in __init__\n",
      "    self.model = PhiModel(vllm_config=vllm_config,\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 213, in __init__\n",
      "    self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 625, in make_layers\n",
      "    [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 626, in <listcomp>\n",
      "    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 215, in <lambda>\n",
      "    lambda prefix: PhiLayer(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 181, in __init__\n",
      "    self.mlp = PhiMLP(config, quant_config)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/models/phi.py\", line 153, in __init__\n",
      "    self.fc2 = RowParallelLinear(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 1199, in __init__\n",
      "    self.quant_method.create_weights(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "    weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 8.19 MiB is free. Process 2623 has 81.80 MiB memory in use. Process 92539 has 2.20 GiB memory in use. Including non-PyTorch memory, this process has 1.51 GiB memory in use. Of the allocated memory 1.38 GiB is allocated by PyTorch, and 37.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W601 10:36:00.594535458 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/bin/vllm\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py\", line 56, in main\n",
      "    args.dispatch_function(args)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py\", line 42, in cmd\n",
      "    uvloop.run(run_server(args))\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n",
      "    return loop.run_until_complete(wrapper())\n",
      "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n",
      "    return await main\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 1324, in run_server\n",
      "    async with build_async_engine_client(args) as engine_client:\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 153, in build_async_engine_client\n",
      "    async with build_async_engine_client_from_engine_args(\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "    return await anext(self.gen)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 185, in build_async_engine_client_from_engine_args\n",
      "    async_llm = AsyncLLM.from_vllm_config(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py\", line 157, in from_vllm_config\n",
      "    return cls(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py\", line 123, in __init__\n",
      "    self.engine_core = core_client_class(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 734, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 418, in __init__\n",
      "    self._wait_for_engine_startup(output_address, parallel_config)\n",
      "  File \"/home/igor/Documents/Ramis_homework/rami_env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 484, in _wait_for_engine_startup\n",
      "    raise RuntimeError(\"Engine core initialization failed. \"\n",
      "RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m server_process \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mProcess(target\u001b[38;5;241m=\u001b[39mstart_vllm_server)\n\u001b[1;32m     22\u001b[0m server_process\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwe are probably ready\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "\n",
    "def start_vllm_server():\n",
    "    # This function will run the `vllm` server command\n",
    "    # cmd = [\"vllm\", \"serve\", \"unsloth/gemma-2b-it\", \"--dtype\", \"half\"]\n",
    "    cmd = [\n",
    "        \"vllm\", \"serve\", \"microsoft/phi-1_5\", \n",
    "        # \"vllm\", \"serve\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \n",
    "        \"--dtype\", \"half\",\n",
    "        \"--gpu_memory_utilization\", \"0.8\",\n",
    "        \"--max-model-len\", \"32\",\n",
    "        \"--max-num-seqs\", \"1\",\n",
    "        \"--swap-space\", \"4\",\n",
    "        \"--max-num-batched-tokens\", \"32\"\n",
    "    ]\n",
    "    subprocess.run(cmd)\n",
    "\n",
    "\n",
    "server_process = multiprocessing.Process(target=start_vllm_server)\n",
    "server_process.start()\n",
    "time.sleep(60)\n",
    "print(\"we are probably ready\")\n",
    "\n",
    "import requests\n",
    "\n",
    "r = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
    "                #   \"model\": \"unsloth/gemma-2b-it\",\n",
    "                  # \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                  \"model\": \"microsoft/phi-1_5\",\n",
    "                  \"prompt\": \"Hello there\",\n",
    "                  \"max_tokens\": 10,\n",
    "                  \"temperature\": 0.8\n",
    "                  \n",
    "              })\n",
    "print(r.json())\n",
    "print(r.json()[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31f5df",
   "metadata": {},
   "source": [
    "# Нагрузочное тестирование - 25 баллов\n",
    "\n",
    "20 баллов - узнать, сколько запросов в секунду выдержит VLLM сервинг модели при ограничении latency в 5 секунд.\n",
    "Можно использовать самописную функцию через multiprocessing/threading, можно использовать любой готовый инструмент.\n",
    "\n",
    "В качестве пейлоадов предлагается брать тексты длины 100-128 и генерировать к ним не более 10 токенов. Сами пейлоады можно взять из любого датасета, например https://huggingface.co/datasets/Intel/orca_dpo_pairs.\n",
    "\n",
    "Об аргументах vllm serve можно почитать в документации https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
    "\n",
    "\n",
    "5 баллов - если сможете посчитать на этом пейлоаде ttft - time to first token, то есть сколько занимает prefill стадия генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def generate_responses_batch(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        messages_list,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.8,\n",
    "        device='cuda'\n",
    "    ) -> list[str]:\n",
    "\n",
    "    # Составляем промпт в LLM, используя apply_chat_template, чтобы проставились все нужные спец тоцены\n",
    "    input_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "    ### ваш код здесь\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_new_tokens\n",
    "      )\n",
    "    llm = LLM(model=model_name,\n",
    "              dtype=torch.half,\n",
    "              enforce_eager=True,\n",
    "              gpu_memory_utilization=.8,\n",
    "              # max_model_len = 4096*4,\n",
    "              # max_num_seqs=4,\n",
    "              # kv_cache_dtype=\"fp8\"\n",
    "              )\n",
    "    outputs = llm.generate(input_texts, sampling_params)\n",
    "\n",
    "\n",
    "    out_texts = []\n",
    "    for output in outputs:\n",
    "      prompt = output.prompt\n",
    "      out_texts.append(output.outputs[0].text)\n",
    "\n",
    "    return out_texts\n",
    "\n",
    "assert all(\n",
    "    \"London\" in candidate\n",
    "    for candidate in generate_responses_batch(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        messages_list=[[{\"role\": \"user\", \"content\": \"What is the capital of Great Britain?\"}]] * 4,\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886b031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c334b3",
   "metadata": {},
   "source": [
    "# Часть 2\n",
    "Далее предоставлено 2 варианта выполнения задания. Баллы будут начисляться только на один из них\n",
    "## Вариант 1. Квантизация - 15 баллов\n",
    "Квантизируйте модель в VLLM любым доступным способом, напишите, как сократились затраты памяти и как изменилась скорость инференса. Обязательно укажите, на какой видеокарте проводились замеры!\n",
    "Не забудьте про то, что квантизировать можно и kv cache.\n",
    "\n",
    "Внимательно проверьте и убедитесь, что ваш ускоритель поддержан в https://docs.vllm.ai/en/latest/quantization/supported_hardware.html\n",
    "\n",
    "## Вариант 2. Батчевалка - 15 баллов\n",
    "Предлагается написать сервер на питоне, который поддерживал бы батчевание запросов.\n",
    "\n",
    "Сервер принимает POST запрос с телом вида\n",
    "\n",
    "```json\n",
    "{\"text\": \"Hello there\"}\n",
    "```\n",
    "\n",
    "\n",
    "Необходимо написать сервер, который:\n",
    "1. Имел бы возможность работать с несколькими клиентами за раз (не блокировался бы на обработку одного запроса). Для этого можно использовать async, gevent, треды и т.д.\n",
    "2. Использовал бы батчовую обработку следующим образом:\n",
    "если пришло несколько запросов (для примера 2)\n",
    "\n",
    "```json\n",
    "запрос 1\n",
    "{\"text\": \"Hello there \"}\n",
    "запрос 2\n",
    "{\"text\": \"handsome\"}\n",
    "```\n",
    "\n",
    "то каждый клиент получал бы в ответ конкатенацию этих запросов (в произвольном порядке), т.е. оба клиента получили бы ответ\n",
    "```json\n",
    "{\"text\": \"Hello there handsome\"}\n",
    "или\n",
    "{\"text\": \"handsomeHello there \"}\n",
    "```\n",
    "\n",
    "Сервер должен иметь 2 конфигурируемых параметра:\n",
    "1. Максимальный размер батча, который он может обработать\n",
    "2. Максимальное время ожидания, которое ждет сэмпл перед обработкой. Т.е. если у нас батч размера 5, а у нас всего один сэмпл, и прошло максимальное время ожидания - этот сэмпл попадает в батч один и обрабатывается один.\n",
    "\n",
    "Для хранения данных в очереди можно использовать queue.Queue или любой другой удобный способ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe94205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rami_hw",
   "language": "python",
   "name": "rami_hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
