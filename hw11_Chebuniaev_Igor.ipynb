{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6831fe55",
   "metadata": {},
   "source": [
    "# KV Cache - 10 баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29b7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045cc82",
   "metadata": {},
   "source": [
    "Представим, что у нас есть очень простая мини-LLM:\n",
    "1. Она эмбеддит токены\n",
    "2. Считает аттеншн (обычный, не multihead) токенов друг с другом с causal mask (не смотрит в будущее!)\n",
    "3. После этого выходы attention подаются в линейный слой для получения распределеняи по словарю\n",
    "\n",
    "На примере такой модели давайте попробуем имплементировать KV-Cache.\n",
    "\n",
    "Ниже за вас написан метод forward - этот метод это обычный forward нейросети, который считает attention всех токенов со всеми токенами.\n",
    "\n",
    "Вам же нужно имплементировать метод forward_kv_cache, который принимает:\n",
    "* x - тензор размерности \\[batch, seq_len = 1\\]\n",
    "* prev_output - выход модели с предыдущего шага типа Output\n",
    "\n",
    "Метод forward_kv_cache должен выполнять следующие действия:\n",
    "1. Эмбеддинг токена x\n",
    "2. Проекция x в QKV\n",
    "3. Расширение k_cache и v_cache из prev_ouptut k/v проекциями x\n",
    "4. Подсчет аттеншена между q_x и k_cache и v_cache\n",
    "5. Конкатенация аттеншена в prev_output.attention_weights\n",
    "6. Возврат logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce42ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before generation tensor([[1, 3, 0]])\n",
      "After generation no cache tensor([[1, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]])\n",
      "After generation kv cache tensor([[1, 3, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Output:\n",
    "    logits: torch.Tensor = None\n",
    "    k_cache: torch.Tensor = None\n",
    "    v_cache: torch.Tensor = None\n",
    "    attn_weights: torch.Tensor = None\n",
    "    \n",
    "\n",
    "\n",
    "class SimpleAttentionLLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.lin = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        q = self.W_Q(x)\n",
    "        k = self.W_K(x)\n",
    "        v = self.W_V(x)\n",
    "        \n",
    "        attn_scores = torch.matmul(q, k.permute(0, 2, 1))\n",
    "        mask = torch.tril(torch.ones_like(attn_scores))\n",
    "        attn_scores = attn_scores.masked_fill(~mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=2)\n",
    "        weights_V = torch.matmul(attn_weights, v)\n",
    "        logits = self.lin(weights_V)\n",
    "        return Output(\n",
    "            logits=logits,\n",
    "            k_cache=k,\n",
    "            v_cache=v,\n",
    "            attn_weights=attn_weights # batch, seq_len, seq_len\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward_kv_cache(self, x, prev_output):\n",
    "        # эмбеддинг токена x\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # 1. Проецируем x в q, k, v\n",
    "        q = self.W_Q(x)\n",
    "        k = self.W_K(x)\n",
    "        v = self.W_V(x)\n",
    "        \n",
    "        # print(f\"{q.shape=}\")\n",
    "        # print(f\"{k.shape=}\")\n",
    "\n",
    "        # берем старый кэш\n",
    "        k_cache = prev_output.k_cache\n",
    "        v_cache = prev_output.v_cache\n",
    "\n",
    "        # расширяем его состояниями k, v последнего токена\n",
    "        # с помощью torch.cat\n",
    "        k_cache_new = torch.cat(tensors=(k_cache, k), dim=1)\n",
    "        v_cache_new = torch.cat(tensors=(v_cache, v), dim=1)\n",
    "        \n",
    "        # считаем attention_score, то есть матричное умножение между q и k_cache_new\n",
    "        \n",
    "        attn_scores = q @ k_cache_new.permute(0, 2, 1)\n",
    "    \n",
    "        \n",
    "        # считаем softmax\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        # домножаем softmax на V\n",
    "        \n",
    "        # print(f\"{attn_scores.shape=}\")\n",
    "        # print(f\"{v_cache_new.shape=}\")\n",
    "        weights_V = attn_weights @ v_cache_new\n",
    "        logits = self.lin(weights_V)\n",
    "        \n",
    "        batch = x.size(0)\n",
    "        seq_len = k_cache.size(1)\n",
    "        \n",
    "        attn_weights_old = prev_output.attn_weights\n",
    "        zeros_right = torch.zeros(batch, seq_len, 1)\n",
    "        # добавляем нули в аттеншене так, чтобы у старых токенов был нулевой аттеншн на новый токен\n",
    "        attn_weights_all = torch.cat((attn_weights_old, zeros_right), dim=2)\n",
    "        # Добавляем аттеншн текущего нового токена по старым\n",
    "        attn_weights_all = torch.cat((attn_weights_all, attn_weights), dim=1)\n",
    "        \n",
    "        return Output(\n",
    "            logits=logits,\n",
    "            k_cache=k_cache_new,\n",
    "            v_cache=v_cache_new,\n",
    "            attn_weights=attn_weights_all\n",
    "        )\n",
    "        \n",
    "torch.manual_seed(1)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "d_model = 128\n",
    "vocab_size = 7\n",
    "\n",
    "layer = SimpleAttentionLLM(d_model, vocab_size)\n",
    "for param in layer.parameters():\n",
    "    nn.init.normal_(param)\n",
    "\n",
    "x = torch.randint(0, 7, (batch_size, seq_len))\n",
    "x_copy = x.clone()\n",
    "\n",
    "print(\"Before generation\", x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        outputs = layer(x)\n",
    "        logits = outputs.logits\n",
    "        # берем последний токен, т.к. по нему предсказываем!\n",
    "        next_token = logits[:, -1].argmax(dim=1, keepdim=True)\n",
    "        x = torch.cat((x, next_token), dim=1) # добавляем новый токен по размерности seq_len\n",
    "\n",
    "no_cache_output = outputs\n",
    "print(\"After generation no cache\", x)\n",
    "\n",
    "\n",
    "x = x_copy.clone()\n",
    "final_tokens = x.clone()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # prefill\n",
    "    outputs = layer(x)\n",
    "    logits = outputs.logits\n",
    "    # берем последний токен, т.к. по нему предсказываем!\n",
    "    next_token = logits[:, -1].argmax(dim=1, keepdim=True)\n",
    "    # обратите внимание, что раньше было 10 шагов!\n",
    "    final_tokens = torch.cat((final_tokens, next_token), dim=1)\n",
    "    for i in range(9):\n",
    "        outputs = layer.forward_kv_cache(next_token, outputs)\n",
    "        next_token = outputs.logits[:, -1].argmax(dim=1, keepdim=True)\n",
    "        final_tokens = torch.cat((final_tokens, next_token), dim=1)\n",
    "    \n",
    "print(\"After generation kv cache\", final_tokens)\n",
    "\n",
    "cache_outputs = outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62b506",
   "metadata": {},
   "source": [
    "Выведем матрицы attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "305f232d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attention kv cache')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(no_cache_output.attn_weights[0])\n",
    "axes[0].set_title(\"Attention no cache\")\n",
    "axes[1].imshow(cache_outputs.attn_weights[0])\n",
    "axes[1].set_title(\"Attention kv cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "326b101e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Downloading vllm-0.9.0.tar.gz (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[239 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting cmake>=3.26\n",
      "  \u001b[31m   \u001b[0m   Downloading cmake-4.0.2-py3-none-macosx_10_10_universal2.whl (48.7 MB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l                                              0.0/48.7 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.0/48.7 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.0/48.7 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.0/48.7 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.0/48.7 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.0/48.7 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.1/48.7 MB 221.3 kB/s eta 0:03:40\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.1/48.7 MB 221.3 kB/s eta 0:03:40\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.1/48.7 MB 207.0 kB/s eta 0:03:56\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.1/48.7 MB 308.3 kB/s eta 0:02:38\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.1/48.7 MB 347.3 kB/s eta 0:02:20\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.2/48.7 MB 422.5 kB/s eta 0:01:55\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.2/48.7 MB 478.3 kB/s eta 0:01:42\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.3/48.7 MB 645.4 kB/s eta 0:01:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K                                              0.5/48.7 MB 827.5 kB/s eta 0:00:59\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ╸                                        0.6/48.7 MB 1.0 MB/s eta 0:00:47\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ╸                                        0.9/48.7 MB 1.3 MB/s eta 0:00:37\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ╸                                        1.2/48.7 MB 1.7 MB/s eta 0:00:28\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━                                        1.5/48.7 MB 2.0 MB/s eta 0:00:24\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━╸                                       1.9/48.7 MB 2.5 MB/s eta 0:00:20\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━╸                                       2.3/48.7 MB 2.8 MB/s eta 0:00:17\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━╸                                       2.3/48.7 MB 2.8 MB/s eta 0:00:17\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━╸                                       2.3/48.7 MB 2.8 MB/s eta 0:00:17\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━╸                                       2.3/48.7 MB 2.8 MB/s eta 0:00:17\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━                                       2.5/48.7 MB 2.6 MB/s eta 0:00:18\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━                                       2.8/48.7 MB 2.7 MB/s eta 0:00:18\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━╸                                      3.2/48.7 MB 3.0 MB/s eta 0:00:15\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━                                     5.3/48.7 MB 3.7 MB/s eta 0:00:12\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━╸                                    5.6/48.7 MB 3.8 MB/s eta 0:00:12\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━╸                                    5.6/48.7 MB 3.8 MB/s eta 0:00:12\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━╸                                    5.6/48.7 MB 3.8 MB/s eta 0:00:12\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━╸                                    5.7/48.7 MB 3.6 MB/s eta 0:00:13\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━                                   7.7/48.7 MB 4.7 MB/s eta 0:00:09\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━╸                                  8.1/48.7 MB 4.8 MB/s eta 0:00:09\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━                                  8.5/48.7 MB 4.9 MB/s eta 0:00:09\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━                                  8.9/48.7 MB 5.0 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━╸                                 9.3/48.7 MB 5.1 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━╸                                 9.6/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 9.8/48.7 MB 5.2 MB/s eta 0:00:08\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.2/48.7 MB 7.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.2/48.7 MB 7.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.2/48.7 MB 7.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.2/48.7 MB 7.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.2/48.7 MB 7.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.3/48.7 MB 6.4 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.3/48.7 MB 6.4 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.3/48.7 MB 6.4 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               12.3/48.7 MB 5.9 MB/s eta 0:00:07\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━                              13.5/48.7 MB 6.8 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━                              13.5/48.7 MB 6.8 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━                              13.5/48.7 MB 6.8 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━                              13.5/48.7 MB 6.8 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━╸                             14.3/48.7 MB 7.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━╸                             14.3/48.7 MB 7.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━╸                             14.3/48.7 MB 7.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━╸                             14.3/48.7 MB 7.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━╸                             14.6/48.7 MB 7.0 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━                            16.0/48.7 MB 7.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━                            16.2/48.7 MB 7.6 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━                            16.2/48.7 MB 7.6 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━                            16.2/48.7 MB 7.6 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━                            16.2/48.7 MB 7.6 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━                           17.2/48.7 MB 6.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━                           17.2/48.7 MB 6.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━                           17.2/48.7 MB 6.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━                           17.2/48.7 MB 6.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━                           17.2/48.7 MB 6.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━                           17.3/48.7 MB 6.1 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━╸                         19.1/48.7 MB 6.4 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━╸                         19.1/48.7 MB 6.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━╸                         19.1/48.7 MB 6.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━╸                         19.1/48.7 MB 6.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━╸                         19.4/48.7 MB 5.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━                         19.6/48.7 MB 5.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━                         19.8/48.7 MB 5.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━╸                        20.2/48.7 MB 6.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━                        20.8/48.7 MB 6.7 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━                        21.1/48.7 MB 6.6 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━╸                       21.4/48.7 MB 6.4 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━╸                       21.6/48.7 MB 6.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━╸                       21.7/48.7 MB 6.1 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━╸                       21.7/48.7 MB 6.1 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━╸                       21.9/48.7 MB 5.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.1/48.7 MB 5.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.1/48.7 MB 5.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.1/48.7 MB 5.8 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.1/48.7 MB 5.3 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.2/48.7 MB 5.3 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.4/48.7 MB 5.2 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━                       22.4/48.7 MB 5.2 MB/s eta 0:00:06\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━                      23.5/48.7 MB 5.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━                      23.6/48.7 MB 5.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━                      23.6/48.7 MB 5.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━╸                     23.8/48.7 MB 6.1 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━                     24.4/48.7 MB 6.1 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━                     24.4/48.7 MB 6.1 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━                     24.4/48.7 MB 5.9 MB/s eta 0:00:05\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━                     24.8/48.7 MB 6.2 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━                    25.9/48.7 MB 6.1 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━                    26.1/48.7 MB 5.9 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━╸                   26.4/48.7 MB 6.0 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━╸                   26.8/48.7 MB 6.4 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━                   27.2/48.7 MB 6.3 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━╸                  27.5/48.7 MB 6.9 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━╸                  27.8/48.7 MB 6.9 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━╸                  28.0/48.7 MB 6.7 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━╸                  28.0/48.7 MB 6.7 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━                  28.2/48.7 MB 6.4 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━                  28.2/48.7 MB 6.4 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━                  28.2/48.7 MB 6.4 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━                  28.2/48.7 MB 5.9 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━╸                 29.1/48.7 MB 5.9 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━                 29.4/48.7 MB 6.2 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━╸                30.2/48.7 MB 6.5 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━╸                30.5/48.7 MB 6.4 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                30.7/48.7 MB 6.3 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                30.8/48.7 MB 6.2 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                30.8/48.7 MB 6.2 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                30.9/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                31.0/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                31.0/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                31.0/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                31.0/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                31.0/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━                31.0/48.7 MB 5.8 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━╸               31.2/48.7 MB 5.1 MB/s eta 0:00:04\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━╸              32.6/48.7 MB 6.5 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━╸              32.9/48.7 MB 6.8 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━              33.1/48.7 MB 6.6 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━              33.4/48.7 MB 6.4 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸             33.7/48.7 MB 6.3 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸             33.8/48.7 MB 6.6 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸             34.0/48.7 MB 6.4 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━             34.4/48.7 MB 6.3 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━             34.6/48.7 MB 6.2 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸            35.0/48.7 MB 6.6 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸            35.2/48.7 MB 6.5 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━            35.5/48.7 MB 6.3 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━            35.5/48.7 MB 6.3 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━            35.5/48.7 MB 6.3 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸           36.5/48.7 MB 6.2 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━           36.8/48.7 MB 6.2 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━           37.0/48.7 MB 6.2 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━           37.0/48.7 MB 6.2 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━           37.1/48.7 MB 5.9 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━           37.1/48.7 MB 5.9 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━           37.1/48.7 MB 5.9 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸          37.2/48.7 MB 5.5 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸          37.4/48.7 MB 5.4 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸          37.4/48.7 MB 5.4 MB/s eta 0:00:03\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸         38.5/48.7 MB 6.3 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸         38.6/48.7 MB 6.3 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸         38.6/48.7 MB 6.3 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸         38.6/48.7 MB 6.3 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸         39.0/48.7 MB 5.9 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━         39.3/48.7 MB 5.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸        40.1/48.7 MB 5.8 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━        40.2/48.7 MB 5.7 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━        40.6/48.7 MB 5.7 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━        40.6/48.7 MB 5.7 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸       41.0/48.7 MB 5.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸       41.0/48.7 MB 5.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸       41.0/48.7 MB 5.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸       41.0/48.7 MB 5.3 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━       41.6/48.7 MB 6.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━       41.6/48.7 MB 6.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━       41.6/48.7 MB 6.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━       41.6/48.7 MB 6.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━       41.6/48.7 MB 6.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━       41.6/48.7 MB 6.6 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━      42.9/48.7 MB 5.8 MB/s eta 0:00:02\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸     43.3/48.7 MB 5.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸     43.3/48.7 MB 5.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸     43.4/48.7 MB 5.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸     43.7/48.7 MB 5.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸    44.9/48.7 MB 6.1 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸    45.0/48.7 MB 6.1 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸    45.0/48.7 MB 5.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━    45.3/48.7 MB 5.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━    45.3/48.7 MB 5.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━    45.4/48.7 MB 5.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   46.5/48.7 MB 6.0 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   46.7/48.7 MB 6.0 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸  47.0/48.7 MB 5.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸  47.3/48.7 MB 6.3 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  47.6/48.7 MB 6.8 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  47.8/48.7 MB 6.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  47.8/48.7 MB 6.9 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  48.1/48.7 MB 6.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  48.1/48.7 MB 6.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  48.1/48.7 MB 6.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 48.7/48.7 MB 6.2 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 48.7/48.7 MB 6.2 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.7/48.7 MB 5.8 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m \u001b[?25hCollecting ninja\n",
      "  \u001b[31m   \u001b[0m   Downloading ninja-1.11.1.4-py3-none-macosx_10_9_universal2.whl (279 kB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l                                              0.0/279.1 kB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 276.5/279.1 kB 7.1 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 279.1/279.1 kB 5.5 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m \u001b[?25hCollecting packaging>=24.2\n",
      "  \u001b[31m   \u001b[0m   Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l                                              0.0/66.5 kB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 4.0 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m \u001b[?25hCollecting setuptools<80.0.0,>=77.0.3\n",
      "  \u001b[31m   \u001b[0m   Downloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l                                              0.0/1.3 MB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━                                 0.3/1.3 MB 7.8 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━                               0.3/1.3 MB 7.8 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━                             0.4/1.3 MB 3.2 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━                    0.7/1.3 MB 4.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━                    0.7/1.3 MB 4.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━                    0.7/1.3 MB 4.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━                   0.7/1.3 MB 2.6 MB/s eta 0:00:01\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 4.1 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m \u001b[?25hCollecting setuptools-scm>=8.0\n",
      "  \u001b[31m   \u001b[0m   Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l                                              0.0/43.9 kB ? eta -:--:--\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.9/43.9 kB 3.8 MB/s eta 0:00:00\n",
      "  \u001b[31m   \u001b[0m \u001b[?25hERROR: Could not find a version that satisfies the requirement torch==2.7.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)\n",
      "  \u001b[31m   \u001b[0m ERROR: No matching distribution found for torch==2.7.0\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m [notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "  \u001b[31m   \u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45fc6",
   "metadata": {},
   "source": [
    "По возможности данное задание необходимо выполнять на сервере, чтобы было удобно запускать параллельно фреймворк и нагрузочное тестирование. Для удобства работы в jupyter сделан следующий трюк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "\n",
    "def start_vllm_server():\n",
    "    # This function will run the `vllm` server command\n",
    "    cmd = [\"vllm\", \"serve\", \"unsloth/gemma-2b-it\", \"--dtype\", \"half\"]\n",
    "    subprocess.run(cmd)\n",
    "\n",
    "\n",
    "server_process = multiprocessing.Process(target=start_vllm_server)\n",
    "server_process.start()\n",
    "time.sleep(60)\n",
    "print(\"we are probably ready\")\n",
    "\n",
    "import requests\n",
    "\n",
    "r = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
    "                  \"model\": \"unsloth/gemma-2b-it\",\n",
    "                  \"prompt\": \"Hello there\",\n",
    "                  \"max_tokens\": 20,\n",
    "                  \"temperature\": 0.8\n",
    "              })\n",
    "print(r.json())\n",
    "print(r.json()[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31f5df",
   "metadata": {},
   "source": [
    "# Нагрузочное тестирование - 25 баллов\n",
    "\n",
    "20 баллов - узнать, сколько запросов в секунду выдержит VLLM сервинг модели при ограничении latency в 5 секунд.\n",
    "Можно использовать самописную функцию через multiprocessing/threading, можно использовать любой готовый инструмент.\n",
    "\n",
    "В качестве пейлоадов предлагается брать тексты длины 100-128 и генерировать к ним не более 10 токенов. Сами пейлоады можно взять из любого датасета, например https://huggingface.co/datasets/Intel/orca_dpo_pairs.\n",
    "\n",
    "Об аргументах vllm serve можно почитать в документации https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
    "\n",
    "\n",
    "5 баллов - если сможете посчитать на этом пейлоаде ttft - time to first token, то есть сколько занимает prefill стадия генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def generate_responses_batch(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        messages_list,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.8,\n",
    "        device='cuda'\n",
    "    ) -> list[str]:\n",
    "\n",
    "    # Составляем промпт в LLM, используя apply_chat_template, чтобы проставились все нужные спец тоцены\n",
    "    input_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "    ### ваш код здесь\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_new_tokens\n",
    "      )\n",
    "    llm = LLM(model=model_name,\n",
    "              dtype=torch.half,\n",
    "              enforce_eager=True,\n",
    "              gpu_memory_utilization=.8,\n",
    "              # max_model_len = 4096*4,\n",
    "              # max_num_seqs=4,\n",
    "              # kv_cache_dtype=\"fp8\"\n",
    "              )\n",
    "    outputs = llm.generate(input_texts, sampling_params)\n",
    "\n",
    "\n",
    "    out_texts = []\n",
    "    for output in outputs:\n",
    "      prompt = output.prompt\n",
    "      out_texts.append(output.outputs[0].text)\n",
    "\n",
    "    return out_texts\n",
    "\n",
    "assert all(\n",
    "    \"London\" in candidate\n",
    "    for candidate in generate_responses_batch(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        messages_list=[[{\"role\": \"user\", \"content\": \"What is the capital of Great Britain?\"}]] * 4,\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886b031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c334b3",
   "metadata": {},
   "source": [
    "# Часть 2\n",
    "Далее предоставлено 2 варианта выполнения задания. Баллы будут начисляться только на один из них\n",
    "## Вариант 1. Квантизация - 15 баллов\n",
    "Квантизируйте модель в VLLM любым доступным способом, напишите, как сократились затраты памяти и как изменилась скорость инференса. Обязательно укажите, на какой видеокарте проводились замеры!\n",
    "Не забудьте про то, что квантизировать можно и kv cache.\n",
    "\n",
    "Внимательно проверьте и убедитесь, что ваш ускоритель поддержан в https://docs.vllm.ai/en/latest/quantization/supported_hardware.html\n",
    "\n",
    "## Вариант 2. Батчевалка - 15 баллов\n",
    "Предлагается написать сервер на питоне, который поддерживал бы батчевание запросов.\n",
    "\n",
    "Сервер принимает POST запрос с телом вида\n",
    "\n",
    "```json\n",
    "{\"text\": \"Hello there\"}\n",
    "```\n",
    "\n",
    "\n",
    "Необходимо написать сервер, который:\n",
    "1. Имел бы возможность работать с несколькими клиентами за раз (не блокировался бы на обработку одного запроса). Для этого можно использовать async, gevent, треды и т.д.\n",
    "2. Использовал бы батчовую обработку следующим образом:\n",
    "если пришло несколько запросов (для примера 2)\n",
    "\n",
    "```json\n",
    "запрос 1\n",
    "{\"text\": \"Hello there \"}\n",
    "запрос 2\n",
    "{\"text\": \"handsome\"}\n",
    "```\n",
    "\n",
    "то каждый клиент получал бы в ответ конкатенацию этих запросов (в произвольном порядке), т.е. оба клиента получили бы ответ\n",
    "```json\n",
    "{\"text\": \"Hello there handsome\"}\n",
    "или\n",
    "{\"text\": \"handsomeHello there \"}\n",
    "```\n",
    "\n",
    "Сервер должен иметь 2 конфигурируемых параметра:\n",
    "1. Максимальный размер батча, который он может обработать\n",
    "2. Максимальное время ожидания, которое ждет сэмпл перед обработкой. Т.е. если у нас батч размера 5, а у нас всего один сэмпл, и прошло максимальное время ожидания - этот сэмпл попадает в батч один и обрабатывается один.\n",
    "\n",
    "Для хранения данных в очереди можно использовать queue.Queue или любой другой удобный способ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe94205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_train",
   "language": "python",
   "name": "multi_train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
