{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Домашнее задание (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В этом домашнем задании вы познакомитесь с основами NLP, научитесь обрабатывать тексты.\n",
    "\n",
    "В местах, где используется `...` (elipsis), требуется заменить его на код.\n",
    "\n",
    "Установим необходимые зависимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T19:21:30.570036Z",
     "start_time": "2024-11-27T19:21:27.452197Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (25.0.1)\n",
      "Requirement already satisfied: nltk in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: seqeval in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: datasets in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: click in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install nltk tqdm seqeval scikit-learn datasets numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T19:21:40.162946Z",
     "start_time": "2024-11-27T19:21:40.157819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/igorchebuniaev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/igorchebuniaev/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/igorchebuniaev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Callable\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Токенизация (15 баллов)\n",
    "\n",
    "Токенизация - это процесс преобразования текста в набор токенов.\n",
    "Наивная реализация разбивает текст по пробелам. Более умные реализации учитывают пунктуацию.\n",
    "\n",
    "### Библиотека NLTK (2 балла)\n",
    "\n",
    "Научимся работать с токенизацией NLTK, где уже реализована работа с пунктуацией.\n",
    "\n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tokenize OK\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text: str, language: str = \"english\") -> List[str]:\n",
    "    # используйте функцию nltk.word_tokenize для разбиения текста на токены\n",
    "    # https://www.nltk.org/api/nltk.tokenize.word_tokenize.html\n",
    "    # raise NotImplementedError()\n",
    "    return nltk.word_tokenize(text, language)\n",
    "\n",
    "\n",
    "assert tokenize(\"\") == []\n",
    "assert tokenize(\"Hello, world!\") == [\"Hello\", \",\", \"world\", \"!\"]\n",
    "assert tokenize(\"EU rejects German call to boycott British lamb.\") == [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]\n",
    "print(\"test_tokenize OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация (3 балла)\n",
    "\n",
    "Добавим нормализацию после токенизации. Пробуем [лемматизацию](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.wordnet.WordNetLemmatizer) , [стемминг](https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.EnglishStemmer) и [юникод](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize) нормализацию. Напишем функцию, которая будет принимать на вход токен после токенизации, нормализовать в NFC юникод форму, переводит в нижний регистр, лемматизирует слово и, если слово не изменилось после лемматизации, применяет стемминг.\n",
    "\n",
    "\n",
    "Создайте функцию `normalize`:\n",
    "   - Функция `normalize` должна принимать строку `token` и возвращать нормализованный токен.\n",
    "   - Примените к токену Unicode нормализацию с помощью `unicode_nfc_normalizer`.\n",
    "   - Преобразуйте токен в нижний регистр.\n",
    "   - Примените лемматизацию с помощью `lemmatizer`.\n",
    "   - Если лемматизированный токен отличается от исходного, верните его. В противном случае, примените стемминг с помощью `stemmer` и верните результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normalize OK\n"
     ]
    }
   ],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "unicode_nfc_normalizer = lambda token: unicodedata.normalize(\"NFC\", token)\n",
    "\n",
    "\n",
    "def normalize(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Нормализует токен, применяя Unicode нормализацию, преобразование в нижний регистр,\n",
    "    лемматизацию и стемминг при необходимости.\n",
    "\n",
    "    :param token: Токен для нормализации\n",
    "    :return: Нормализованный токен\n",
    "    \"\"\"\n",
    "    input_token = token\n",
    "    for func in [unicode_nfc_normalizer, str.lower, lemmatizer.lemmatize]:\n",
    "        token = func(token)\n",
    "    if token != input_token.lower():\n",
    "        return token\n",
    "    return stemmer.stem(token)\n",
    "\n",
    "test_tokens = [\"Worlds\", \"churches\", \"Helping\"]\n",
    "assert [normalize(token) for token in test_tokens] == [\"world\", \"church\", \"help\"]\n",
    "print(\"test_normalize OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем Словарь (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Современные токенайзеры не только разбивают строки на токены, но и преобразуют последовательность токенов в последовательность числел. Объединим функцию токенизации, нормализации и отображения из токенов в индексы в один объект токенайзера.\n",
    "\n",
    "Напишите класс `Tokenizer` для токенизации и нормализации текста.\n",
    "\n",
    "Построение словаря:\n",
    "   - Создайте метод `_build_vocabulary`, который принимает список текстов `texts` и обновляет словарь токенов.\n",
    "   - Для каждого текста:\n",
    "     - Токенизируйте и нормализуйте текст.\n",
    "     - Обновите счетчик вхождений слов.\n",
    "   - После обработки всех текстов для каждого слова, которое встречается не менее `min_count` раз, добавьте слово в словарь `word2idx` и список `idx2word`.\n",
    "\n",
    "Кодирование и декодирование:\n",
    "   - Создайте метод `encode_word`, который принимает слово `word` и возвращает его индекс с применением нормализации.\n",
    "   - Создайте метод `encode`, который принимает текст `text` и возвращает список индексов токенов.\n",
    "   - Создайте метод `decode`, который принимает список индексов `input_ids` и возвращает текст, вставляя пробелы между токенами.\n",
    "\n",
    "> Note: для функций, которые могут долго исполнятся (`_build_vocab`), рекомендуется использовать библиотеку tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0114e4f72ee749e5a8f8bdbd4f46931a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11821a128e994fd3b8a6dd11fe1467bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tokenizer OK\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            texts: List[str], \n",
    "            min_count: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация токенизатора.\n",
    "\n",
    "        :param texts: список текстов для построения словаря\n",
    "        :param tokenize_fn: функция для токенизации текста\n",
    "        :param normalize_fn: функция для нормализации токенов\n",
    "        :param min_count: минимальное количество вхождений слова для включения в словарь\n",
    "        \"\"\"\n",
    "        self.min_count = min_count\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.unk_token_id = 3\n",
    "        self.idx2word = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "        self.word2count = Counter()\n",
    "        self._build_vocabulary(texts)\n",
    "\n",
    "    def _build_vocabulary(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Построение словаря на основе списка текстов.\n",
    "\n",
    "        :param texts: список текстов\n",
    "        \"\"\"\n",
    "        # Проходимся по всем текстам, токенизируем текст,\n",
    "        # нормализуем токены и обновляем self.word2count\n",
    "        for text in tqdm(texts, desc=\"Считаем слова\"):\n",
    "            tokens = tokenize(text)\n",
    "            tokens = [normalize(token) for token in tokens]\n",
    "            self.word2count.update(tokens)\n",
    "            \n",
    "        # Теперь у нас есть заполненный self.word2count\n",
    "        # ключи - нормализованные токены, значения - их встерчаемость в тексте\n",
    "        # нужно добавить в словарь новый токен (обновить self.word2idx и self.idx2word), \n",
    "        # если его встречаемость не меньше self.min_count\n",
    "        for key in tqdm(self.word2count, desc=\"Строим словарь\"):\n",
    "            if self.word2count[key] >= self.min_count:\n",
    "                self.word2idx[key] = len(self.word2idx)\n",
    "                self.idx2word.append(key)\n",
    "\n",
    "\n",
    "    def encode_word(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Кодирование слова в индекс с применением нормализации.\n",
    "\n",
    "        :param text: слово\n",
    "        :return: индекс слова\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError()\n",
    "        return self.word2idx.get(normalize(text), self.unk_token_id)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирование текста в набор индексов.\n",
    "\n",
    "        :param text: текст\n",
    "        :return: набор индексов токенов\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError()\n",
    "        tokens = tokenize(text)\n",
    "        tokens = [normalize(token) for token in tokens]\n",
    "        return [self.encode_word(token) for token in tokens]\n",
    "\n",
    "    def decode(self, input_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
    "\n",
    "        :param input_ids: набор индексов токенов\n",
    "        :return: текст\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError()\n",
    "        return \" \".join([self.idx2word[idx] for idx in input_ids])\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает количество уникальных токенов в словаре.\n",
    "\n",
    "        :return: количество уникальных токенов\n",
    "        \"\"\"\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def __contains__(self, item: str) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, содержится ли слово в словаре.\n",
    "\n",
    "        :param item: слово\n",
    "        :return: True, если слово содержится в словаре, иначе False\n",
    "        \"\"\"\n",
    "        return item in self.word2idx\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Возвращает строковое представление словаря.\n",
    "\n",
    "        :return: строковое представление словаря\n",
    "        \"\"\"\n",
    "        return str(self.word2idx)\n",
    "\n",
    "\n",
    "corpus = [\"Hello, world!\", \"I love Python!\"]\n",
    "tokenizer = Tokenizer(corpus, min_count=1)\n",
    "encoded = tokenizer.encode(\"Hello, Python! I love you\")\n",
    "assert tokenizer.decode(encoded) == \"hello , python ! i love <UNK>\"\n",
    "print(\"test_tokenizer OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (20 баллов)\n",
    "\n",
    "\n",
    "### Класс TFIDF (10 баллов)\n",
    "\n",
    "Создайте класс `TFIDF` для вычисления TF-IDF значений.\n",
    "\n",
    "Вам нужно реализовать следующие функции:\n",
    "### add_doc\n",
    "0. Увеличиваем счетчик num_docs - число документов в обучающей выборке\n",
    "1. Токенизируем текст\n",
    "2. Берем уникальные токены\n",
    "3. Обновляем self.term2num_docs - массив, в котором для каждого токена хранится число того, в скольких уникальных документах этот токен встречается. Токен `<UNK>` игнорируем.\n",
    "\n",
    "### idf\n",
    "Считаем логарифмированный inverse document frequency для документа\n",
    "\n",
    "$$\n",
    "idf = -log \\frac {n_t + 1} {N}\n",
    "$$\n",
    "где $n_t$ - в скольких документах встречается токен, $N$ - число различных документов. За эти параметры у нас отвечают параметры `self.term2num_docs` и `self.num_docs`. (единицы мы добавляем, чтобы избежа\n",
    "\n",
    "### predict\n",
    "1. Получаем набор документов, на выходе генерируем numpy матрицу tf-idf размера len(docs) x len(vocabulary)\n",
    "2. Токенизируем каждый текст\n",
    "3. Для каждого уникального токена в тексте (кроме `<UNK>`) считаем tf - как часто данный токен встречается во всем тексте (с учетом дублей и `<UNK>` токена!)\n",
    "4. Для каждого токена считаем idf из одноименной функции\n",
    "5. Заполняем соответствующие элементы массива\n",
    "6. Нормализуем вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация TFIDF.\n",
    "\n",
    "        :param tokenizer: токенизатор для преобразования текста в токены\n",
    "        :param default_idf: значение IDF для неизвестных токенов\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_docs = 0\n",
    "        self.term2num_docs = [0 for _ in tokenizer.word2idx]  # для подсчёта IDF\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает размер словаря.\n",
    "\n",
    "        :return: размер словаря\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def add_doc(self, doc: str) -> None:\n",
    "        \"\"\"\n",
    "        Добавляет документ в модель TFIDF.\n",
    "        1. Увеличиваем счетчик числа документов\n",
    "        2. Токенизируем текст\n",
    "        3. Для всех уникальных токенов обновляем self.term2numdocs для подсчета IDF\n",
    "\n",
    "        :param doc: документ для добавления\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError()\n",
    "        self.num_docs += 1\n",
    "        tokens = set(tokenize(doc))\n",
    "        ixs = self.tokenizer.encode(' '.join(tokens))\n",
    "        for ix in ixs:\n",
    "            # пропускаем <UNK>\n",
    "            if ix == self.tokenizer.unk_token_id:\n",
    "                continue    \n",
    "            self.term2num_docs[ix] += 1\n",
    "        \n",
    "\n",
    "    def fit(self, docs: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Обучает модель TFIDF на корпусе docs.\n",
    "\n",
    "        :param docs: корпус для обучения\n",
    "        \"\"\"\n",
    "        for doc in docs:\n",
    "            self.add_doc(doc)\n",
    "\n",
    "    def predict(self, docs: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает TFIDF значения для списка документов.\n",
    "        1. Создаем np.array размерности (len(docs), vocab_size)\n",
    "        2. Для каждого документа\n",
    "            а. Токенизируем его\n",
    "            б. Считаем tf для каждого токена кроме unk\n",
    "            в. Считаем idf для каждого токена кроме unk\n",
    "            г. Заполняем соответствующее значение в матрице\n",
    "        3. Нормализуем матрицу по размерности словаря (по строкам). При нормализации, \n",
    "        чтобы избежать деления на 0 делите не на норму ветктора, а на норму вектора + 1e-5\n",
    "\n",
    "        :param docs: список документов\n",
    "        :return: матрица TFIDF значений\n",
    "        \"\"\"\n",
    "        # создаем numpy массив нулей размера len(docs) на размерность словаря\n",
    "        result = np.zeros(shape = (len(docs), self.vocab_size))\n",
    "        # для каждого документа будем считать его вектор tf-idf\n",
    "        for doc_idx, document_text in enumerate(docs):\n",
    "            # токенизируем документ\n",
    "            tokens = tokenize(document_text)\n",
    "            # считаем tf для каждого токена\n",
    "            tf = Counter(tokens)\n",
    "            # для каждого токена считаем его tf-idf\n",
    "            for token in tokens:\n",
    "                token_id = self.tokenizer.encode_word(token)\n",
    "                if token_id == self.tokenizer.unk_token_id:\n",
    "                    continue\n",
    "                tf_idf = tf[token] * self.idf(token_id)\n",
    "                result[doc_idx, token_id] = tf_idf\n",
    "        norm = np.linalg.norm(result, axis=1)[:, None]\n",
    "        # нормализуем документ\n",
    "        result = result / (norm + 1e-5)\n",
    "        return result\n",
    "\n",
    "    def idf(self, token_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Вычисляет IDF (обратную частоту документа) для термина.\n",
    "\n",
    "        :param term: термин\n",
    "        :return: IDF значение\n",
    "        \"\"\"\n",
    "        # raise NotImplementedError()\n",
    "        return np.log(self.num_docs / (self.term2num_docs[token_id] + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695f79e95bcc4854af19500f9def87c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb659dd95634c29ac0862906a545029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tfidf OK\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"hello there\", \"my favourite frut is\", \"i love bananas\", \"hello mama\", \"I need to eat\",\n",
    "    \"how can I get to the studio\", \"bottom gear\"\n",
    "]\n",
    "tokenizer = Tokenizer(corpus, min_count=1)\n",
    "tfidf = TFIDF(tokenizer)\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "assert not np.any(tfidf.predict([\"all tokens abscent from vocab should be zeros vector\"]))\n",
    "reference = np.zeros((1, len(tfidf.tokenizer)))\n",
    "reference[0, tfidf.tokenizer.encode_word(\"hello\")] = 2 / 3 * -np.log(3/7)\n",
    "reference[0, tfidf.tokenizer.encode_word(\"mama\")] = 1 / 3 * -np.log(2/7)\n",
    "reference /= 0.7024715222440031\n",
    "assert np.allclose(tfidf.predict([\"hello hello mama\"]), reference)\n",
    "print(\"test_tfidf OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с помощью TF-IDF - 10 баллов\n",
    "В этом задании предлагается обучить с помощью полученного векторизатора TF-IDF логистическую регресиию.\n",
    "Для этого мы возьмем корпус IMDB - отзывы фильмов. Это задача бинарной классификации, в которой нужно определить - положительный отзыв или отрицательный.\n",
    "\n",
    "Задача будет решаться следующими этапами:\n",
    "1. Загружаем текстовый корпус, обучаем словарь и TFIDF\n",
    "2. Векторизуем корпус текстов\n",
    "3. По векторизованному корпусу и меткам текстов обучаем логистическую регрессию, смотрим на качество на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0\n",
      "Text\n",
      "Someone actually gave this movie 2 stars. There's a very high chance they need immediate professional help as anyone who doesn't spend 30 seconds to see if you can award no stars is quite literally scary.<br /><br />This film is ... well ... I guess it's pretty much some kind of attempt at a horrible porn / snuff movie with no porn or no real horrible bits (apart from the acting, plot, story, sets, dialogue and sound). I wrongly assumed it was about zombies. <br /><br />Watching it is actually quite scary in fairness; you're terrified someone will come over and you'll never be able to describe what it is and they'll go away thinking you're a freak that watches home-made amateur torture videos or something along those lines. <br /><br />I'm so taken aback I'm writing this review on my mobile so I don't forget to attempt to bring the rating down further than the current 1.6 to save others from the same horrible fate that I just suffered. <br /><br />I worst film I've ever seen and I can say (with hand on heart) it will never, never be topped.\n"
     ]
    }
   ],
   "source": [
    "# загружаем датасет\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_dataset = [imdb[\"train\"][i] for i in range(10_000, 15_000)]\n",
    "test_dataset = [imdb[\"test\"][i] for i in range(10_000, 15_000)]\n",
    "\n",
    "print(\"Label\")\n",
    "print(train_dataset[0][\"label\"])\n",
    "print(\"Text\")\n",
    "print(train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ed1a2bc9174b938bf7ec94346ace0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fd7ef3b6384ca2b909bba4e00160be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/34695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8432\n"
     ]
    }
   ],
   "source": [
    "train_texts = [sample[\"text\"] for sample in train_dataset]\n",
    "test_texts = [sample[\"text\"] for sample in test_dataset]\n",
    "\n",
    "# Создаем токенизатор, min_count можете взять на свое усмотрение, но лучше брать в районе 5-10;\n",
    "\n",
    "# 4. Обучаем по ним LogisticRegression, accuracy на тесте должен быть 0.75+\n",
    "\n",
    "\n",
    "# 1. Токенизатор обучаем по входным текстам\n",
    "tokenizer = Tokenizer(train_texts, min_count=5)\n",
    "# 2. Обучаем по этим же текстам с этим токенайзером TFIDF\n",
    "tfidf = TFIDF(tokenizer)\n",
    "tfidf.fit(train_texts)\n",
    "\n",
    "# 3. превращаем тексты в numpy матрицы\n",
    "X_train = tfidf.predict(train_texts)\n",
    "X_test = tfidf.predict(test_texts)\n",
    "\n",
    "Y_train = np.array([sample[\"label\"] for sample in train_dataset])\n",
    "Y_test = np.array([sample[\"label\"] for sample in test_dataset])\n",
    "\n",
    "# Обучаем логистическую регрессию и смотрим на качество\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "accuracy = (prediction == Y_test).sum() / Y_test.shape[0]\n",
    "print(f\"accuracy = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим логистическую регрессию со второй TF-IDF моделью и сравним результаты. Для этого воспользуйтесь классом [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) из sklearn. Результаты должны быть похожи на вашу реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igorchebuniaev/Library/Caches/pypoetry/virtualenvs/gmvae-giXL_VnD-py3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8568\n"
     ]
    }
   ],
   "source": [
    "train_texts = [sample[\"text\"] for sample in train_dataset]\n",
    "test_texts = [sample[\"text\"] for sample in test_dataset]\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=5)\n",
    "\n",
    "X_train = tfidf.fit_transform(train_texts)\n",
    "X_test = tfidf.transform(test_texts)\n",
    "\n",
    "Y_train = np.array([sample[\"label\"] for sample in train_dataset])\n",
    "Y_test = np.array([sample[\"label\"] for sample in test_dataset])\n",
    "\n",
    "# Обучаем логистическую регрессию и смотрим на качество\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "accuracy = (prediction == Y_test).sum() / Y_test.shape[0]\n",
    "print(f\"accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-граммные языковые модели (15 баллов)\n",
    "\n",
    "### Расширяем Токенайзер (3 балла)\n",
    "\n",
    "Перед созданием языковой модели, расширим токенизационный класс. Добавим два флага в сигнатуру метода `encode`, чтобы управлять добавлением служебных токенов во время токенизации. Существующий метод `decode` уже пропускает `<PAD>` токен, добавим флаг `skip_special_tokens` для пропуска всех специальных токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoSTokenizerEoS(Tokenizer):\n",
    "    def encode(self, text: str, add_bos: bool = True, add_eos: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирование текста в набор индексов.\n",
    "\n",
    "        :param text: текст\n",
    "        :param add_bos: добавление begin-of-sentence токена в начало\n",
    "        :param add_eos: добавление end-of-sentence токена в конец\n",
    "        :return: набор индексов токенов\n",
    "        \"\"\"\n",
    "        tokens = tokenize(text)\n",
    "        tokens = [normalize(token) for token in tokens]\n",
    "        if add_bos:\n",
    "            tokens = [\"<BOS>\"] + tokens\n",
    "        if add_eos:\n",
    "            tokens = tokens + [\"<EOS>\"]\n",
    "        return [self.word2idx.get(token, self.unk_token_id) for token in tokens]\n",
    "\n",
    "    def decode(self, input_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n",
    "\n",
    "        :param input_ids: набор индексов токенов\n",
    "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "        :return: текст\n",
    "        \"\"\"\n",
    "        if skip_special_tokens:\n",
    "            input_ids = [idx for idx in input_ids if idx not in range(4)]\n",
    "        return \" \".join([self.idx2word[idx] for idx in input_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61911dfd0cd24fb8a127e7bb719022d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40692a2e4b7e4e70ae9bac7e27d6b794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\", \"Hello there <EOS>\"]\n",
    "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
    "# tokenizer.encode(\"hello world\", add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581edebefe504069ae1b2f38502691ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faf78bcb76149c49b22889cbcbe709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bos_tokenizer OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\", \"Hello there <EOS>\"]\n",
    "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
    "# референсные индексы для world справа изменены (8 -> 6), обсуждение тут: https://t.me/c/2282620518/157/183\n",
    "assert tokenizer.encode(\"hello world\", add_bos=True, add_eos=True) == [1, 4, 6, 2]\n",
    "assert tokenizer.encode(\"hello world\", add_bos=False, add_eos=False) == [4, 6]\n",
    "print(\"test_bos_tokenizer OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём NGram Модель (12 баллов)\n",
    "\n",
    "Создайте класс `NGramLanguageModel` для построения n-граммной языковой модели. В этом задании вы можете как опираться на предложенную структуру модели, так и сделать свою имплементацию.\n",
    "\n",
    "Построение модели:\n",
    "   - Создайте метод `_build_model`, который принимает список текстов `texts` и обновляет частоты n-грамм.\n",
    "   - Для каждого текста:\n",
    "     - Токенизируйте текст и добавьте токен `\"<EOS>\"` в конец.\n",
    "     - Для каждого токена:\n",
    "       - Определите префикс длиной `n-1`.\n",
    "       - Обновите частоты n-грамм и частоты префиксов.\n",
    "\n",
    "Генерация следующего токена:\n",
    "   - Создайте метод `generate_next_token`, который принимает префикс `prefix` и возвращает следующий токен.\n",
    "   - Преобразуйте префикс в кортеж.\n",
    "   - Получите распределение частот для префикса.\n",
    "   - Если распределение пустое, верните токен `\"<UNK>\"`.\n",
    "   - Верните токен с наибольшей частотой.\n",
    "   - Если включен флаг sample возьмите токен пропорционально встречаемости\n",
    "\n",
    "Автодополнение текста:\n",
    "   - Создайте метод `autocomplete`, который принимает текст `text` и максимальную длину `max_len`, и возвращает завершенный текст.\n",
    "   - Токенизируйте текст.\n",
    "   - Пока длина токенов меньше `max_len`:\n",
    "     - Определите префикс длиной `n-1`.\n",
    "     - Сгенерируйте следующий токен.\n",
    "     - Добавьте токен в список токенов.\n",
    "     - Если токен равен `\"<EOS>\"`, завершите генерацию.\n",
    "   - Декодируйте и верните текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c0995862c845399bfa003fbd28b7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098c30fb61e24cf9a01e6e627390343f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7b453fa8f043e0b444aed9c34d477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Считаем слова:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da765e6a812401cae907ad82f3c75a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Строим словарь:   0%|          | 0/20956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Продолжение фразы `the movie was`\n",
      "the movi wa releas the rosenstrass protest in berlin and marri the woman listen in silenc , when they re obvious be compar to his work. < br / > <\n",
      "test_ngram_model OK\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int, tokenizer: BoSTokenizerEoS, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Создание n-граммной языковой модели.\n",
    "\n",
    "        :param n: порядок n-грамм\n",
    "        :param vocabulary: словарь\n",
    "        \"\"\"\n",
    "        assert n >= 2\n",
    "        self.n = n\n",
    "        self.tokenizer = tokenizer\n",
    "        # Словарь, ключ которого - префикс\n",
    "        # значение - Counter(), в котором ключ это продолжение n-граммы, \n",
    "        # а значение - частота встречи этого продолжения\n",
    "        self.frequencies = defaultdict(lambda: Counter())  # частота n-грамм\n",
    "        self._build_model(texts)\n",
    "\n",
    "    def _build_model(self, texts: List[str]):\n",
    "        \"\"\"\n",
    "        Построение модели на основе списка текстов.\n",
    "\n",
    "        :param texts: список текстов\n",
    "        \"\"\"\n",
    "        # Токенизируйте текст и добавьте токен `\"<EOS>\"` в конец.\n",
    "        for text in texts:\n",
    "            embedding = self.tokenizer.encode(text, add_eos=True)\n",
    "            # Для каждого токена:\n",
    "            # Определите префикс длиной `n-1`.\n",
    "            for i in range(len(embedding) - self.n + 1):\n",
    "                prefix = tuple(embedding[i:i + self.n - 1])\n",
    "                current_token = embedding[i + self.n - 1]\n",
    "                # Обновите частоты n-грамм и частоты префиксов.\n",
    "                self.frequencies[prefix].update([current_token])\n",
    "            # Обновите частоты n-грамм и частоты префиксов.\n",
    "\n",
    "\n",
    "    def generate_next_token(self, prefix: List[int], sample: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Генерация следующего токена по префиксу.\n",
    "\n",
    "        :param prefix: префикс\n",
    "        :param sample: используем ли мы сэмплинг в генерации\n",
    "        :return: следующий токен\n",
    "        \"\"\"\n",
    "        # Преобразуйте префикс в кортеж.\n",
    "        prefix = tuple(prefix)\n",
    "        # Получите распределение частот для префикса.\n",
    "        distribution = self.frequencies[prefix]\n",
    "        # Если распределение пустое, верните токен `\"<UNK>\"`.\n",
    "        if not distribution:\n",
    "            return self.tokenizer.unk_token_id\n",
    "        # Верните токен с наибольшей частотой.\n",
    "        if not sample:\n",
    "            return distribution.most_common(1)[0][0]\n",
    "        # Если включен флаг sample возьмите токен пропорционально встречаемости\n",
    "        tokens = list(distribution.keys())\n",
    "        weights = list(distribution.values())\n",
    "        return random.choices(tokens, weights=weights)[0]\n",
    "        \n",
    "        \n",
    "\n",
    "    def autocomplete(self, text: str, max_len: int = 32, \n",
    "                     skip_special_tokens: bool = True, sample: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Автоматическое дополнение текста.\n",
    "\n",
    "        :param text: текст\n",
    "        :param max_len: максимальная длина текста\n",
    "        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n",
    "        :return: завершенный текст\n",
    "        \"\"\"\n",
    "        # Токенизируйте текст.\n",
    "        tokens = self.tokenizer.encode(text, add_bos=True, add_eos=False)\n",
    "        # Пока длина токенов меньше `max_len`:\n",
    "        while len(tokens) < max_len:\n",
    "            #     Определите префикс длиной `n-1`.\n",
    "            prefix = tokens[-self.n + 1:]\n",
    "            #     Сгенерируйте следующий токен.\n",
    "            next_token = self.generate_next_token(prefix=prefix, sample=sample)\n",
    "            #     Добавьте токен в список токенов.\n",
    "            tokens.append(next_token)\n",
    "            #     Если токен равен `\"<EOS>\"`, завершите генерацию.\n",
    "            if next_token == self.tokenizer.word2idx[\"<EOS>\"]:\n",
    "                break\n",
    "        # Декодируйте и верните текст.\n",
    "        return self.tokenizer.decode(input_ids=tokens, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\"]\n",
    "tokenizer = BoSTokenizerEoS(corpus, min_count=1)\n",
    "ngram_lm = NGramLanguageModel(2, tokenizer, corpus)\n",
    "\n",
    "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10) == \"hello , python !\"\n",
    "assert ngram_lm.autocomplete(\"Hello, Python\", max_len=10, skip_special_tokens=False) == \"<BOS> hello , python ! <EOS>\"\n",
    "\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_texts = [imdb[\"train\"][i][\"text\"] for i in range(13_000, 15_000)]\n",
    "tokenizer = BoSTokenizerEoS(texts=train_texts, min_count=3)\n",
    "ngram_lm = NGramLanguageModel(3, tokenizer, train_texts)\n",
    "print(\"Продолжение фразы `the movie was`\")\n",
    "random.seed(1)\n",
    "print(ngram_lm.autocomplete(\"the movie was\", sample=True))\n",
    "print(\"test_ngram_model OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthpop",
   "language": "python",
   "name": "synthpop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
